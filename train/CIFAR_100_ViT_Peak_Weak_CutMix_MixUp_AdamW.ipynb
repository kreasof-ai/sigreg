{
  "cells": [
    {
      "cell_type": "code",
      "id": "pBxX3I1kYwojaamntSxiNZTK",
      "metadata": {
        "tags": [],
        "id": "pBxX3I1kYwojaamntSxiNZTK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d21cdfca-cb15-4ca5-8d68-3cd35211ce72"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "import math\n",
        "import time\n",
        "\n",
        "# ==========================================\n",
        "# 1. Configuration (Tuned for ViT on CIFAR)\n",
        "# ==========================================\n",
        "REG_MODE = 'weak'\n",
        "SIGR_ALPHA = 0.1   # Strength of the physics constraint\n",
        "SKETCH_DIM = 64    # Dimension of the random observer\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "LEARNING_RATE = 1e-3  # Slightly higher initial LR for AdamW with cosine schedule\n",
        "EPOCHS = 400\n",
        "WEIGHT_DECAY = 0.05\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "if torch.backends.mps.is_available(): DEVICE = 'mps'\n",
        "\n",
        "# Regularization Config\n",
        "MIXUP_ALPHA = 0.8\n",
        "CUTMIX_ALPHA = 1.0\n",
        "DROP_PATH_RATE = 0.1\n",
        "\n",
        "print(f\"Training on device: {DEVICE}\")\n",
        "\n",
        "# ==========================================\n",
        "# 2. Data Preparation (THE FIX: Strong Augmentation)\n",
        "# ==========================================\n",
        "def get_data_loaders():\n",
        "    print('==> Preparing data with Strong Augmentation...')\n",
        "\n",
        "    mean = (0.5071, 0.4867, 0.4408)\n",
        "    std = (0.2675, 0.2565, 0.2761)\n",
        "\n",
        "    # FIX 1: Add RandAugment\n",
        "    transform_train = transforms.Compose([\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandAugment(num_ops=2, magnitude=9), # <--- CRITICAL FOR ViT\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean, std),\n",
        "    ])\n",
        "\n",
        "    transform_test = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean, std),\n",
        "    ])\n",
        "\n",
        "    trainset = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=transform_train)\n",
        "    # Increase workers to handle augmentation load\n",
        "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
        "\n",
        "    testset = torchvision.datasets.CIFAR100(root='./data', train=False, download=True, transform=transform_test)\n",
        "    testloader = torch.utils.data.DataLoader(testset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
        "\n",
        "    return trainloader, testloader\n",
        "\n",
        "\n",
        "# ------------------------------------------\n",
        "# Physics Engine: The Regularizers\n",
        "# ------------------------------------------\n",
        "\n",
        "def sigreg_weak_loss(x, sketch_dim=64):\n",
        "    \"\"\"\n",
        "    Forces Covariance(x) ~ Identity.\n",
        "    Matches the 2nd Moment (Spherical Cloud).\n",
        "    \"\"\"\n",
        "    N, C = x.size()\n",
        "    # 1. Sketching (Optional for C=512, but good for consistency)\n",
        "    if C > sketch_dim:\n",
        "        S = torch.randn(sketch_dim, C, device=x.device) / (C ** 0.5)\n",
        "        x = x @ S.T  # [N, sketch_dim]\n",
        "    else:\n",
        "        sketch_dim = C\n",
        "\n",
        "    # 2. Centering & Covariance\n",
        "    x = x - x.mean(dim=0, keepdim=True)\n",
        "    cov = (x.T @ x) / (N - 1 + 1e-6)\n",
        "\n",
        "    # 3. Target Identity\n",
        "    target = torch.eye(sketch_dim, device=x.device)\n",
        "\n",
        "    # 4. Off-diagonal suppression + Diagonal maintenance\n",
        "    return torch.norm(cov - target, p='fro')\n",
        "\n",
        "def sigreg_strong_loss(x, sketch_dim=64):\n",
        "    \"\"\"\n",
        "    Forces ECF(x) ~ ECF(Gaussian).\n",
        "    Matches ALL Moments (Maximum Entropy Cloud).\n",
        "    Exact implementation of LeJEPA Algorithm 1.\n",
        "    \"\"\"\n",
        "    N, C = x.size()\n",
        "\n",
        "    # 1. Projection (The Observer)\n",
        "    # Project channels down to sketch_dim\n",
        "    A = torch.randn(C, sketch_dim, device=x.device)\n",
        "    A = A / (A.norm(p=2, dim=0, keepdim=True) + 1e-6)\n",
        "\n",
        "    # 2. Integration Points\n",
        "    t = torch.linspace(-5, 5, 17, device=x.device)\n",
        "\n",
        "    # 3. Theoretical Gaussian CF\n",
        "    exp_f = torch.exp(-0.5 * t**2)\n",
        "\n",
        "    # 4. Empirical CF\n",
        "    # proj: [N, sketch_dim]\n",
        "    proj = x @ A\n",
        "\n",
        "    # args: [N, sketch_dim, T]\n",
        "    args = proj.unsqueeze(2) * t.view(1, 1, -1)\n",
        "\n",
        "    # ecf: [sketch_dim, T] (Mean over batch)\n",
        "    ecf = torch.exp(1j * args).mean(dim=0)\n",
        "\n",
        "    # 5. Weighted L2 Distance\n",
        "    # |ecf - gauss|^2 * gauss_weight\n",
        "    diff_sq = (ecf - exp_f.unsqueeze(0)).abs().square()\n",
        "    err = diff_sq * exp_f.unsqueeze(0)\n",
        "\n",
        "    # 6. Integrate\n",
        "    loss = torch.trapz(err, t, dim=1) * N\n",
        "\n",
        "    return loss.mean()\n",
        "\n",
        "# ==========================================\n",
        "# 3. Mixup / CutMix Utilities\n",
        "# ==========================================\n",
        "def rand_bbox(size, lam):\n",
        "    W = size[2]\n",
        "    H = size[3]\n",
        "    cut_rat = np.sqrt(1. - lam)\n",
        "    cut_w = int(W * cut_rat)\n",
        "    cut_h = int(H * cut_rat)\n",
        "\n",
        "    cx = np.random.randint(W)\n",
        "    cy = np.random.randint(H)\n",
        "\n",
        "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
        "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
        "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
        "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
        "\n",
        "    return bbx1, bby1, bbx2, bby2\n",
        "\n",
        "def drop_path(x, drop_prob: float = 0., training: bool = False):\n",
        "    if drop_prob == 0. or not training: return x\n",
        "    keep_prob = 1 - drop_prob\n",
        "    shape = (x.shape[0],) + (1,) * (x.ndim - 1)\n",
        "    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
        "    random_tensor.floor_()\n",
        "    return x.div(keep_prob) * random_tensor\n",
        "\n",
        "class DropPath(nn.Module):\n",
        "    def __init__(self, drop_prob=None):\n",
        "        super(DropPath, self).__init__()\n",
        "        self.drop_prob = drop_prob\n",
        "    def forward(self, x): return drop_path(x, self.drop_prob, self.training)\n",
        "\n",
        "class PatchEmbedding(nn.Module):\n",
        "    def __init__(self, img_size=32, patch_size=4, in_chans=3, embed_dim=192):\n",
        "        super().__init__()\n",
        "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "    def forward(self, x):\n",
        "        x = self.proj(x).flatten(2).transpose(1, 2)\n",
        "        return x\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, dim, num_heads=8, qkv_bias=False, attn_drop=0., proj_drop=0.):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        head_dim = dim // num_heads\n",
        "        self.scale = head_dim ** -0.5\n",
        "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
        "        self.attn_drop = nn.Dropout(attn_drop)\n",
        "        self.proj = nn.Linear(dim, dim)\n",
        "        self.proj_drop = nn.Dropout(proj_drop)\n",
        "    def forward(self, x):\n",
        "        B, N, C = x.shape\n",
        "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        attn = self.attn_drop(attn)\n",
        "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
        "        x = self.proj(x)\n",
        "        x = self.proj_drop(x)\n",
        "        return x\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, dim, hidden_dim, drop=0.):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(dim, hidden_dim)\n",
        "        self.act = nn.GELU()\n",
        "        self.fc2 = nn.Linear(hidden_dim, dim)\n",
        "        self.drop = nn.Dropout(drop)\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.drop(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.drop(x)\n",
        "        return x\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, dim, num_heads, mlp_ratio=4., drop=0., attn_drop=0., drop_path=0., reg_mode='strong', sketch_dim=64):\n",
        "        super().__init__()\n",
        "        self.norm1 = nn.LayerNorm(dim)\n",
        "        self.attn = MultiHeadAttention(dim, num_heads=num_heads, qkv_bias=True, attn_drop=attn_drop, proj_drop=drop)\n",
        "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
        "        self.norm2 = nn.LayerNorm(dim)\n",
        "        self.mlp = MLP(dim, int(dim * mlp_ratio), drop=drop)\n",
        "\n",
        "        self.reg_mode = reg_mode\n",
        "        self.sketch_dim = sketch_dim\n",
        "    def forward(self, x):\n",
        "        x = x + self.drop_path(self.attn(self.norm1(x)))\n",
        "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
        "\n",
        "\n",
        "        # --- PHYSICS INJECTION ---\n",
        "        reg_loss = torch.tensor(0.0, device=x.device)\n",
        "        if self.reg_mode != 'baseline':\n",
        "            # Global Average Pool of the tokens [B, N, C] -> [B, C]\n",
        "            # This represents the \"Image Vector\" at this depth\n",
        "            flat_rep = x.mean(dim=1)\n",
        "\n",
        "            # Crucial: Pre-Norm vs Post-Norm context.\n",
        "            # LayerNorm forces variance=1. SIGReg forces Distribution=Gaussian.\n",
        "            # They are compatible.\n",
        "            if self.reg_mode == 'weak':\n",
        "                reg_loss = sigreg_weak_loss(flat_rep, self.sketch_dim)\n",
        "            elif self.reg_mode == 'strong':\n",
        "                reg_loss = sigreg_strong_loss(flat_rep, self.sketch_dim)\n",
        "\n",
        "        return x, reg_loss\n",
        "\n",
        "class VisionTransformer(nn.Module):\n",
        "    def __init__(self, img_size=32, patch_size=4, in_chans=3, num_classes=100, embed_dim=192, depth=9, num_heads=3, mlp_ratio=4., drop_rate=0., attn_drop_rate=0., drop_path_rate=0.1, reg_mode='strong', sketch_dim=64):\n",
        "        super().__init__()\n",
        "        self.patch_embed = PatchEmbedding(img_size, patch_size, in_chans, embed_dim)\n",
        "        num_patches = self.patch_embed.proj.weight.shape[2] # Just a hack to get patch count logic\n",
        "        num_patches = (img_size // patch_size) ** 2\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n",
        "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]\n",
        "        self.blocks = nn.ModuleList([\n",
        "            TransformerBlock(embed_dim, num_heads, mlp_ratio, drop_rate, attn_drop_rate, dpr[i], reg_mode, sketch_dim)\n",
        "            for i in range(depth)])\n",
        "        self.norm = nn.LayerNorm(embed_dim)\n",
        "        self.head = nn.Linear(embed_dim, num_classes)\n",
        "        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n",
        "        nn.init.trunc_normal_(self.cls_token, std=0.02)\n",
        "        nn.init.xavier_uniform_(self.head.weight)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B = x.shape[0]\n",
        "        x = self.patch_embed(x)\n",
        "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
        "        x = torch.cat((cls_tokens, x), dim=1)\n",
        "        x = x + self.pos_embed\n",
        "\n",
        "        total_phys_loss = 0.0\n",
        "\n",
        "        # Pass through blocks\n",
        "        for blk in self.blocks:\n",
        "            x, l_loss = blk(x, )\n",
        "            total_phys_loss += l_loss\n",
        "\n",
        "        x = self.norm(x)\n",
        "        out = self.head(x[:, 0])\n",
        "        return out, (total_phys_loss / len(self.blocks))\n",
        "\n",
        "# ==========================================\n",
        "# 5. Training Engine (Updated for Mixup/CutMix)\n",
        "# ==========================================\n",
        "def train(epoch, net, trainloader, optimizer, criterion):\n",
        "    net.train()\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    phys_loss_meter = 0\n",
        "\n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
        "\n",
        "        # Apply Mixup/CutMix\n",
        "        r = np.random.rand(1)\n",
        "        if r < 0.5: # Mixup\n",
        "            lam = np.random.beta(MIXUP_ALPHA, MIXUP_ALPHA)\n",
        "            index = torch.randperm(inputs.size(0)).to(DEVICE)\n",
        "            inputs = lam * inputs + (1 - lam) * inputs[index, :]\n",
        "            targets_a, targets_b = targets, targets[index]\n",
        "        else: # CutMix\n",
        "            lam = np.random.beta(CUTMIX_ALPHA, CUTMIX_ALPHA)\n",
        "            rand_index = torch.randperm(inputs.size(0)).to(DEVICE)\n",
        "            target_a = targets\n",
        "            target_b = targets[rand_index]\n",
        "            bbx1, bby1, bbx2, bby2 = rand_bbox(inputs.size(), lam)\n",
        "            inputs[:, :, bbx1:bbx2, bby1:bby2] = inputs[rand_index, :, bbx1:bbx2, bby1:bby2]\n",
        "            lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (inputs.size()[-1] * inputs.size()[-2]))\n",
        "            targets_a, targets_b = target_a, target_b\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward\n",
        "        outputs, p_loss = net(inputs)\n",
        "\n",
        "        # Task Loss\n",
        "        c_loss = criterion(outputs, targets_a) * lam + criterion(outputs, targets_b) * (1. - lam)\n",
        "\n",
        "        # Total Loss\n",
        "        loss = (1 - SIGR_ALPHA) * c_loss + (SIGR_ALPHA * p_loss)\n",
        "\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(net.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += ((1 - SIGR_ALPHA) * c_loss).item() # Log only task loss for comparison\n",
        "        phys_loss_meter += (SIGR_ALPHA * p_loss).item()\n",
        "\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += (lam * predicted.eq(targets_a).float() + (1 - lam) * predicted.eq(targets_b).float()).sum().item()\n",
        "\n",
        "    acc = 100. * correct / total\n",
        "    return train_loss / (batch_idx + 1), acc, phys_loss_meter / (batch_idx + 1)\n",
        "\n",
        "def test(epoch, net, testloader, criterion):\n",
        "    net.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "            inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
        "            outputs, _ = net(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "    acc = 100. * correct / total\n",
        "    return test_loss / (batch_idx + 1), acc\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    trainloader, testloader = get_data_loaders()\n",
        "\n",
        "    net = VisionTransformer(img_size=32, patch_size=4, embed_dim=192, depth=9, num_heads=3, drop_path_rate=0.1, reg_mode=REG_MODE, sketch_dim=SKETCH_DIM)\n",
        "    net = net.to(DEVICE)\n",
        "\n",
        "    # Standard CrossEntropy for final eval, SoftLabel for training is handled by Mixup logic\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # FIX 3: Robust AdamW Baseline\n",
        "    optimizer = optim.AdamW(net.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
        "\n",
        "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS, eta_min=1e-5)\n",
        "\n",
        "    print(f\"Starting training for {EPOCHS} epochs with RandAugment + Mixup/CutMix...\")\n",
        "\n",
        "    best_acc = 0\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        start_time = time.time()\n",
        "\n",
        "        train_loss, train_acc, physics_loss = train(epoch, net, trainloader, optimizer, criterion)\n",
        "        test_loss, test_acc = test(epoch, net, testloader, criterion)\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        if test_acc > best_acc:\n",
        "            best_acc = test_acc\n",
        "            # torch.save(net.state_dict(), f'thermo_resnet_{REG_MODE}.pth')\n",
        "\n",
        "        epoch_time = time.time() - start_time\n",
        "\n",
        "        print(f\"Epoch {epoch+1} | T: {epoch_time:.0f}s | \"\n",
        "              f\"Train: {train_loss:.4f} ({train_acc:.1f}%) | \"\n",
        "              f\"Phys: {physics_loss:.2f} | \"\n",
        "              f\"Val: {test_loss:.4f} ({test_acc:.2f}%) | \"\n",
        "              f\"Best: {best_acc:.2f}%\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on device: cuda\n",
            "==> Preparing data with Strong Augmentation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 169M/169M [00:13<00:00, 12.6MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training for 400 epochs with RandAugment + Mixup/CutMix...\n",
            "Epoch 1 | T: 28s | Train: 4.0098 (3.9%) | Phys: 0.68 | Val: 3.9097 (9.76%) | Best: 9.76%\n",
            "Epoch 2 | T: 27s | Train: 3.8060 (6.8%) | Phys: 0.62 | Val: 3.6774 (13.55%) | Best: 13.55%\n",
            "Epoch 3 | T: 27s | Train: 3.7151 (8.3%) | Phys: 0.60 | Val: 3.5369 (16.48%) | Best: 16.48%\n",
            "Epoch 4 | T: 27s | Train: 3.6722 (9.2%) | Phys: 0.60 | Val: 3.4010 (18.92%) | Best: 18.92%\n",
            "Epoch 5 | T: 27s | Train: 3.6061 (10.3%) | Phys: 0.59 | Val: 3.3542 (20.06%) | Best: 20.06%\n",
            "Epoch 6 | T: 27s | Train: 3.5988 (10.8%) | Phys: 0.59 | Val: 3.2294 (20.93%) | Best: 20.93%\n",
            "Epoch 7 | T: 27s | Train: 3.5395 (11.7%) | Phys: 0.59 | Val: 3.1924 (22.58%) | Best: 22.58%\n",
            "Epoch 8 | T: 27s | Train: 3.5121 (12.2%) | Phys: 0.59 | Val: 3.0503 (25.73%) | Best: 25.73%\n",
            "Epoch 9 | T: 27s | Train: 3.5049 (12.7%) | Phys: 0.59 | Val: 3.0873 (24.19%) | Best: 25.73%\n",
            "Epoch 10 | T: 27s | Train: 3.4621 (13.3%) | Phys: 0.59 | Val: 2.9907 (26.81%) | Best: 26.81%\n",
            "Epoch 11 | T: 28s | Train: 3.4594 (13.4%) | Phys: 0.59 | Val: 2.9745 (27.18%) | Best: 27.18%\n",
            "Epoch 12 | T: 27s | Train: 3.3959 (15.0%) | Phys: 0.59 | Val: 2.8886 (28.97%) | Best: 28.97%\n",
            "Epoch 13 | T: 27s | Train: 3.3954 (15.1%) | Phys: 0.59 | Val: 2.8936 (29.31%) | Best: 29.31%\n",
            "Epoch 14 | T: 27s | Train: 3.3756 (15.5%) | Phys: 0.59 | Val: 2.7987 (30.05%) | Best: 30.05%\n",
            "Epoch 15 | T: 27s | Train: 3.3514 (16.0%) | Phys: 0.59 | Val: 2.8639 (29.71%) | Best: 30.05%\n",
            "Epoch 16 | T: 27s | Train: 3.3222 (16.5%) | Phys: 0.59 | Val: 2.7044 (32.53%) | Best: 32.53%\n",
            "Epoch 17 | T: 27s | Train: 3.3326 (16.4%) | Phys: 0.59 | Val: 2.7063 (33.01%) | Best: 33.01%\n",
            "Epoch 18 | T: 27s | Train: 3.2807 (17.7%) | Phys: 0.59 | Val: 2.7001 (32.64%) | Best: 33.01%\n",
            "Epoch 19 | T: 27s | Train: 3.2729 (17.8%) | Phys: 0.59 | Val: 2.6192 (35.15%) | Best: 35.15%\n",
            "Epoch 20 | T: 27s | Train: 3.2635 (18.1%) | Phys: 0.59 | Val: 2.6084 (35.83%) | Best: 35.83%\n",
            "Epoch 21 | T: 27s | Train: 3.2209 (18.9%) | Phys: 0.59 | Val: 2.6083 (35.15%) | Best: 35.83%\n",
            "Epoch 22 | T: 27s | Train: 3.1587 (20.2%) | Phys: 0.59 | Val: 2.4789 (38.02%) | Best: 38.02%\n",
            "Epoch 23 | T: 27s | Train: 3.1886 (19.7%) | Phys: 0.59 | Val: 2.5241 (37.52%) | Best: 38.02%\n",
            "Epoch 24 | T: 27s | Train: 3.1617 (20.4%) | Phys: 0.59 | Val: 2.4071 (39.65%) | Best: 39.65%\n",
            "Epoch 25 | T: 27s | Train: 3.1834 (20.1%) | Phys: 0.59 | Val: 2.4097 (39.88%) | Best: 39.88%\n",
            "Epoch 26 | T: 27s | Train: 3.1169 (21.4%) | Phys: 0.59 | Val: 2.3518 (40.37%) | Best: 40.37%\n",
            "Epoch 27 | T: 27s | Train: 3.0838 (22.5%) | Phys: 0.59 | Val: 2.3305 (40.55%) | Best: 40.55%\n",
            "Epoch 28 | T: 27s | Train: 3.1119 (21.8%) | Phys: 0.59 | Val: 2.2646 (42.20%) | Best: 42.20%\n",
            "Epoch 29 | T: 27s | Train: 3.0748 (22.8%) | Phys: 0.59 | Val: 2.2433 (43.54%) | Best: 43.54%\n",
            "Epoch 30 | T: 27s | Train: 3.0501 (23.2%) | Phys: 0.59 | Val: 2.2462 (42.48%) | Best: 43.54%\n",
            "Epoch 31 | T: 27s | Train: 3.0590 (22.9%) | Phys: 0.59 | Val: 2.2371 (42.93%) | Best: 43.54%\n",
            "Epoch 32 | T: 27s | Train: 3.0027 (24.2%) | Phys: 0.59 | Val: 2.1846 (45.43%) | Best: 45.43%\n",
            "Epoch 33 | T: 27s | Train: 2.9650 (24.9%) | Phys: 0.59 | Val: 2.2394 (43.96%) | Best: 45.43%\n",
            "Epoch 34 | T: 27s | Train: 3.0113 (24.3%) | Phys: 0.59 | Val: 2.1632 (45.00%) | Best: 45.43%\n",
            "Epoch 35 | T: 27s | Train: 2.9537 (25.7%) | Phys: 0.59 | Val: 2.1034 (46.44%) | Best: 46.44%\n",
            "Epoch 36 | T: 27s | Train: 2.9433 (26.1%) | Phys: 0.59 | Val: 2.0294 (47.94%) | Best: 47.94%\n",
            "Epoch 37 | T: 27s | Train: 2.9113 (26.7%) | Phys: 0.59 | Val: 2.1312 (45.70%) | Best: 47.94%\n",
            "Epoch 38 | T: 27s | Train: 2.8855 (27.2%) | Phys: 0.59 | Val: 2.0350 (47.86%) | Best: 47.94%\n",
            "Epoch 39 | T: 27s | Train: 2.8631 (27.7%) | Phys: 0.59 | Val: 2.0165 (48.64%) | Best: 48.64%\n",
            "Epoch 40 | T: 27s | Train: 2.8148 (28.8%) | Phys: 0.59 | Val: 1.9399 (50.34%) | Best: 50.34%\n",
            "Epoch 41 | T: 27s | Train: 2.8808 (27.6%) | Phys: 0.59 | Val: 1.9870 (49.17%) | Best: 50.34%\n",
            "Epoch 42 | T: 27s | Train: 2.8090 (29.2%) | Phys: 0.59 | Val: 1.9169 (50.78%) | Best: 50.78%\n",
            "Epoch 43 | T: 27s | Train: 2.8267 (28.9%) | Phys: 0.59 | Val: 2.0086 (49.00%) | Best: 50.78%\n",
            "Epoch 44 | T: 27s | Train: 2.8378 (28.7%) | Phys: 0.59 | Val: 1.8954 (50.85%) | Best: 50.85%\n",
            "Epoch 45 | T: 27s | Train: 2.8081 (29.4%) | Phys: 0.59 | Val: 1.8441 (52.71%) | Best: 52.71%\n",
            "Epoch 46 | T: 27s | Train: 2.7928 (29.7%) | Phys: 0.59 | Val: 1.8573 (52.16%) | Best: 52.71%\n",
            "Epoch 47 | T: 27s | Train: 2.7532 (30.8%) | Phys: 0.59 | Val: 1.8811 (52.11%) | Best: 52.71%\n",
            "Epoch 48 | T: 27s | Train: 2.8021 (29.6%) | Phys: 0.59 | Val: 1.8612 (51.25%) | Best: 52.71%\n",
            "Epoch 49 | T: 27s | Train: 2.7515 (30.9%) | Phys: 0.59 | Val: 1.8369 (53.26%) | Best: 53.26%\n",
            "Epoch 50 | T: 27s | Train: 2.7432 (31.2%) | Phys: 0.59 | Val: 1.7576 (54.18%) | Best: 54.18%\n",
            "Epoch 51 | T: 27s | Train: 2.6878 (32.5%) | Phys: 0.59 | Val: 1.8135 (53.64%) | Best: 54.18%\n",
            "Epoch 52 | T: 27s | Train: 2.7225 (31.2%) | Phys: 0.59 | Val: 1.7683 (54.55%) | Best: 54.55%\n",
            "Epoch 53 | T: 27s | Train: 2.7113 (32.3%) | Phys: 0.59 | Val: 1.7680 (54.64%) | Best: 54.64%\n",
            "Epoch 54 | T: 27s | Train: 2.7247 (31.6%) | Phys: 0.59 | Val: 1.7332 (54.87%) | Best: 54.87%\n",
            "Epoch 55 | T: 27s | Train: 2.7193 (32.1%) | Phys: 0.59 | Val: 1.7631 (54.32%) | Best: 54.87%\n",
            "Epoch 56 | T: 27s | Train: 2.6738 (33.1%) | Phys: 0.59 | Val: 1.7364 (54.95%) | Best: 54.95%\n",
            "Epoch 57 | T: 27s | Train: 2.7170 (31.9%) | Phys: 0.59 | Val: 1.7123 (55.84%) | Best: 55.84%\n",
            "Epoch 58 | T: 27s | Train: 2.6607 (33.4%) | Phys: 0.59 | Val: 1.7011 (55.75%) | Best: 55.84%\n",
            "Epoch 59 | T: 27s | Train: 2.6437 (34.0%) | Phys: 0.59 | Val: 1.7161 (55.59%) | Best: 55.84%\n",
            "Epoch 60 | T: 27s | Train: 2.6376 (34.1%) | Phys: 0.59 | Val: 1.6838 (56.42%) | Best: 56.42%\n",
            "Epoch 61 | T: 27s | Train: 2.6288 (34.1%) | Phys: 0.59 | Val: 1.6659 (56.35%) | Best: 56.42%\n",
            "Epoch 62 | T: 27s | Train: 2.6526 (33.7%) | Phys: 0.59 | Val: 1.7358 (55.16%) | Best: 56.42%\n",
            "Epoch 63 | T: 27s | Train: 2.6184 (34.7%) | Phys: 0.59 | Val: 1.6456 (56.69%) | Best: 56.69%\n",
            "Epoch 64 | T: 27s | Train: 2.6366 (34.2%) | Phys: 0.59 | Val: 1.6858 (56.33%) | Best: 56.69%\n",
            "Epoch 65 | T: 27s | Train: 2.5575 (36.2%) | Phys: 0.59 | Val: 1.6541 (57.85%) | Best: 57.85%\n",
            "Epoch 66 | T: 27s | Train: 2.5778 (35.7%) | Phys: 0.59 | Val: 1.5814 (58.91%) | Best: 58.91%\n",
            "Epoch 67 | T: 27s | Train: 2.5885 (35.5%) | Phys: 0.59 | Val: 1.6394 (57.94%) | Best: 58.91%\n",
            "Epoch 68 | T: 27s | Train: 2.5728 (35.7%) | Phys: 0.59 | Val: 1.6273 (58.03%) | Best: 58.91%\n",
            "Epoch 69 | T: 27s | Train: 2.5774 (35.6%) | Phys: 0.59 | Val: 1.6023 (58.35%) | Best: 58.91%\n",
            "Epoch 70 | T: 27s | Train: 2.5371 (36.3%) | Phys: 0.59 | Val: 1.5998 (58.38%) | Best: 58.91%\n",
            "Epoch 71 | T: 27s | Train: 2.5577 (36.2%) | Phys: 0.59 | Val: 1.5724 (58.92%) | Best: 58.92%\n",
            "Epoch 72 | T: 27s | Train: 2.5520 (36.5%) | Phys: 0.59 | Val: 1.5789 (58.78%) | Best: 58.92%\n",
            "Epoch 73 | T: 27s | Train: 2.5704 (36.1%) | Phys: 0.59 | Val: 1.5687 (59.84%) | Best: 59.84%\n",
            "Epoch 74 | T: 27s | Train: 2.4874 (38.2%) | Phys: 0.59 | Val: 1.5515 (59.69%) | Best: 59.84%\n",
            "Epoch 75 | T: 27s | Train: 2.5635 (36.4%) | Phys: 0.59 | Val: 1.5695 (59.08%) | Best: 59.84%\n",
            "Epoch 76 | T: 27s | Train: 2.5363 (37.1%) | Phys: 0.59 | Val: 1.5847 (58.71%) | Best: 59.84%\n",
            "Epoch 77 | T: 27s | Train: 2.5139 (37.6%) | Phys: 0.59 | Val: 1.5299 (59.68%) | Best: 59.84%\n",
            "Epoch 78 | T: 27s | Train: 2.4890 (38.1%) | Phys: 0.59 | Val: 1.5296 (59.90%) | Best: 59.90%\n",
            "Epoch 79 | T: 27s | Train: 2.4992 (38.1%) | Phys: 0.59 | Val: 1.5396 (59.86%) | Best: 59.90%\n",
            "Epoch 80 | T: 27s | Train: 2.5421 (36.9%) | Phys: 0.59 | Val: 1.5251 (59.84%) | Best: 59.90%\n",
            "Epoch 81 | T: 27s | Train: 2.5234 (37.2%) | Phys: 0.59 | Val: 1.5358 (60.04%) | Best: 60.04%\n",
            "Epoch 82 | T: 27s | Train: 2.4627 (38.9%) | Phys: 0.59 | Val: 1.5001 (60.96%) | Best: 60.96%\n",
            "Epoch 83 | T: 27s | Train: 2.5012 (38.3%) | Phys: 0.59 | Val: 1.5047 (60.73%) | Best: 60.96%\n",
            "Epoch 84 | T: 27s | Train: 2.4523 (39.0%) | Phys: 0.59 | Val: 1.5264 (60.82%) | Best: 60.96%\n",
            "Epoch 85 | T: 27s | Train: 2.4356 (39.6%) | Phys: 0.59 | Val: 1.5280 (60.66%) | Best: 60.96%\n",
            "Epoch 86 | T: 27s | Train: 2.5193 (37.6%) | Phys: 0.59 | Val: 1.4821 (61.32%) | Best: 61.32%\n",
            "Epoch 87 | T: 27s | Train: 2.4445 (39.3%) | Phys: 0.59 | Val: 1.5028 (60.93%) | Best: 61.32%\n",
            "Epoch 88 | T: 27s | Train: 2.4597 (39.0%) | Phys: 0.59 | Val: 1.5159 (60.75%) | Best: 61.32%\n",
            "Epoch 89 | T: 27s | Train: 2.3905 (40.8%) | Phys: 0.59 | Val: 1.4781 (61.37%) | Best: 61.37%\n",
            "Epoch 90 | T: 27s | Train: 2.4114 (40.3%) | Phys: 0.59 | Val: 1.4795 (61.50%) | Best: 61.50%\n",
            "Epoch 91 | T: 27s | Train: 2.4539 (39.6%) | Phys: 0.59 | Val: 1.4512 (61.46%) | Best: 61.50%\n",
            "Epoch 92 | T: 27s | Train: 2.4638 (39.2%) | Phys: 0.59 | Val: 1.5072 (60.95%) | Best: 61.50%\n",
            "Epoch 93 | T: 27s | Train: 2.4095 (40.5%) | Phys: 0.59 | Val: 1.4741 (61.46%) | Best: 61.50%\n",
            "Epoch 94 | T: 27s | Train: 2.4301 (39.9%) | Phys: 0.59 | Val: 1.4960 (61.00%) | Best: 61.50%\n",
            "Epoch 95 | T: 27s | Train: 2.3863 (40.9%) | Phys: 0.59 | Val: 1.4568 (61.95%) | Best: 61.95%\n",
            "Epoch 96 | T: 27s | Train: 2.4493 (39.6%) | Phys: 0.59 | Val: 1.4354 (62.29%) | Best: 62.29%\n",
            "Epoch 97 | T: 27s | Train: 2.3626 (41.6%) | Phys: 0.59 | Val: 1.4782 (61.34%) | Best: 62.29%\n",
            "Epoch 98 | T: 27s | Train: 2.4323 (40.2%) | Phys: 0.59 | Val: 1.4464 (62.02%) | Best: 62.29%\n",
            "Epoch 99 | T: 27s | Train: 2.3704 (41.6%) | Phys: 0.59 | Val: 1.4515 (61.89%) | Best: 62.29%\n",
            "Epoch 100 | T: 27s | Train: 2.4172 (40.4%) | Phys: 0.59 | Val: 1.4606 (61.83%) | Best: 62.29%\n",
            "Epoch 101 | T: 27s | Train: 2.3458 (42.3%) | Phys: 0.59 | Val: 1.4116 (62.68%) | Best: 62.68%\n",
            "Epoch 102 | T: 27s | Train: 2.4439 (39.8%) | Phys: 0.59 | Val: 1.4475 (61.77%) | Best: 62.68%\n",
            "Epoch 103 | T: 27s | Train: 2.3483 (42.3%) | Phys: 0.59 | Val: 1.4690 (61.41%) | Best: 62.68%\n",
            "Epoch 104 | T: 27s | Train: 2.3453 (42.2%) | Phys: 0.59 | Val: 1.4066 (63.09%) | Best: 63.09%\n",
            "Epoch 105 | T: 27s | Train: 2.3739 (41.7%) | Phys: 0.59 | Val: 1.3988 (62.92%) | Best: 63.09%\n",
            "Epoch 106 | T: 27s | Train: 2.3192 (42.8%) | Phys: 0.59 | Val: 1.4540 (62.21%) | Best: 63.09%\n",
            "Epoch 107 | T: 27s | Train: 2.3370 (42.5%) | Phys: 0.59 | Val: 1.4565 (62.31%) | Best: 63.09%\n",
            "Epoch 108 | T: 27s | Train: 2.4024 (40.9%) | Phys: 0.59 | Val: 1.4121 (62.80%) | Best: 63.09%\n",
            "Epoch 109 | T: 27s | Train: 2.3597 (41.9%) | Phys: 0.59 | Val: 1.4397 (62.42%) | Best: 63.09%\n",
            "Epoch 110 | T: 27s | Train: 2.3728 (41.9%) | Phys: 0.59 | Val: 1.4262 (62.75%) | Best: 63.09%\n",
            "Epoch 111 | T: 27s | Train: 2.3331 (42.7%) | Phys: 0.59 | Val: 1.4346 (62.25%) | Best: 63.09%\n",
            "Epoch 112 | T: 27s | Train: 2.3872 (41.5%) | Phys: 0.59 | Val: 1.4244 (62.52%) | Best: 63.09%\n",
            "Epoch 113 | T: 27s | Train: 2.3235 (43.3%) | Phys: 0.59 | Val: 1.4004 (62.98%) | Best: 63.09%\n",
            "Epoch 114 | T: 27s | Train: 2.3890 (41.1%) | Phys: 0.59 | Val: 1.3768 (63.37%) | Best: 63.37%\n",
            "Epoch 115 | T: 27s | Train: 2.3765 (41.7%) | Phys: 0.59 | Val: 1.3975 (63.44%) | Best: 63.44%\n",
            "Epoch 116 | T: 27s | Train: 2.3458 (42.7%) | Phys: 0.59 | Val: 1.3923 (63.71%) | Best: 63.71%\n",
            "Epoch 117 | T: 27s | Train: 2.3204 (43.2%) | Phys: 0.59 | Val: 1.3749 (64.24%) | Best: 64.24%\n",
            "Epoch 118 | T: 27s | Train: 2.2798 (44.3%) | Phys: 0.59 | Val: 1.4171 (63.29%) | Best: 64.24%\n",
            "Epoch 119 | T: 27s | Train: 2.2603 (44.8%) | Phys: 0.59 | Val: 1.3828 (63.25%) | Best: 64.24%\n",
            "Epoch 120 | T: 27s | Train: 2.2675 (44.4%) | Phys: 0.59 | Val: 1.3676 (63.85%) | Best: 64.24%\n",
            "Epoch 121 | T: 27s | Train: 2.2191 (45.8%) | Phys: 0.59 | Val: 1.3491 (64.19%) | Best: 64.24%\n",
            "Epoch 122 | T: 27s | Train: 2.2984 (43.7%) | Phys: 0.59 | Val: 1.3808 (63.78%) | Best: 64.24%\n",
            "Epoch 123 | T: 27s | Train: 2.3058 (43.9%) | Phys: 0.59 | Val: 1.3929 (63.57%) | Best: 64.24%\n",
            "Epoch 124 | T: 27s | Train: 2.2595 (44.7%) | Phys: 0.59 | Val: 1.3653 (63.86%) | Best: 64.24%\n",
            "Epoch 125 | T: 27s | Train: 2.3539 (42.4%) | Phys: 0.59 | Val: 1.4345 (62.74%) | Best: 64.24%\n",
            "Epoch 126 | T: 27s | Train: 2.3308 (43.0%) | Phys: 0.59 | Val: 1.3470 (64.21%) | Best: 64.24%\n",
            "Epoch 127 | T: 27s | Train: 2.2666 (44.6%) | Phys: 0.59 | Val: 1.3978 (63.13%) | Best: 64.24%\n",
            "Epoch 128 | T: 27s | Train: 2.2801 (44.3%) | Phys: 0.59 | Val: 1.3793 (63.93%) | Best: 64.24%\n",
            "Epoch 129 | T: 27s | Train: 2.3157 (43.3%) | Phys: 0.59 | Val: 1.3541 (64.52%) | Best: 64.52%\n",
            "Epoch 130 | T: 27s | Train: 2.2478 (45.2%) | Phys: 0.59 | Val: 1.3231 (65.18%) | Best: 65.18%\n",
            "Epoch 131 | T: 27s | Train: 2.2674 (44.9%) | Phys: 0.59 | Val: 1.3373 (64.98%) | Best: 65.18%\n",
            "Epoch 132 | T: 27s | Train: 2.2556 (45.1%) | Phys: 0.59 | Val: 1.3840 (63.40%) | Best: 65.18%\n",
            "Epoch 133 | T: 27s | Train: 2.2565 (45.0%) | Phys: 0.59 | Val: 1.3938 (63.76%) | Best: 65.18%\n",
            "Epoch 134 | T: 27s | Train: 2.2522 (44.9%) | Phys: 0.59 | Val: 1.3373 (64.70%) | Best: 65.18%\n",
            "Epoch 135 | T: 27s | Train: 2.2719 (44.8%) | Phys: 0.59 | Val: 1.3908 (63.43%) | Best: 65.18%\n",
            "Epoch 136 | T: 27s | Train: 2.2811 (44.2%) | Phys: 0.59 | Val: 1.3426 (64.30%) | Best: 65.18%\n",
            "Epoch 137 | T: 27s | Train: 2.2522 (45.0%) | Phys: 0.59 | Val: 1.3692 (64.27%) | Best: 65.18%\n",
            "Epoch 138 | T: 27s | Train: 2.2457 (45.3%) | Phys: 0.59 | Val: 1.3400 (64.82%) | Best: 65.18%\n",
            "Epoch 139 | T: 27s | Train: 2.2334 (45.8%) | Phys: 0.59 | Val: 1.3479 (64.60%) | Best: 65.18%\n",
            "Epoch 140 | T: 27s | Train: 2.2438 (45.3%) | Phys: 0.59 | Val: 1.3417 (64.65%) | Best: 65.18%\n",
            "Epoch 141 | T: 27s | Train: 2.2641 (44.8%) | Phys: 0.59 | Val: 1.3739 (64.40%) | Best: 65.18%\n",
            "Epoch 142 | T: 27s | Train: 2.2365 (45.7%) | Phys: 0.59 | Val: 1.3249 (65.06%) | Best: 65.18%\n",
            "Epoch 143 | T: 27s | Train: 2.1987 (46.7%) | Phys: 0.59 | Val: 1.3588 (64.39%) | Best: 65.18%\n",
            "Epoch 144 | T: 27s | Train: 2.2489 (45.2%) | Phys: 0.59 | Val: 1.3685 (64.48%) | Best: 65.18%\n",
            "Epoch 145 | T: 27s | Train: 2.1522 (48.2%) | Phys: 0.59 | Val: 1.3377 (64.93%) | Best: 65.18%\n",
            "Epoch 146 | T: 27s | Train: 2.2146 (46.2%) | Phys: 0.59 | Val: 1.3389 (64.73%) | Best: 65.18%\n",
            "Epoch 147 | T: 27s | Train: 2.2216 (46.2%) | Phys: 0.59 | Val: 1.3311 (64.91%) | Best: 65.18%\n",
            "Epoch 148 | T: 27s | Train: 2.2044 (46.3%) | Phys: 0.59 | Val: 1.3158 (65.37%) | Best: 65.37%\n",
            "Epoch 149 | T: 27s | Train: 2.2020 (47.0%) | Phys: 0.59 | Val: 1.3363 (64.73%) | Best: 65.37%\n",
            "Epoch 150 | T: 27s | Train: 2.1926 (47.0%) | Phys: 0.59 | Val: 1.3437 (64.71%) | Best: 65.37%\n",
            "Epoch 151 | T: 27s | Train: 2.1633 (47.6%) | Phys: 0.59 | Val: 1.3308 (64.76%) | Best: 65.37%\n",
            "Epoch 152 | T: 27s | Train: 2.1843 (47.4%) | Phys: 0.59 | Val: 1.3297 (64.56%) | Best: 65.37%\n",
            "Epoch 153 | T: 27s | Train: 2.2084 (46.1%) | Phys: 0.59 | Val: 1.3413 (65.10%) | Best: 65.37%\n",
            "Epoch 154 | T: 27s | Train: 2.2675 (45.0%) | Phys: 0.59 | Val: 1.3356 (64.61%) | Best: 65.37%\n",
            "Epoch 155 | T: 27s | Train: 2.1826 (47.2%) | Phys: 0.59 | Val: 1.3016 (66.10%) | Best: 66.10%\n",
            "Epoch 156 | T: 27s | Train: 2.2493 (45.6%) | Phys: 0.59 | Val: 1.3379 (64.70%) | Best: 66.10%\n",
            "Epoch 157 | T: 27s | Train: 2.1898 (47.1%) | Phys: 0.59 | Val: 1.2969 (65.55%) | Best: 66.10%\n",
            "Epoch 158 | T: 27s | Train: 2.1943 (47.1%) | Phys: 0.59 | Val: 1.3125 (65.32%) | Best: 66.10%\n",
            "Epoch 159 | T: 27s | Train: 2.2125 (46.7%) | Phys: 0.59 | Val: 1.3163 (65.74%) | Best: 66.10%\n",
            "Epoch 160 | T: 27s | Train: 2.2528 (45.3%) | Phys: 0.59 | Val: 1.3334 (64.75%) | Best: 66.10%\n",
            "Epoch 161 | T: 27s | Train: 2.2194 (46.0%) | Phys: 0.59 | Val: 1.3479 (65.19%) | Best: 66.10%\n",
            "Epoch 162 | T: 27s | Train: 2.2117 (46.7%) | Phys: 0.59 | Val: 1.2897 (66.04%) | Best: 66.10%\n",
            "Epoch 163 | T: 27s | Train: 2.2215 (46.2%) | Phys: 0.59 | Val: 1.3351 (64.92%) | Best: 66.10%\n",
            "Epoch 164 | T: 27s | Train: 2.1638 (47.7%) | Phys: 0.59 | Val: 1.3106 (65.65%) | Best: 66.10%\n",
            "Epoch 165 | T: 27s | Train: 2.1710 (47.8%) | Phys: 0.59 | Val: 1.3268 (65.03%) | Best: 66.10%\n",
            "Epoch 166 | T: 27s | Train: 2.1237 (48.7%) | Phys: 0.59 | Val: 1.2978 (65.71%) | Best: 66.10%\n",
            "Epoch 167 | T: 27s | Train: 2.1529 (48.3%) | Phys: 0.59 | Val: 1.2946 (66.42%) | Best: 66.42%\n",
            "Epoch 168 | T: 27s | Train: 2.1467 (48.2%) | Phys: 0.59 | Val: 1.2891 (65.53%) | Best: 66.42%\n",
            "Epoch 169 | T: 27s | Train: 2.1231 (48.9%) | Phys: 0.59 | Val: 1.2741 (66.08%) | Best: 66.42%\n",
            "Epoch 170 | T: 27s | Train: 2.1177 (49.3%) | Phys: 0.59 | Val: 1.3077 (65.60%) | Best: 66.42%\n",
            "Epoch 171 | T: 27s | Train: 2.1656 (47.7%) | Phys: 0.59 | Val: 1.2990 (65.92%) | Best: 66.42%\n",
            "Epoch 172 | T: 27s | Train: 2.1957 (46.9%) | Phys: 0.59 | Val: 1.2803 (66.28%) | Best: 66.42%\n",
            "Epoch 173 | T: 27s | Train: 2.1336 (48.9%) | Phys: 0.59 | Val: 1.2786 (66.56%) | Best: 66.56%\n",
            "Epoch 174 | T: 27s | Train: 2.1567 (47.9%) | Phys: 0.59 | Val: 1.3137 (65.35%) | Best: 66.56%\n",
            "Epoch 175 | T: 27s | Train: 2.0967 (49.8%) | Phys: 0.59 | Val: 1.3010 (65.67%) | Best: 66.56%\n",
            "Epoch 176 | T: 27s | Train: 2.1058 (49.3%) | Phys: 0.59 | Val: 1.2776 (66.30%) | Best: 66.56%\n",
            "Epoch 177 | T: 27s | Train: 2.1655 (47.9%) | Phys: 0.59 | Val: 1.2858 (66.23%) | Best: 66.56%\n",
            "Epoch 178 | T: 27s | Train: 2.1411 (48.6%) | Phys: 0.59 | Val: 1.2879 (66.54%) | Best: 66.56%\n",
            "Epoch 179 | T: 27s | Train: 2.1455 (48.6%) | Phys: 0.59 | Val: 1.3071 (66.03%) | Best: 66.56%\n",
            "Epoch 180 | T: 27s | Train: 2.1219 (48.9%) | Phys: 0.59 | Val: 1.3263 (65.78%) | Best: 66.56%\n",
            "Epoch 181 | T: 27s | Train: 2.1147 (49.4%) | Phys: 0.59 | Val: 1.3017 (65.81%) | Best: 66.56%\n",
            "Epoch 182 | T: 27s | Train: 2.1872 (47.3%) | Phys: 0.59 | Val: 1.3109 (65.88%) | Best: 66.56%\n",
            "Epoch 183 | T: 27s | Train: 2.1631 (48.3%) | Phys: 0.59 | Val: 1.2808 (66.45%) | Best: 66.56%\n",
            "Epoch 184 | T: 27s | Train: 2.0881 (50.0%) | Phys: 0.59 | Val: 1.2894 (66.18%) | Best: 66.56%\n",
            "Epoch 185 | T: 27s | Train: 2.1056 (49.8%) | Phys: 0.59 | Val: 1.2896 (66.10%) | Best: 66.56%\n",
            "Epoch 186 | T: 27s | Train: 2.1527 (48.1%) | Phys: 0.59 | Val: 1.2742 (66.70%) | Best: 66.70%\n",
            "Epoch 187 | T: 27s | Train: 2.1702 (47.9%) | Phys: 0.59 | Val: 1.2953 (66.25%) | Best: 66.70%\n",
            "Epoch 188 | T: 27s | Train: 2.1146 (49.3%) | Phys: 0.59 | Val: 1.2950 (66.33%) | Best: 66.70%\n",
            "Epoch 189 | T: 27s | Train: 2.1397 (48.7%) | Phys: 0.59 | Val: 1.2692 (66.64%) | Best: 66.70%\n",
            "Epoch 190 | T: 27s | Train: 2.0917 (49.8%) | Phys: 0.59 | Val: 1.2955 (65.89%) | Best: 66.70%\n",
            "Epoch 191 | T: 27s | Train: 2.1201 (49.2%) | Phys: 0.59 | Val: 1.2498 (67.10%) | Best: 67.10%\n",
            "Epoch 192 | T: 27s | Train: 2.0958 (49.8%) | Phys: 0.59 | Val: 1.2675 (66.81%) | Best: 67.10%\n",
            "Epoch 193 | T: 27s | Train: 2.0416 (51.2%) | Phys: 0.59 | Val: 1.2432 (67.54%) | Best: 67.54%\n",
            "Epoch 194 | T: 27s | Train: 2.1070 (49.4%) | Phys: 0.59 | Val: 1.2799 (66.52%) | Best: 67.54%\n",
            "Epoch 195 | T: 27s | Train: 2.0766 (49.9%) | Phys: 0.59 | Val: 1.2335 (67.40%) | Best: 67.54%\n",
            "Epoch 196 | T: 27s | Train: 2.1024 (49.9%) | Phys: 0.59 | Val: 1.2660 (66.98%) | Best: 67.54%\n",
            "Epoch 197 | T: 27s | Train: 2.0517 (51.1%) | Phys: 0.59 | Val: 1.2604 (67.17%) | Best: 67.54%\n",
            "Epoch 198 | T: 27s | Train: 2.1198 (48.9%) | Phys: 0.59 | Val: 1.2920 (66.51%) | Best: 67.54%\n",
            "Epoch 199 | T: 27s | Train: 2.0746 (50.6%) | Phys: 0.59 | Val: 1.2928 (66.36%) | Best: 67.54%\n",
            "Epoch 200 | T: 27s | Train: 2.0246 (51.9%) | Phys: 0.59 | Val: 1.3000 (66.60%) | Best: 67.54%\n",
            "Epoch 201 | T: 27s | Train: 2.0174 (52.3%) | Phys: 0.59 | Val: 1.2815 (67.16%) | Best: 67.54%\n",
            "Epoch 202 | T: 27s | Train: 2.1215 (49.4%) | Phys: 0.59 | Val: 1.2824 (67.11%) | Best: 67.54%\n",
            "Epoch 203 | T: 27s | Train: 2.0240 (51.8%) | Phys: 0.59 | Val: 1.2599 (66.83%) | Best: 67.54%\n",
            "Epoch 204 | T: 27s | Train: 2.0687 (51.1%) | Phys: 0.59 | Val: 1.2604 (67.53%) | Best: 67.54%\n",
            "Epoch 205 | T: 27s | Train: 2.0602 (51.2%) | Phys: 0.59 | Val: 1.3015 (67.07%) | Best: 67.54%\n",
            "Epoch 206 | T: 27s | Train: 2.0171 (52.4%) | Phys: 0.59 | Val: 1.2800 (67.27%) | Best: 67.54%\n",
            "Epoch 207 | T: 27s | Train: 2.0138 (52.3%) | Phys: 0.59 | Val: 1.2934 (66.59%) | Best: 67.54%\n",
            "Epoch 208 | T: 27s | Train: 2.0440 (51.4%) | Phys: 0.59 | Val: 1.2785 (67.15%) | Best: 67.54%\n",
            "Epoch 209 | T: 27s | Train: 2.0623 (50.8%) | Phys: 0.59 | Val: 1.2713 (67.62%) | Best: 67.62%\n",
            "Epoch 210 | T: 27s | Train: 2.0376 (51.9%) | Phys: 0.59 | Val: 1.2717 (67.23%) | Best: 67.62%\n",
            "Epoch 211 | T: 27s | Train: 2.0176 (51.9%) | Phys: 0.59 | Val: 1.2634 (66.77%) | Best: 67.62%\n",
            "Epoch 212 | T: 27s | Train: 2.0015 (52.5%) | Phys: 0.59 | Val: 1.2784 (66.90%) | Best: 67.62%\n",
            "Epoch 213 | T: 27s | Train: 2.0919 (50.2%) | Phys: 0.59 | Val: 1.2729 (67.22%) | Best: 67.62%\n",
            "Epoch 214 | T: 27s | Train: 1.9843 (53.0%) | Phys: 0.59 | Val: 1.2348 (67.50%) | Best: 67.62%\n",
            "Epoch 215 | T: 27s | Train: 2.0251 (51.8%) | Phys: 0.59 | Val: 1.2248 (67.69%) | Best: 67.69%\n",
            "Epoch 216 | T: 27s | Train: 2.0456 (51.1%) | Phys: 0.59 | Val: 1.2632 (67.45%) | Best: 67.69%\n",
            "Epoch 217 | T: 27s | Train: 2.0604 (51.0%) | Phys: 0.59 | Val: 1.2885 (66.78%) | Best: 67.69%\n",
            "Epoch 218 | T: 27s | Train: 1.9709 (53.5%) | Phys: 0.59 | Val: 1.2613 (67.14%) | Best: 67.69%\n",
            "Epoch 219 | T: 27s | Train: 2.0314 (51.6%) | Phys: 0.59 | Val: 1.2485 (67.20%) | Best: 67.69%\n",
            "Epoch 220 | T: 27s | Train: 2.0657 (50.9%) | Phys: 0.59 | Val: 1.2566 (67.65%) | Best: 67.69%\n",
            "Epoch 221 | T: 27s | Train: 2.0048 (52.6%) | Phys: 0.59 | Val: 1.2509 (67.41%) | Best: 67.69%\n",
            "Epoch 222 | T: 27s | Train: 1.9729 (53.6%) | Phys: 0.59 | Val: 1.2510 (67.51%) | Best: 67.69%\n",
            "Epoch 223 | T: 27s | Train: 2.0116 (52.4%) | Phys: 0.59 | Val: 1.2359 (67.51%) | Best: 67.69%\n",
            "Epoch 224 | T: 27s | Train: 1.9924 (53.0%) | Phys: 0.59 | Val: 1.2431 (67.41%) | Best: 67.69%\n",
            "Epoch 225 | T: 27s | Train: 2.0084 (52.4%) | Phys: 0.59 | Val: 1.2561 (67.26%) | Best: 67.69%\n",
            "Epoch 226 | T: 27s | Train: 2.0082 (52.6%) | Phys: 0.59 | Val: 1.2538 (67.35%) | Best: 67.69%\n",
            "Epoch 227 | T: 27s | Train: 2.0383 (51.8%) | Phys: 0.59 | Val: 1.2302 (67.86%) | Best: 67.86%\n",
            "Epoch 228 | T: 27s | Train: 1.9953 (53.0%) | Phys: 0.59 | Val: 1.2396 (67.67%) | Best: 67.86%\n",
            "Epoch 229 | T: 27s | Train: 1.9843 (53.4%) | Phys: 0.59 | Val: 1.2325 (67.79%) | Best: 67.86%\n",
            "Epoch 230 | T: 27s | Train: 1.9674 (53.5%) | Phys: 0.59 | Val: 1.2195 (68.37%) | Best: 68.37%\n",
            "Epoch 231 | T: 27s | Train: 2.0016 (52.9%) | Phys: 0.59 | Val: 1.2161 (68.45%) | Best: 68.45%\n",
            "Epoch 232 | T: 27s | Train: 2.0014 (52.8%) | Phys: 0.59 | Val: 1.2554 (67.78%) | Best: 68.45%\n",
            "Epoch 233 | T: 27s | Train: 2.0158 (51.9%) | Phys: 0.59 | Val: 1.2092 (68.26%) | Best: 68.45%\n",
            "Epoch 234 | T: 27s | Train: 1.9833 (53.4%) | Phys: 0.59 | Val: 1.2436 (68.12%) | Best: 68.45%\n",
            "Epoch 235 | T: 27s | Train: 2.0204 (52.6%) | Phys: 0.59 | Val: 1.2418 (68.52%) | Best: 68.52%\n",
            "Epoch 236 | T: 27s | Train: 1.9444 (54.1%) | Phys: 0.59 | Val: 1.2259 (68.63%) | Best: 68.63%\n",
            "Epoch 237 | T: 27s | Train: 1.9206 (55.1%) | Phys: 0.59 | Val: 1.2272 (68.15%) | Best: 68.63%\n",
            "Epoch 238 | T: 27s | Train: 1.9404 (54.3%) | Phys: 0.59 | Val: 1.2183 (68.35%) | Best: 68.63%\n",
            "Epoch 239 | T: 27s | Train: 1.9990 (52.5%) | Phys: 0.59 | Val: 1.2453 (67.90%) | Best: 68.63%\n",
            "Epoch 240 | T: 27s | Train: 1.9277 (54.6%) | Phys: 0.59 | Val: 1.2475 (67.72%) | Best: 68.63%\n",
            "Epoch 241 | T: 27s | Train: 1.9463 (54.5%) | Phys: 0.59 | Val: 1.2100 (68.63%) | Best: 68.63%\n",
            "Epoch 242 | T: 27s | Train: 1.8999 (55.6%) | Phys: 0.59 | Val: 1.2295 (68.14%) | Best: 68.63%\n",
            "Epoch 243 | T: 27s | Train: 1.9283 (54.8%) | Phys: 0.59 | Val: 1.2212 (68.51%) | Best: 68.63%\n",
            "Epoch 244 | T: 27s | Train: 1.9492 (54.3%) | Phys: 0.59 | Val: 1.2188 (68.37%) | Best: 68.63%\n",
            "Epoch 245 | T: 27s | Train: 1.8827 (56.1%) | Phys: 0.59 | Val: 1.2261 (67.92%) | Best: 68.63%\n",
            "Epoch 246 | T: 27s | Train: 1.9440 (54.5%) | Phys: 0.59 | Val: 1.2413 (68.88%) | Best: 68.88%\n",
            "Epoch 247 | T: 27s | Train: 1.8359 (57.5%) | Phys: 0.59 | Val: 1.2458 (68.24%) | Best: 68.88%\n",
            "Epoch 248 | T: 27s | Train: 1.9534 (53.9%) | Phys: 0.59 | Val: 1.2654 (68.01%) | Best: 68.88%\n",
            "Epoch 249 | T: 27s | Train: 2.0594 (51.2%) | Phys: 0.59 | Val: 1.2045 (68.88%) | Best: 68.88%\n",
            "Epoch 250 | T: 27s | Train: 2.0125 (52.7%) | Phys: 0.59 | Val: 1.2164 (68.76%) | Best: 68.88%\n",
            "Epoch 251 | T: 27s | Train: 1.9295 (54.6%) | Phys: 0.59 | Val: 1.2344 (68.27%) | Best: 68.88%\n",
            "Epoch 252 | T: 27s | Train: 1.9147 (55.3%) | Phys: 0.59 | Val: 1.2427 (68.34%) | Best: 68.88%\n",
            "Epoch 253 | T: 27s | Train: 1.8691 (56.5%) | Phys: 0.59 | Val: 1.2138 (68.94%) | Best: 68.94%\n",
            "Epoch 254 | T: 27s | Train: 1.8490 (57.1%) | Phys: 0.59 | Val: 1.2210 (68.44%) | Best: 68.94%\n",
            "Epoch 255 | T: 27s | Train: 1.8849 (56.0%) | Phys: 0.59 | Val: 1.2129 (68.44%) | Best: 68.94%\n",
            "Epoch 256 | T: 27s | Train: 1.8868 (56.0%) | Phys: 0.59 | Val: 1.2311 (68.31%) | Best: 68.94%\n",
            "Epoch 257 | T: 27s | Train: 1.9026 (55.4%) | Phys: 0.59 | Val: 1.1939 (69.02%) | Best: 69.02%\n",
            "Epoch 258 | T: 27s | Train: 1.9344 (54.8%) | Phys: 0.59 | Val: 1.2072 (68.61%) | Best: 69.02%\n",
            "Epoch 259 | T: 27s | Train: 1.9012 (55.7%) | Phys: 0.59 | Val: 1.2195 (68.60%) | Best: 69.02%\n",
            "Epoch 260 | T: 27s | Train: 1.8815 (56.2%) | Phys: 0.59 | Val: 1.2206 (68.26%) | Best: 69.02%\n",
            "Epoch 261 | T: 27s | Train: 1.8966 (55.4%) | Phys: 0.59 | Val: 1.1950 (69.38%) | Best: 69.38%\n",
            "Epoch 262 | T: 27s | Train: 1.9544 (54.4%) | Phys: 0.59 | Val: 1.2386 (68.19%) | Best: 69.38%\n",
            "Epoch 263 | T: 27s | Train: 1.8798 (56.1%) | Phys: 0.59 | Val: 1.2031 (69.63%) | Best: 69.63%\n",
            "Epoch 264 | T: 27s | Train: 1.8985 (55.7%) | Phys: 0.59 | Val: 1.2090 (69.29%) | Best: 69.63%\n",
            "Epoch 265 | T: 27s | Train: 1.8828 (55.5%) | Phys: 0.59 | Val: 1.1843 (69.10%) | Best: 69.63%\n",
            "Epoch 266 | T: 27s | Train: 1.8571 (57.1%) | Phys: 0.59 | Val: 1.1888 (69.34%) | Best: 69.63%\n",
            "Epoch 267 | T: 27s | Train: 1.8592 (56.6%) | Phys: 0.59 | Val: 1.2069 (69.20%) | Best: 69.63%\n",
            "Epoch 268 | T: 27s | Train: 1.8810 (56.4%) | Phys: 0.59 | Val: 1.1961 (69.23%) | Best: 69.63%\n",
            "Epoch 269 | T: 27s | Train: 1.9398 (54.5%) | Phys: 0.60 | Val: 1.2003 (69.17%) | Best: 69.63%\n",
            "Epoch 270 | T: 27s | Train: 1.8596 (56.5%) | Phys: 0.59 | Val: 1.2015 (69.72%) | Best: 69.72%\n",
            "Epoch 271 | T: 27s | Train: 1.8543 (56.8%) | Phys: 0.59 | Val: 1.1778 (69.75%) | Best: 69.75%\n",
            "Epoch 272 | T: 27s | Train: 1.8527 (56.9%) | Phys: 0.59 | Val: 1.2065 (69.08%) | Best: 69.75%\n",
            "Epoch 273 | T: 27s | Train: 1.8760 (56.6%) | Phys: 0.60 | Val: 1.2300 (68.82%) | Best: 69.75%\n",
            "Epoch 274 | T: 27s | Train: 1.8793 (56.2%) | Phys: 0.59 | Val: 1.2198 (69.15%) | Best: 69.75%\n",
            "Epoch 275 | T: 27s | Train: 1.8942 (56.2%) | Phys: 0.60 | Val: 1.2227 (68.85%) | Best: 69.75%\n",
            "Epoch 276 | T: 27s | Train: 1.8864 (56.0%) | Phys: 0.60 | Val: 1.1819 (69.87%) | Best: 69.87%\n",
            "Epoch 277 | T: 27s | Train: 1.8474 (57.4%) | Phys: 0.59 | Val: 1.1847 (69.65%) | Best: 69.87%\n",
            "Epoch 278 | T: 27s | Train: 1.7975 (58.9%) | Phys: 0.60 | Val: 1.1814 (69.34%) | Best: 69.87%\n",
            "Epoch 279 | T: 27s | Train: 1.8367 (57.6%) | Phys: 0.59 | Val: 1.1897 (69.64%) | Best: 69.87%\n",
            "Epoch 280 | T: 27s | Train: 1.8250 (57.9%) | Phys: 0.60 | Val: 1.1823 (69.95%) | Best: 69.95%\n",
            "Epoch 281 | T: 27s | Train: 1.7948 (58.9%) | Phys: 0.59 | Val: 1.1992 (69.53%) | Best: 69.95%\n",
            "Epoch 282 | T: 27s | Train: 1.8725 (56.8%) | Phys: 0.60 | Val: 1.1941 (69.83%) | Best: 69.95%\n",
            "Epoch 283 | T: 27s | Train: 1.8391 (57.4%) | Phys: 0.60 | Val: 1.2016 (69.93%) | Best: 69.95%\n",
            "Epoch 284 | T: 27s | Train: 1.8308 (57.8%) | Phys: 0.60 | Val: 1.1902 (69.71%) | Best: 69.95%\n",
            "Epoch 285 | T: 27s | Train: 1.8193 (58.1%) | Phys: 0.60 | Val: 1.2030 (69.26%) | Best: 69.95%\n",
            "Epoch 286 | T: 27s | Train: 1.8234 (57.9%) | Phys: 0.60 | Val: 1.1961 (69.53%) | Best: 69.95%\n",
            "Epoch 287 | T: 27s | Train: 1.8816 (56.6%) | Phys: 0.60 | Val: 1.1927 (69.51%) | Best: 69.95%\n",
            "Epoch 288 | T: 27s | Train: 1.8501 (57.3%) | Phys: 0.60 | Val: 1.1839 (69.75%) | Best: 69.95%\n",
            "Epoch 289 | T: 27s | Train: 1.8372 (57.4%) | Phys: 0.60 | Val: 1.1771 (69.76%) | Best: 69.95%\n",
            "Epoch 290 | T: 27s | Train: 1.8307 (57.7%) | Phys: 0.60 | Val: 1.1818 (69.73%) | Best: 69.95%\n",
            "Epoch 291 | T: 27s | Train: 1.7669 (59.6%) | Phys: 0.60 | Val: 1.2032 (69.66%) | Best: 69.95%\n",
            "Epoch 292 | T: 27s | Train: 1.8269 (57.6%) | Phys: 0.60 | Val: 1.1935 (69.80%) | Best: 69.95%\n",
            "Epoch 293 | T: 27s | Train: 1.8691 (56.4%) | Phys: 0.60 | Val: 1.1724 (70.26%) | Best: 70.26%\n",
            "Epoch 294 | T: 27s | Train: 1.7826 (58.8%) | Phys: 0.60 | Val: 1.1580 (70.33%) | Best: 70.33%\n",
            "Epoch 295 | T: 27s | Train: 1.8059 (58.4%) | Phys: 0.60 | Val: 1.1897 (69.52%) | Best: 70.33%\n",
            "Epoch 296 | T: 27s | Train: 1.7294 (60.4%) | Phys: 0.60 | Val: 1.1931 (69.63%) | Best: 70.33%\n",
            "Epoch 297 | T: 27s | Train: 1.7605 (59.7%) | Phys: 0.60 | Val: 1.1741 (69.81%) | Best: 70.33%\n",
            "Epoch 298 | T: 27s | Train: 1.7857 (59.1%) | Phys: 0.60 | Val: 1.1801 (69.94%) | Best: 70.33%\n",
            "Epoch 299 | T: 27s | Train: 1.8352 (57.8%) | Phys: 0.60 | Val: 1.1791 (69.86%) | Best: 70.33%\n",
            "Epoch 300 | T: 27s | Train: 1.8605 (56.9%) | Phys: 0.60 | Val: 1.1715 (70.56%) | Best: 70.56%\n",
            "Epoch 301 | T: 27s | Train: 1.7850 (59.1%) | Phys: 0.60 | Val: 1.1879 (70.24%) | Best: 70.56%\n",
            "Epoch 302 | T: 27s | Train: 1.8360 (57.5%) | Phys: 0.60 | Val: 1.1831 (70.16%) | Best: 70.56%\n",
            "Epoch 303 | T: 27s | Train: 1.8338 (57.5%) | Phys: 0.60 | Val: 1.1701 (70.54%) | Best: 70.56%\n",
            "Epoch 304 | T: 27s | Train: 1.8457 (57.2%) | Phys: 0.60 | Val: 1.1954 (70.57%) | Best: 70.57%\n",
            "Epoch 305 | T: 27s | Train: 1.7966 (59.0%) | Phys: 0.60 | Val: 1.1821 (70.11%) | Best: 70.57%\n",
            "Epoch 306 | T: 27s | Train: 1.7760 (58.8%) | Phys: 0.60 | Val: 1.1745 (70.52%) | Best: 70.57%\n",
            "Epoch 307 | T: 27s | Train: 1.7581 (59.5%) | Phys: 0.60 | Val: 1.1927 (70.10%) | Best: 70.57%\n",
            "Epoch 308 | T: 27s | Train: 1.8520 (57.0%) | Phys: 0.60 | Val: 1.1611 (70.52%) | Best: 70.57%\n",
            "Epoch 309 | T: 27s | Train: 1.7592 (59.7%) | Phys: 0.60 | Val: 1.1571 (70.30%) | Best: 70.57%\n",
            "Epoch 310 | T: 27s | Train: 1.7586 (59.9%) | Phys: 0.60 | Val: 1.1832 (70.14%) | Best: 70.57%\n",
            "Epoch 311 | T: 27s | Train: 1.7745 (59.2%) | Phys: 0.60 | Val: 1.1659 (70.58%) | Best: 70.58%\n",
            "Epoch 312 | T: 27s | Train: 1.8574 (56.6%) | Phys: 0.60 | Val: 1.1813 (70.25%) | Best: 70.58%\n",
            "Epoch 313 | T: 27s | Train: 1.8472 (57.4%) | Phys: 0.60 | Val: 1.1692 (70.31%) | Best: 70.58%\n",
            "Epoch 314 | T: 27s | Train: 1.7955 (58.9%) | Phys: 0.60 | Val: 1.1755 (70.07%) | Best: 70.58%\n",
            "Epoch 315 | T: 27s | Train: 1.7972 (58.7%) | Phys: 0.60 | Val: 1.1779 (70.10%) | Best: 70.58%\n",
            "Epoch 316 | T: 27s | Train: 1.7708 (59.3%) | Phys: 0.60 | Val: 1.1581 (70.15%) | Best: 70.58%\n",
            "Epoch 317 | T: 27s | Train: 1.7100 (61.2%) | Phys: 0.60 | Val: 1.1627 (70.24%) | Best: 70.58%\n",
            "Epoch 318 | T: 27s | Train: 1.7284 (61.3%) | Phys: 0.60 | Val: 1.1772 (69.92%) | Best: 70.58%\n",
            "Epoch 319 | T: 27s | Train: 1.7043 (61.4%) | Phys: 0.60 | Val: 1.1677 (70.54%) | Best: 70.58%\n",
            "Epoch 320 | T: 27s | Train: 1.7778 (59.4%) | Phys: 0.60 | Val: 1.1601 (70.58%) | Best: 70.58%\n",
            "Epoch 321 | T: 27s | Train: 1.7254 (60.4%) | Phys: 0.60 | Val: 1.1576 (70.56%) | Best: 70.58%\n",
            "Epoch 322 | T: 27s | Train: 1.7841 (59.0%) | Phys: 0.60 | Val: 1.1625 (70.44%) | Best: 70.58%\n",
            "Epoch 323 | T: 27s | Train: 1.7896 (59.0%) | Phys: 0.60 | Val: 1.1552 (70.48%) | Best: 70.58%\n",
            "Epoch 324 | T: 27s | Train: 1.7850 (58.9%) | Phys: 0.60 | Val: 1.1775 (70.40%) | Best: 70.58%\n",
            "Epoch 325 | T: 27s | Train: 1.7120 (61.0%) | Phys: 0.60 | Val: 1.1645 (70.50%) | Best: 70.58%\n",
            "Epoch 326 | T: 27s | Train: 1.7163 (60.9%) | Phys: 0.60 | Val: 1.1720 (70.38%) | Best: 70.58%\n",
            "Epoch 327 | T: 27s | Train: 1.8036 (58.2%) | Phys: 0.60 | Val: 1.1618 (70.61%) | Best: 70.61%\n",
            "Epoch 328 | T: 27s | Train: 1.7553 (59.8%) | Phys: 0.60 | Val: 1.1576 (70.63%) | Best: 70.63%\n",
            "Epoch 329 | T: 27s | Train: 1.7455 (60.1%) | Phys: 0.60 | Val: 1.1422 (70.94%) | Best: 70.94%\n",
            "Epoch 330 | T: 27s | Train: 1.7244 (60.7%) | Phys: 0.60 | Val: 1.1602 (70.76%) | Best: 70.94%\n",
            "Epoch 331 | T: 28s | Train: 1.7178 (61.0%) | Phys: 0.60 | Val: 1.1642 (70.72%) | Best: 70.94%\n",
            "Epoch 332 | T: 27s | Train: 1.7414 (59.7%) | Phys: 0.60 | Val: 1.1587 (70.76%) | Best: 70.94%\n",
            "Epoch 333 | T: 27s | Train: 1.7633 (59.5%) | Phys: 0.60 | Val: 1.1875 (70.34%) | Best: 70.94%\n",
            "Epoch 334 | T: 27s | Train: 1.7343 (60.2%) | Phys: 0.60 | Val: 1.1726 (70.97%) | Best: 70.97%\n",
            "Epoch 335 | T: 27s | Train: 1.7571 (59.4%) | Phys: 0.60 | Val: 1.1455 (70.88%) | Best: 70.97%\n",
            "Epoch 336 | T: 27s | Train: 1.7562 (59.8%) | Phys: 0.60 | Val: 1.1600 (70.93%) | Best: 70.97%\n",
            "Epoch 337 | T: 27s | Train: 1.6696 (62.2%) | Phys: 0.60 | Val: 1.1480 (70.86%) | Best: 70.97%\n",
            "Epoch 338 | T: 28s | Train: 1.7413 (60.2%) | Phys: 0.60 | Val: 1.1550 (70.92%) | Best: 70.97%\n",
            "Epoch 339 | T: 27s | Train: 1.6998 (61.5%) | Phys: 0.60 | Val: 1.1436 (70.72%) | Best: 70.97%\n",
            "Epoch 340 | T: 27s | Train: 1.6938 (61.5%) | Phys: 0.60 | Val: 1.1441 (71.07%) | Best: 71.07%\n",
            "Epoch 341 | T: 27s | Train: 1.6634 (62.2%) | Phys: 0.60 | Val: 1.1508 (71.07%) | Best: 71.07%\n",
            "Epoch 342 | T: 27s | Train: 1.7402 (60.5%) | Phys: 0.60 | Val: 1.1535 (70.93%) | Best: 71.07%\n",
            "Epoch 343 | T: 27s | Train: 1.7341 (60.7%) | Phys: 0.60 | Val: 1.1608 (70.85%) | Best: 71.07%\n",
            "Epoch 344 | T: 27s | Train: 1.6942 (61.6%) | Phys: 0.60 | Val: 1.1741 (70.50%) | Best: 71.07%\n",
            "Epoch 345 | T: 27s | Train: 1.7229 (60.9%) | Phys: 0.60 | Val: 1.1587 (70.84%) | Best: 71.07%\n",
            "Epoch 346 | T: 27s | Train: 1.7183 (61.0%) | Phys: 0.60 | Val: 1.1505 (70.72%) | Best: 71.07%\n",
            "Epoch 347 | T: 27s | Train: 1.7223 (60.9%) | Phys: 0.60 | Val: 1.1450 (71.24%) | Best: 71.24%\n",
            "Epoch 348 | T: 27s | Train: 1.8134 (58.4%) | Phys: 0.60 | Val: 1.1620 (70.83%) | Best: 71.24%\n",
            "Epoch 349 | T: 27s | Train: 1.6941 (61.7%) | Phys: 0.60 | Val: 1.1523 (70.90%) | Best: 71.24%\n",
            "Epoch 350 | T: 27s | Train: 1.6796 (62.0%) | Phys: 0.60 | Val: 1.1644 (70.64%) | Best: 71.24%\n",
            "Epoch 351 | T: 27s | Train: 1.7461 (60.1%) | Phys: 0.60 | Val: 1.1365 (70.83%) | Best: 71.24%\n",
            "Epoch 352 | T: 27s | Train: 1.6935 (61.7%) | Phys: 0.60 | Val: 1.1672 (70.48%) | Best: 71.24%\n",
            "Epoch 353 | T: 27s | Train: 1.6922 (61.8%) | Phys: 0.60 | Val: 1.1481 (70.87%) | Best: 71.24%\n",
            "Epoch 354 | T: 27s | Train: 1.7134 (61.0%) | Phys: 0.60 | Val: 1.1486 (71.27%) | Best: 71.27%\n",
            "Epoch 355 | T: 27s | Train: 1.7421 (59.9%) | Phys: 0.60 | Val: 1.1572 (70.93%) | Best: 71.27%\n",
            "Epoch 356 | T: 27s | Train: 1.7091 (61.2%) | Phys: 0.60 | Val: 1.1552 (70.94%) | Best: 71.27%\n",
            "Epoch 357 | T: 27s | Train: 1.7022 (61.5%) | Phys: 0.60 | Val: 1.1594 (70.77%) | Best: 71.27%\n",
            "Epoch 358 | T: 27s | Train: 1.6751 (62.1%) | Phys: 0.60 | Val: 1.1431 (71.18%) | Best: 71.27%\n",
            "Epoch 359 | T: 27s | Train: 1.7317 (60.7%) | Phys: 0.60 | Val: 1.1553 (71.24%) | Best: 71.27%\n",
            "Epoch 360 | T: 27s | Train: 1.6946 (61.4%) | Phys: 0.60 | Val: 1.1516 (71.17%) | Best: 71.27%\n",
            "Epoch 361 | T: 27s | Train: 1.7220 (60.4%) | Phys: 0.60 | Val: 1.1410 (71.39%) | Best: 71.39%\n",
            "Epoch 362 | T: 27s | Train: 1.7325 (60.3%) | Phys: 0.60 | Val: 1.1417 (71.31%) | Best: 71.39%\n",
            "Epoch 363 | T: 27s | Train: 1.6458 (62.7%) | Phys: 0.60 | Val: 1.1446 (71.17%) | Best: 71.39%\n",
            "Epoch 364 | T: 27s | Train: 1.7527 (59.8%) | Phys: 0.60 | Val: 1.1454 (71.18%) | Best: 71.39%\n",
            "Epoch 365 | T: 27s | Train: 1.6198 (63.6%) | Phys: 0.60 | Val: 1.1475 (71.13%) | Best: 71.39%\n",
            "Epoch 366 | T: 27s | Train: 1.6248 (63.4%) | Phys: 0.60 | Val: 1.1446 (71.35%) | Best: 71.39%\n",
            "Epoch 367 | T: 27s | Train: 1.6853 (61.5%) | Phys: 0.60 | Val: 1.1533 (71.10%) | Best: 71.39%\n",
            "Epoch 368 | T: 27s | Train: 1.6798 (61.8%) | Phys: 0.60 | Val: 1.1389 (71.26%) | Best: 71.39%\n",
            "Epoch 369 | T: 27s | Train: 1.6772 (61.7%) | Phys: 0.60 | Val: 1.1414 (71.26%) | Best: 71.39%\n",
            "Epoch 370 | T: 27s | Train: 1.7154 (61.0%) | Phys: 0.60 | Val: 1.1475 (71.25%) | Best: 71.39%\n",
            "Epoch 371 | T: 27s | Train: 1.6901 (61.3%) | Phys: 0.60 | Val: 1.1458 (71.31%) | Best: 71.39%\n",
            "Epoch 372 | T: 27s | Train: 1.7608 (59.7%) | Phys: 0.60 | Val: 1.1468 (71.49%) | Best: 71.49%\n",
            "Epoch 373 | T: 27s | Train: 1.6987 (60.9%) | Phys: 0.60 | Val: 1.1417 (71.48%) | Best: 71.49%\n",
            "Epoch 374 | T: 27s | Train: 1.6973 (61.2%) | Phys: 0.60 | Val: 1.1374 (71.65%) | Best: 71.65%\n",
            "Epoch 375 | T: 27s | Train: 1.7205 (60.3%) | Phys: 0.60 | Val: 1.1416 (71.29%) | Best: 71.65%\n",
            "Epoch 376 | T: 27s | Train: 1.6617 (62.7%) | Phys: 0.60 | Val: 1.1464 (71.44%) | Best: 71.65%\n",
            "Epoch 377 | T: 27s | Train: 1.6694 (62.4%) | Phys: 0.60 | Val: 1.1442 (71.46%) | Best: 71.65%\n",
            "Epoch 378 | T: 27s | Train: 1.6153 (63.7%) | Phys: 0.60 | Val: 1.1451 (71.30%) | Best: 71.65%\n",
            "Epoch 379 | T: 27s | Train: 1.7294 (60.6%) | Phys: 0.60 | Val: 1.1331 (71.53%) | Best: 71.65%\n",
            "Epoch 380 | T: 27s | Train: 1.6316 (63.0%) | Phys: 0.60 | Val: 1.1273 (71.47%) | Best: 71.65%\n",
            "Epoch 381 | T: 27s | Train: 1.7167 (60.6%) | Phys: 0.60 | Val: 1.1403 (71.31%) | Best: 71.65%\n",
            "Epoch 382 | T: 27s | Train: 1.6568 (62.5%) | Phys: 0.60 | Val: 1.1386 (71.39%) | Best: 71.65%\n",
            "Epoch 383 | T: 27s | Train: 1.5931 (63.9%) | Phys: 0.60 | Val: 1.1358 (71.33%) | Best: 71.65%\n",
            "Epoch 384 | T: 27s | Train: 1.6947 (61.7%) | Phys: 0.60 | Val: 1.1547 (71.32%) | Best: 71.65%\n",
            "Epoch 385 | T: 27s | Train: 1.6962 (61.4%) | Phys: 0.60 | Val: 1.1425 (71.46%) | Best: 71.65%\n",
            "Epoch 386 | T: 27s | Train: 1.6859 (61.7%) | Phys: 0.60 | Val: 1.1449 (71.37%) | Best: 71.65%\n",
            "Epoch 387 | T: 27s | Train: 1.6672 (62.1%) | Phys: 0.60 | Val: 1.1359 (71.42%) | Best: 71.65%\n",
            "Epoch 388 | T: 27s | Train: 1.7571 (59.9%) | Phys: 0.60 | Val: 1.1448 (71.36%) | Best: 71.65%\n",
            "Epoch 389 | T: 27s | Train: 1.6766 (62.1%) | Phys: 0.60 | Val: 1.1404 (71.11%) | Best: 71.65%\n",
            "Epoch 390 | T: 27s | Train: 1.7020 (61.0%) | Phys: 0.60 | Val: 1.1464 (71.42%) | Best: 71.65%\n",
            "Epoch 391 | T: 27s | Train: 1.7189 (61.0%) | Phys: 0.60 | Val: 1.1458 (71.39%) | Best: 71.65%\n",
            "Epoch 392 | T: 27s | Train: 1.6443 (63.1%) | Phys: 0.60 | Val: 1.1405 (71.40%) | Best: 71.65%\n",
            "Epoch 393 | T: 27s | Train: 1.7101 (61.1%) | Phys: 0.60 | Val: 1.1429 (71.44%) | Best: 71.65%\n",
            "Epoch 394 | T: 27s | Train: 1.6388 (63.2%) | Phys: 0.60 | Val: 1.1444 (71.35%) | Best: 71.65%\n",
            "Epoch 395 | T: 27s | Train: 1.7415 (60.2%) | Phys: 0.60 | Val: 1.1484 (71.21%) | Best: 71.65%\n",
            "Epoch 396 | T: 27s | Train: 1.6779 (61.6%) | Phys: 0.60 | Val: 1.1353 (71.48%) | Best: 71.65%\n",
            "Epoch 397 | T: 27s | Train: 1.6668 (62.3%) | Phys: 0.60 | Val: 1.1419 (71.20%) | Best: 71.65%\n",
            "Epoch 398 | T: 27s | Train: 1.6446 (62.5%) | Phys: 0.60 | Val: 1.1425 (71.14%) | Best: 71.65%\n",
            "Epoch 399 | T: 27s | Train: 1.6159 (63.5%) | Phys: 0.60 | Val: 1.1406 (71.21%) | Best: 71.65%\n",
            "Epoch 400 | T: 27s | Train: 1.7616 (59.5%) | Phys: 0.60 | Val: 1.1358 (71.36%) | Best: 71.65%\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": [],
      "name": "CIFAR-100 ViT SIGReg Weak",
      "gpuType": "L4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}