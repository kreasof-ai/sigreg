{
  "cells": [
    {
      "cell_type": "code",
      "id": "3amcbKc5hLopVnnXSD9Wt1yi",
      "metadata": {
        "tags": [],
        "id": "3amcbKc5hLopVnnXSD9Wt1yi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce056f64-087c-4416-ca89-05f1ea23336c"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "\n",
        "# ==========================================\n",
        "# 1. Configuration & Physics\n",
        "# ==========================================\n",
        "# OPTIONS: 'baseline', 'weak' (Covariance), 'strong' (LeJEPA/Epps-Pulley)\n",
        "REG_MODE = 'weak'\n",
        "SIGR_ALPHA = 0.1   # Strength of the physics constraint\n",
        "SKETCH_DIM = 64    # Dimension of the random observer\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "LEARNING_RATE = 0.1\n",
        "EPOCHS = 400\n",
        "WEIGHT_DECAY = 5e-4\n",
        "MOMENTUM = 0.9\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "MIXUP_ALPHA = 0.8\n",
        "CUTMIX_ALPHA = 1.0\n",
        "\n",
        "# Check for Apple Silicon (MPS)\n",
        "if torch.backends.mps.is_available():\n",
        "    DEVICE = 'mps'\n",
        "\n",
        "print(f\"Training on: {DEVICE} | Mode: {REG_MODE} | Alpha: {SIGR_ALPHA}\")\n",
        "\n",
        "def rand_bbox(size, lam):\n",
        "    W = size[2]\n",
        "    H = size[3]\n",
        "    cut_rat = np.sqrt(1. - lam)\n",
        "    cut_w = int(W * cut_rat)\n",
        "    cut_h = int(H * cut_rat)\n",
        "\n",
        "    cx = np.random.randint(W)\n",
        "    cy = np.random.randint(H)\n",
        "\n",
        "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
        "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
        "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
        "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
        "\n",
        "    return bbx1, bby1, bbx2, bby2\n",
        "\n",
        "# ------------------------------------------\n",
        "# Physics Engine: The Regularizers\n",
        "# ------------------------------------------\n",
        "\n",
        "def sigreg_weak_loss(x, sketch_dim=64):\n",
        "    \"\"\"\n",
        "    Forces Covariance(x) ~ Identity.\n",
        "    Matches the 2nd Moment (Spherical Cloud).\n",
        "    \"\"\"\n",
        "    N, C = x.size()\n",
        "    # 1. Sketching (Optional for C=512, but good for consistency)\n",
        "    if C > sketch_dim:\n",
        "        S = torch.randn(sketch_dim, C, device=x.device) / (C ** 0.5)\n",
        "        x = x @ S.T  # [N, sketch_dim]\n",
        "    else:\n",
        "        sketch_dim = C\n",
        "\n",
        "    # 2. Centering & Covariance\n",
        "    x = x - x.mean(dim=0, keepdim=True)\n",
        "    cov = (x.T @ x) / (N - 1 + 1e-6)\n",
        "\n",
        "    # 3. Target Identity\n",
        "    target = torch.eye(sketch_dim, device=x.device)\n",
        "\n",
        "    # 4. Off-diagonal suppression + Diagonal maintenance\n",
        "    return torch.norm(cov - target, p='fro')\n",
        "\n",
        "def sigreg_strong_loss(x, sketch_dim=64):\n",
        "    \"\"\"\n",
        "    Forces ECF(x) ~ ECF(Gaussian).\n",
        "    Matches ALL Moments (Maximum Entropy Cloud).\n",
        "    Exact implementation of LeJEPA Algorithm 1.\n",
        "    \"\"\"\n",
        "    N, C = x.size()\n",
        "\n",
        "    # 1. Projection (The Observer)\n",
        "    # Project channels down to sketch_dim\n",
        "    A = torch.randn(C, sketch_dim, device=x.device)\n",
        "    A = A / (A.norm(p=2, dim=0, keepdim=True) + 1e-6)\n",
        "\n",
        "    # 2. Integration Points\n",
        "    t = torch.linspace(-5, 5, 17, device=x.device)\n",
        "\n",
        "    # 3. Theoretical Gaussian CF\n",
        "    exp_f = torch.exp(-0.5 * t**2)\n",
        "\n",
        "    # 4. Empirical CF\n",
        "    # proj: [N, sketch_dim]\n",
        "    proj = x @ A\n",
        "\n",
        "    # args: [N, sketch_dim, T]\n",
        "    args = proj.unsqueeze(2) * t.view(1, 1, -1)\n",
        "\n",
        "    # ecf: [sketch_dim, T] (Mean over batch)\n",
        "    ecf = torch.exp(1j * args).mean(dim=0)\n",
        "\n",
        "    # 5. Weighted L2 Distance\n",
        "    # |ecf - gauss|^2 * gauss_weight\n",
        "    diff_sq = (ecf - exp_f.unsqueeze(0)).abs().square()\n",
        "    err = diff_sq * exp_f.unsqueeze(0)\n",
        "\n",
        "    # 6. Integrate\n",
        "    loss = torch.trapz(err, t, dim=1) * N\n",
        "\n",
        "    return loss.mean()\n",
        "\n",
        "# ==========================================\n",
        "# 2. Thermodynamic ResNet-18\n",
        "# ==========================================\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "\n",
        "        # --- PHYSICS INJECTION ---\n",
        "        # We calculate loss here, but we need to return it to the main model.\n",
        "        # We use Global Average Pooling to get a (N, C) vector for regularization\n",
        "        # This forces the semantic features to be isotropic.\n",
        "        reg_loss = torch.tensor(0.0, device=x.device)\n",
        "\n",
        "        if REG_MODE != 'baseline':\n",
        "            # Pool spatial dims: [N, C, H, W] -> [N, C]\n",
        "            flat = F.adaptive_avg_pool2d(out, (1, 1)).view(out.size(0), -1)\n",
        "\n",
        "            if REG_MODE == 'weak':\n",
        "                reg_loss = sigreg_weak_loss(flat, SKETCH_DIM)\n",
        "            elif REG_MODE == 'strong':\n",
        "                reg_loss = sigreg_strong_loss(flat, SKETCH_DIM)\n",
        "\n",
        "        return out, reg_loss\n",
        "\n",
        "class ThermoResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=100):\n",
        "        super(ThermoResNet, self).__init__()\n",
        "        self.in_planes = 64\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.ModuleList(layers) # Changed to ModuleList to iterate manually\n",
        "\n",
        "    def forward(self, x):\n",
        "        total_phys_loss = 0.0\n",
        "\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "\n",
        "        # Manually iterate through layers to collect physics losses\n",
        "        for layer_group in [self.layer1, self.layer2, self.layer3, self.layer4]:\n",
        "            for block in layer_group:\n",
        "                out, l_loss = block(out)\n",
        "                total_phys_loss += l_loss\n",
        "\n",
        "        out = F.avg_pool2d(out, 4)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "\n",
        "        # Normalize p_loss by number of layers to keep scale consistent\n",
        "        return out, (total_phys_loss / 8.0)\n",
        "\n",
        "def ResNet18():\n",
        "    return ThermoResNet(BasicBlock, [2, 2, 2, 2])\n",
        "\n",
        "# ==========================================\n",
        "# 3. Data Preparation\n",
        "# ==========================================\n",
        "def get_data_loaders():\n",
        "    print('==> Preparing data...')\n",
        "    mean = (0.5070751592371323, 0.48654887331495095, 0.4409178433670343)\n",
        "    std = (0.2673342858792401, 0.2564384629170883, 0.27615047132568404)\n",
        "\n",
        "    transform_train = transforms.Compose([\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean, std),\n",
        "    ])\n",
        "\n",
        "    transform_test = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean, std),\n",
        "    ])\n",
        "\n",
        "    trainset = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=transform_train)\n",
        "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
        "\n",
        "    testset = torchvision.datasets.CIFAR100(root='./data', train=False, download=True, transform=transform_test)\n",
        "    testloader = torch.utils.data.DataLoader(testset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
        "\n",
        "    return trainloader, testloader\n",
        "\n",
        "# ==========================================\n",
        "# 4. Training Engine\n",
        "# ==========================================\n",
        "def train(epoch, net, trainloader, optimizer, criterion):\n",
        "    net.train()\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    phys_loss_meter = 0\n",
        "\n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
        "\n",
        "        # Apply Mixup/CutMix\n",
        "        r = np.random.rand(1)\n",
        "        if r < 0.5: # Mixup\n",
        "            lam = np.random.beta(MIXUP_ALPHA, MIXUP_ALPHA)\n",
        "            index = torch.randperm(inputs.size(0)).to(DEVICE)\n",
        "            inputs = lam * inputs + (1 - lam) * inputs[index, :]\n",
        "            targets_a, targets_b = targets, targets[index]\n",
        "        else: # CutMix\n",
        "            lam = np.random.beta(CUTMIX_ALPHA, CUTMIX_ALPHA)\n",
        "            rand_index = torch.randperm(inputs.size(0)).to(DEVICE)\n",
        "            target_a = targets\n",
        "            target_b = targets[rand_index]\n",
        "            bbx1, bby1, bbx2, bby2 = rand_bbox(inputs.size(), lam)\n",
        "            inputs[:, :, bbx1:bbx2, bby1:bby2] = inputs[rand_index, :, bbx1:bbx2, bby1:bby2]\n",
        "            lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (inputs.size()[-1] * inputs.size()[-2]))\n",
        "            targets_a, targets_b = target_a, target_b\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward\n",
        "        outputs, p_loss = net(inputs)\n",
        "\n",
        "        # Task Loss\n",
        "        c_loss = criterion(outputs, targets_a) * lam + criterion(outputs, targets_b) * (1. - lam)\n",
        "\n",
        "        # Total Loss\n",
        "        loss = (1 - SIGR_ALPHA) * c_loss + (SIGR_ALPHA * p_loss)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += ((1 - SIGR_ALPHA) * c_loss).item() # Log only task loss for comparison\n",
        "        phys_loss_meter += (SIGR_ALPHA * p_loss).item()\n",
        "\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += (lam * predicted.eq(targets_a).float() + (1 - lam) * predicted.eq(targets_b).float()).sum().item()\n",
        "\n",
        "    acc = 100. * correct / total\n",
        "    return train_loss / (batch_idx + 1), acc, phys_loss_meter / (batch_idx + 1)\n",
        "\n",
        "def test(epoch, net, testloader, criterion):\n",
        "    net.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "            inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
        "            outputs, _ = net(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "    acc = 100. * correct / total\n",
        "    return test_loss / (batch_idx + 1), acc\n",
        "\n",
        "# ==========================================\n",
        "# 5. Main Execution\n",
        "# ==========================================\n",
        "if __name__ == '__main__':\n",
        "    trainloader, testloader = get_data_loaders()\n",
        "\n",
        "    print(f'==> Building model (Mode: {REG_MODE})...')\n",
        "    net = ResNet18()\n",
        "    net = net.to(DEVICE)\n",
        "\n",
        "    if DEVICE == 'cuda':\n",
        "        net = torch.nn.DataParallel(net)\n",
        "        cudnn.benchmark = True\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(net.parameters(), lr=LEARNING_RATE, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
        "\n",
        "    print(f\"Starting training for {EPOCHS} epochs...\")\n",
        "    best_acc = 0\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        start_time = time.time()\n",
        "\n",
        "        tr_loss, tr_acc, phys_loss = train(epoch, net, trainloader, optimizer, criterion)\n",
        "        te_loss, te_acc = test(epoch, net, testloader, criterion)\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        if te_acc > best_acc:\n",
        "            best_acc = te_acc\n",
        "            # torch.save(net.state_dict(), f'thermo_resnet_{REG_MODE}.pth')\n",
        "\n",
        "        epoch_time = time.time() - start_time\n",
        "\n",
        "        print(f\"Epoch {epoch+1:03d} | T: {epoch_time:.0f}s | \"\n",
        "              f\"Train: {tr_loss:.4f} ({tr_acc:.1f}%) | \"\n",
        "              f\"Phys: {phys_loss:.2f} | \"\n",
        "              f\"Val: {te_loss:.4f} ({te_acc:.2f}%) | \"\n",
        "              f\"Best: {best_acc:.2f}%\")\n",
        "\n",
        "    print(f\"Final Best: {best_acc:.2f}%\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on: cuda | Mode: weak | Alpha: 0.1\n",
            "==> Preparing data...\n",
            "==> Building model (Mode: weak)...\n",
            "Starting training for 400 epochs...\n",
            "Epoch 001 | T: 23s | Train: 3.8172 (6.9%) | Phys: 0.77 | Val: 3.5302 (14.98%) | Best: 14.98%\n",
            "Epoch 002 | T: 22s | Train: 3.4903 (12.2%) | Phys: 0.76 | Val: 3.2082 (21.83%) | Best: 21.83%\n",
            "Epoch 003 | T: 22s | Train: 3.2803 (16.7%) | Phys: 0.77 | Val: 2.7725 (29.39%) | Best: 29.39%\n",
            "Epoch 004 | T: 22s | Train: 3.0618 (21.9%) | Phys: 0.77 | Val: 2.5015 (35.11%) | Best: 35.11%\n",
            "Epoch 005 | T: 22s | Train: 2.9708 (24.2%) | Phys: 0.78 | Val: 2.7297 (30.76%) | Best: 35.11%\n",
            "Epoch 006 | T: 22s | Train: 2.8530 (27.6%) | Phys: 0.78 | Val: 2.2589 (41.57%) | Best: 41.57%\n",
            "Epoch 007 | T: 22s | Train: 2.7679 (29.5%) | Phys: 0.79 | Val: 2.0483 (46.48%) | Best: 46.48%\n",
            "Epoch 008 | T: 22s | Train: 2.6863 (31.8%) | Phys: 0.79 | Val: 2.0472 (46.11%) | Best: 46.48%\n",
            "Epoch 009 | T: 22s | Train: 2.6734 (32.5%) | Phys: 0.79 | Val: 1.9998 (47.39%) | Best: 47.39%\n",
            "Epoch 010 | T: 22s | Train: 2.5784 (34.5%) | Phys: 0.79 | Val: 1.9938 (46.99%) | Best: 47.39%\n",
            "Epoch 011 | T: 22s | Train: 2.6013 (34.4%) | Phys: 0.79 | Val: 1.7570 (52.16%) | Best: 52.16%\n",
            "Epoch 012 | T: 22s | Train: 2.5812 (35.1%) | Phys: 0.79 | Val: 1.8441 (51.38%) | Best: 52.16%\n",
            "Epoch 013 | T: 22s | Train: 2.5482 (35.9%) | Phys: 0.79 | Val: 1.7925 (52.74%) | Best: 52.74%\n",
            "Epoch 014 | T: 22s | Train: 2.5090 (37.0%) | Phys: 0.79 | Val: 2.0511 (47.17%) | Best: 52.74%\n",
            "Epoch 015 | T: 22s | Train: 2.5185 (36.8%) | Phys: 0.79 | Val: 2.1032 (46.50%) | Best: 52.74%\n",
            "Epoch 016 | T: 22s | Train: 2.4588 (38.2%) | Phys: 0.79 | Val: 2.0021 (48.19%) | Best: 52.74%\n",
            "Epoch 017 | T: 21s | Train: 2.4823 (37.8%) | Phys: 0.79 | Val: 1.6955 (54.61%) | Best: 54.61%\n",
            "Epoch 018 | T: 22s | Train: 2.4971 (37.7%) | Phys: 0.79 | Val: 1.8719 (51.61%) | Best: 54.61%\n",
            "Epoch 019 | T: 22s | Train: 2.4564 (38.5%) | Phys: 0.79 | Val: 1.8117 (51.71%) | Best: 54.61%\n",
            "Epoch 020 | T: 22s | Train: 2.4557 (38.5%) | Phys: 0.79 | Val: 1.8915 (50.54%) | Best: 54.61%\n",
            "Epoch 021 | T: 22s | Train: 2.4328 (39.2%) | Phys: 0.79 | Val: 1.7848 (53.03%) | Best: 54.61%\n",
            "Epoch 022 | T: 22s | Train: 2.4284 (39.5%) | Phys: 0.79 | Val: 1.5702 (58.42%) | Best: 58.42%\n",
            "Epoch 023 | T: 22s | Train: 2.3528 (41.2%) | Phys: 0.79 | Val: 1.7981 (52.90%) | Best: 58.42%\n",
            "Epoch 024 | T: 22s | Train: 2.4026 (39.9%) | Phys: 0.79 | Val: 1.8330 (51.84%) | Best: 58.42%\n",
            "Epoch 025 | T: 21s | Train: 2.4533 (38.7%) | Phys: 0.79 | Val: 1.8576 (51.21%) | Best: 58.42%\n",
            "Epoch 026 | T: 22s | Train: 2.4317 (39.4%) | Phys: 0.79 | Val: 1.7958 (54.19%) | Best: 58.42%\n",
            "Epoch 027 | T: 22s | Train: 2.3883 (40.5%) | Phys: 0.79 | Val: 1.6591 (56.70%) | Best: 58.42%\n",
            "Epoch 028 | T: 22s | Train: 2.4351 (39.4%) | Phys: 0.79 | Val: 1.7021 (54.38%) | Best: 58.42%\n",
            "Epoch 029 | T: 22s | Train: 2.4221 (39.7%) | Phys: 0.79 | Val: 1.7276 (55.58%) | Best: 58.42%\n",
            "Epoch 030 | T: 22s | Train: 2.3131 (42.3%) | Phys: 0.79 | Val: 1.6510 (56.20%) | Best: 58.42%\n",
            "Epoch 031 | T: 22s | Train: 2.3649 (41.2%) | Phys: 0.79 | Val: 1.8055 (54.82%) | Best: 58.42%\n",
            "Epoch 032 | T: 22s | Train: 2.4093 (40.2%) | Phys: 0.79 | Val: 1.8535 (51.43%) | Best: 58.42%\n",
            "Epoch 033 | T: 22s | Train: 2.3627 (41.5%) | Phys: 0.79 | Val: 1.5696 (59.60%) | Best: 59.60%\n",
            "Epoch 034 | T: 22s | Train: 2.3486 (41.8%) | Phys: 0.79 | Val: 1.7091 (54.65%) | Best: 59.60%\n",
            "Epoch 035 | T: 22s | Train: 2.3411 (41.8%) | Phys: 0.79 | Val: 1.7524 (54.76%) | Best: 59.60%\n",
            "Epoch 036 | T: 22s | Train: 2.3511 (41.4%) | Phys: 0.79 | Val: 1.6593 (57.63%) | Best: 59.60%\n",
            "Epoch 037 | T: 21s | Train: 2.3069 (42.6%) | Phys: 0.79 | Val: 1.7242 (55.63%) | Best: 59.60%\n",
            "Epoch 038 | T: 22s | Train: 2.3380 (41.7%) | Phys: 0.79 | Val: 1.8208 (52.79%) | Best: 59.60%\n",
            "Epoch 039 | T: 22s | Train: 2.3119 (42.6%) | Phys: 0.79 | Val: 1.8042 (54.96%) | Best: 59.60%\n",
            "Epoch 040 | T: 22s | Train: 2.3868 (40.8%) | Phys: 0.79 | Val: 1.5740 (58.32%) | Best: 59.60%\n",
            "Epoch 041 | T: 22s | Train: 2.3359 (42.2%) | Phys: 0.79 | Val: 1.5769 (58.77%) | Best: 59.60%\n",
            "Epoch 042 | T: 22s | Train: 2.3121 (42.5%) | Phys: 0.79 | Val: 1.7785 (53.87%) | Best: 59.60%\n",
            "Epoch 043 | T: 22s | Train: 2.3462 (41.8%) | Phys: 0.79 | Val: 1.9249 (51.01%) | Best: 59.60%\n",
            "Epoch 044 | T: 22s | Train: 2.3572 (41.4%) | Phys: 0.79 | Val: 1.5849 (58.29%) | Best: 59.60%\n",
            "Epoch 045 | T: 22s | Train: 2.3307 (42.0%) | Phys: 0.79 | Val: 1.5422 (59.39%) | Best: 59.60%\n",
            "Epoch 046 | T: 22s | Train: 2.3148 (42.7%) | Phys: 0.79 | Val: 1.5928 (57.90%) | Best: 59.60%\n",
            "Epoch 047 | T: 22s | Train: 2.3572 (41.4%) | Phys: 0.79 | Val: 1.6992 (57.66%) | Best: 59.60%\n",
            "Epoch 048 | T: 22s | Train: 2.3097 (42.7%) | Phys: 0.79 | Val: 1.6563 (59.68%) | Best: 59.68%\n",
            "Epoch 049 | T: 22s | Train: 2.3339 (42.2%) | Phys: 0.79 | Val: 1.6756 (57.76%) | Best: 59.68%\n",
            "Epoch 050 | T: 22s | Train: 2.2802 (43.3%) | Phys: 0.79 | Val: 1.6394 (58.12%) | Best: 59.68%\n",
            "Epoch 051 | T: 22s | Train: 2.3632 (41.1%) | Phys: 0.79 | Val: 1.6109 (58.73%) | Best: 59.68%\n",
            "Epoch 052 | T: 22s | Train: 2.2948 (42.8%) | Phys: 0.79 | Val: 1.7534 (53.62%) | Best: 59.68%\n",
            "Epoch 053 | T: 22s | Train: 2.2779 (43.5%) | Phys: 0.79 | Val: 1.7558 (53.78%) | Best: 59.68%\n",
            "Epoch 054 | T: 22s | Train: 2.2629 (43.9%) | Phys: 0.79 | Val: 1.6097 (57.55%) | Best: 59.68%\n",
            "Epoch 055 | T: 22s | Train: 2.3044 (42.8%) | Phys: 0.79 | Val: 1.8704 (52.29%) | Best: 59.68%\n",
            "Epoch 056 | T: 22s | Train: 2.2910 (43.2%) | Phys: 0.79 | Val: 1.6326 (57.62%) | Best: 59.68%\n",
            "Epoch 057 | T: 22s | Train: 2.3009 (42.8%) | Phys: 0.79 | Val: 1.7539 (54.50%) | Best: 59.68%\n",
            "Epoch 058 | T: 22s | Train: 2.2746 (43.7%) | Phys: 0.79 | Val: 1.8848 (52.21%) | Best: 59.68%\n",
            "Epoch 059 | T: 21s | Train: 2.3109 (42.5%) | Phys: 0.79 | Val: 1.6130 (57.83%) | Best: 59.68%\n",
            "Epoch 060 | T: 22s | Train: 2.3318 (42.1%) | Phys: 0.79 | Val: 1.5758 (59.19%) | Best: 59.68%\n",
            "Epoch 061 | T: 22s | Train: 2.2772 (43.1%) | Phys: 0.79 | Val: 1.6212 (56.90%) | Best: 59.68%\n",
            "Epoch 062 | T: 22s | Train: 2.2689 (43.6%) | Phys: 0.79 | Val: 1.5937 (57.78%) | Best: 59.68%\n",
            "Epoch 063 | T: 22s | Train: 2.2399 (44.3%) | Phys: 0.79 | Val: 1.4221 (61.80%) | Best: 61.80%\n",
            "Epoch 064 | T: 22s | Train: 2.2141 (45.2%) | Phys: 0.79 | Val: 1.7489 (54.65%) | Best: 61.80%\n",
            "Epoch 065 | T: 22s | Train: 2.2719 (43.8%) | Phys: 0.79 | Val: 1.5942 (58.39%) | Best: 61.80%\n",
            "Epoch 066 | T: 22s | Train: 2.2631 (44.2%) | Phys: 0.79 | Val: 1.6147 (58.48%) | Best: 61.80%\n",
            "Epoch 067 | T: 22s | Train: 2.2390 (44.1%) | Phys: 0.79 | Val: 1.5109 (60.21%) | Best: 61.80%\n",
            "Epoch 068 | T: 22s | Train: 2.2550 (44.4%) | Phys: 0.79 | Val: 1.5161 (59.56%) | Best: 61.80%\n",
            "Epoch 069 | T: 22s | Train: 2.2949 (43.3%) | Phys: 0.79 | Val: 1.6280 (58.25%) | Best: 61.80%\n",
            "Epoch 070 | T: 22s | Train: 2.2343 (44.6%) | Phys: 0.79 | Val: 1.5730 (57.87%) | Best: 61.80%\n",
            "Epoch 071 | T: 22s | Train: 2.3161 (42.4%) | Phys: 0.79 | Val: 1.6264 (57.77%) | Best: 61.80%\n",
            "Epoch 072 | T: 22s | Train: 2.2822 (43.4%) | Phys: 0.79 | Val: 1.6081 (58.67%) | Best: 61.80%\n",
            "Epoch 073 | T: 22s | Train: 2.2416 (44.3%) | Phys: 0.79 | Val: 1.4843 (60.49%) | Best: 61.80%\n",
            "Epoch 074 | T: 22s | Train: 2.2614 (43.8%) | Phys: 0.79 | Val: 1.5509 (59.29%) | Best: 61.80%\n",
            "Epoch 075 | T: 22s | Train: 2.3236 (42.2%) | Phys: 0.79 | Val: 1.5997 (58.34%) | Best: 61.80%\n",
            "Epoch 076 | T: 22s | Train: 2.2083 (45.2%) | Phys: 0.79 | Val: 1.5944 (58.31%) | Best: 61.80%\n",
            "Epoch 077 | T: 22s | Train: 2.2666 (44.0%) | Phys: 0.79 | Val: 1.7619 (54.98%) | Best: 61.80%\n",
            "Epoch 078 | T: 22s | Train: 2.2816 (43.4%) | Phys: 0.79 | Val: 1.5824 (59.38%) | Best: 61.80%\n",
            "Epoch 079 | T: 22s | Train: 2.2724 (43.6%) | Phys: 0.79 | Val: 1.5462 (60.50%) | Best: 61.80%\n",
            "Epoch 080 | T: 22s | Train: 2.2874 (43.5%) | Phys: 0.79 | Val: 1.5232 (60.91%) | Best: 61.80%\n",
            "Epoch 081 | T: 22s | Train: 2.2089 (45.5%) | Phys: 0.79 | Val: 1.5205 (61.29%) | Best: 61.80%\n",
            "Epoch 082 | T: 22s | Train: 2.2453 (44.5%) | Phys: 0.79 | Val: 1.4979 (61.01%) | Best: 61.80%\n",
            "Epoch 083 | T: 22s | Train: 2.2546 (44.3%) | Phys: 0.79 | Val: 1.7859 (55.44%) | Best: 61.80%\n",
            "Epoch 084 | T: 22s | Train: 2.2803 (43.2%) | Phys: 0.79 | Val: 1.4578 (61.91%) | Best: 61.91%\n",
            "Epoch 085 | T: 22s | Train: 2.2202 (44.9%) | Phys: 0.79 | Val: 1.4683 (60.90%) | Best: 61.91%\n",
            "Epoch 086 | T: 22s | Train: 2.2377 (44.3%) | Phys: 0.79 | Val: 1.6288 (56.90%) | Best: 61.91%\n",
            "Epoch 087 | T: 22s | Train: 2.2595 (44.0%) | Phys: 0.79 | Val: 1.5960 (58.48%) | Best: 61.91%\n",
            "Epoch 088 | T: 22s | Train: 2.2758 (43.8%) | Phys: 0.79 | Val: 1.4359 (62.26%) | Best: 62.26%\n",
            "Epoch 089 | T: 22s | Train: 2.2556 (44.2%) | Phys: 0.79 | Val: 1.6764 (55.18%) | Best: 62.26%\n",
            "Epoch 090 | T: 22s | Train: 2.2464 (44.1%) | Phys: 0.79 | Val: 1.7406 (55.45%) | Best: 62.26%\n",
            "Epoch 091 | T: 22s | Train: 2.2396 (44.8%) | Phys: 0.79 | Val: 1.8905 (52.49%) | Best: 62.26%\n",
            "Epoch 092 | T: 22s | Train: 2.2242 (45.1%) | Phys: 0.79 | Val: 1.6946 (57.31%) | Best: 62.26%\n",
            "Epoch 093 | T: 22s | Train: 2.2850 (43.6%) | Phys: 0.79 | Val: 1.6415 (57.76%) | Best: 62.26%\n",
            "Epoch 094 | T: 22s | Train: 2.2216 (45.1%) | Phys: 0.79 | Val: 1.5718 (59.84%) | Best: 62.26%\n",
            "Epoch 095 | T: 21s | Train: 2.1952 (45.7%) | Phys: 0.79 | Val: 1.4195 (62.17%) | Best: 62.26%\n",
            "Epoch 096 | T: 22s | Train: 2.2658 (43.9%) | Phys: 0.79 | Val: 1.5934 (57.56%) | Best: 62.26%\n",
            "Epoch 097 | T: 21s | Train: 2.2568 (44.0%) | Phys: 0.79 | Val: 1.5281 (60.15%) | Best: 62.26%\n",
            "Epoch 098 | T: 22s | Train: 2.2007 (45.8%) | Phys: 0.79 | Val: 1.4577 (61.94%) | Best: 62.26%\n",
            "Epoch 099 | T: 22s | Train: 2.2520 (44.4%) | Phys: 0.79 | Val: 1.6158 (58.06%) | Best: 62.26%\n",
            "Epoch 100 | T: 22s | Train: 2.2202 (45.0%) | Phys: 0.79 | Val: 1.7585 (55.49%) | Best: 62.26%\n",
            "Epoch 101 | T: 22s | Train: 2.2285 (44.7%) | Phys: 0.79 | Val: 1.5018 (60.73%) | Best: 62.26%\n",
            "Epoch 102 | T: 22s | Train: 2.2667 (43.7%) | Phys: 0.79 | Val: 1.5172 (60.97%) | Best: 62.26%\n",
            "Epoch 103 | T: 22s | Train: 2.2017 (45.3%) | Phys: 0.79 | Val: 1.5414 (60.22%) | Best: 62.26%\n",
            "Epoch 104 | T: 22s | Train: 2.2137 (45.2%) | Phys: 0.79 | Val: 1.4272 (61.95%) | Best: 62.26%\n",
            "Epoch 105 | T: 22s | Train: 2.1963 (45.3%) | Phys: 0.79 | Val: 1.5932 (59.92%) | Best: 62.26%\n",
            "Epoch 106 | T: 22s | Train: 2.2145 (45.2%) | Phys: 0.79 | Val: 1.6297 (59.16%) | Best: 62.26%\n",
            "Epoch 107 | T: 21s | Train: 2.2164 (45.0%) | Phys: 0.79 | Val: 1.5776 (58.77%) | Best: 62.26%\n",
            "Epoch 108 | T: 22s | Train: 2.1410 (46.8%) | Phys: 0.79 | Val: 1.4981 (60.83%) | Best: 62.26%\n",
            "Epoch 109 | T: 22s | Train: 2.2111 (45.5%) | Phys: 0.79 | Val: 1.7713 (55.26%) | Best: 62.26%\n",
            "Epoch 110 | T: 22s | Train: 2.1984 (45.8%) | Phys: 0.79 | Val: 1.4003 (63.15%) | Best: 63.15%\n",
            "Epoch 111 | T: 21s | Train: 2.1898 (45.6%) | Phys: 0.79 | Val: 1.7226 (56.17%) | Best: 63.15%\n",
            "Epoch 112 | T: 22s | Train: 2.1807 (46.2%) | Phys: 0.79 | Val: 1.6526 (58.30%) | Best: 63.15%\n",
            "Epoch 113 | T: 22s | Train: 2.1843 (46.1%) | Phys: 0.79 | Val: 1.5374 (59.62%) | Best: 63.15%\n",
            "Epoch 114 | T: 22s | Train: 2.2041 (45.5%) | Phys: 0.79 | Val: 1.5232 (61.47%) | Best: 63.15%\n",
            "Epoch 115 | T: 21s | Train: 2.2013 (45.3%) | Phys: 0.79 | Val: 1.5411 (60.54%) | Best: 63.15%\n",
            "Epoch 116 | T: 22s | Train: 2.2001 (45.2%) | Phys: 0.79 | Val: 1.5748 (60.55%) | Best: 63.15%\n",
            "Epoch 117 | T: 22s | Train: 2.1974 (45.6%) | Phys: 0.79 | Val: 1.4409 (62.37%) | Best: 63.15%\n",
            "Epoch 118 | T: 21s | Train: 2.1864 (45.7%) | Phys: 0.79 | Val: 1.6904 (56.20%) | Best: 63.15%\n",
            "Epoch 119 | T: 22s | Train: 2.1830 (45.9%) | Phys: 0.79 | Val: 1.5838 (58.52%) | Best: 63.15%\n",
            "Epoch 120 | T: 21s | Train: 2.1751 (46.3%) | Phys: 0.79 | Val: 1.6000 (59.46%) | Best: 63.15%\n",
            "Epoch 121 | T: 22s | Train: 2.1879 (46.1%) | Phys: 0.79 | Val: 1.5001 (60.54%) | Best: 63.15%\n",
            "Epoch 122 | T: 22s | Train: 2.2638 (43.9%) | Phys: 0.79 | Val: 1.5831 (59.57%) | Best: 63.15%\n",
            "Epoch 123 | T: 22s | Train: 2.2021 (45.4%) | Phys: 0.79 | Val: 1.5381 (59.72%) | Best: 63.15%\n",
            "Epoch 124 | T: 22s | Train: 2.1888 (45.9%) | Phys: 0.79 | Val: 1.5603 (60.48%) | Best: 63.15%\n",
            "Epoch 125 | T: 22s | Train: 2.1390 (47.0%) | Phys: 0.79 | Val: 1.3464 (64.49%) | Best: 64.49%\n",
            "Epoch 126 | T: 22s | Train: 2.1621 (46.5%) | Phys: 0.79 | Val: 1.5723 (59.67%) | Best: 64.49%\n",
            "Epoch 127 | T: 22s | Train: 2.1494 (47.0%) | Phys: 0.79 | Val: 1.4774 (61.95%) | Best: 64.49%\n",
            "Epoch 128 | T: 22s | Train: 2.1766 (46.2%) | Phys: 0.79 | Val: 1.4623 (61.94%) | Best: 64.49%\n",
            "Epoch 129 | T: 22s | Train: 2.1979 (45.7%) | Phys: 0.79 | Val: 1.5339 (60.62%) | Best: 64.49%\n",
            "Epoch 130 | T: 22s | Train: 2.1398 (47.3%) | Phys: 0.79 | Val: 1.5328 (60.55%) | Best: 64.49%\n",
            "Epoch 131 | T: 22s | Train: 2.1674 (46.7%) | Phys: 0.79 | Val: 1.5120 (61.15%) | Best: 64.49%\n",
            "Epoch 132 | T: 22s | Train: 2.1825 (46.3%) | Phys: 0.79 | Val: 1.5551 (60.15%) | Best: 64.49%\n",
            "Epoch 133 | T: 22s | Train: 2.2164 (45.2%) | Phys: 0.79 | Val: 1.4275 (63.35%) | Best: 64.49%\n",
            "Epoch 134 | T: 22s | Train: 2.1724 (46.4%) | Phys: 0.79 | Val: 1.5167 (60.56%) | Best: 64.49%\n",
            "Epoch 135 | T: 22s | Train: 2.1688 (46.1%) | Phys: 0.79 | Val: 1.5476 (59.35%) | Best: 64.49%\n",
            "Epoch 136 | T: 21s | Train: 2.1433 (46.9%) | Phys: 0.79 | Val: 1.4661 (61.65%) | Best: 64.49%\n",
            "Epoch 137 | T: 21s | Train: 2.1307 (47.4%) | Phys: 0.79 | Val: 1.6031 (59.28%) | Best: 64.49%\n",
            "Epoch 138 | T: 22s | Train: 2.1381 (47.1%) | Phys: 0.79 | Val: 1.3916 (63.97%) | Best: 64.49%\n",
            "Epoch 139 | T: 22s | Train: 2.1708 (46.5%) | Phys: 0.79 | Val: 1.4003 (63.66%) | Best: 64.49%\n",
            "Epoch 140 | T: 22s | Train: 2.1380 (47.1%) | Phys: 0.79 | Val: 1.5630 (60.34%) | Best: 64.49%\n",
            "Epoch 141 | T: 22s | Train: 2.1710 (46.6%) | Phys: 0.79 | Val: 1.3686 (64.56%) | Best: 64.56%\n",
            "Epoch 142 | T: 22s | Train: 2.1580 (46.9%) | Phys: 0.79 | Val: 1.5642 (59.35%) | Best: 64.56%\n",
            "Epoch 143 | T: 21s | Train: 2.1518 (47.0%) | Phys: 0.79 | Val: 1.4240 (62.74%) | Best: 64.56%\n",
            "Epoch 144 | T: 22s | Train: 2.0822 (48.5%) | Phys: 0.79 | Val: 1.5799 (58.61%) | Best: 64.56%\n",
            "Epoch 145 | T: 22s | Train: 2.1827 (46.2%) | Phys: 0.79 | Val: 1.4047 (63.05%) | Best: 64.56%\n",
            "Epoch 146 | T: 22s | Train: 2.1055 (48.0%) | Phys: 0.79 | Val: 1.4264 (62.61%) | Best: 64.56%\n",
            "Epoch 147 | T: 22s | Train: 2.1801 (45.9%) | Phys: 0.79 | Val: 1.4400 (62.40%) | Best: 64.56%\n",
            "Epoch 148 | T: 22s | Train: 2.0986 (48.3%) | Phys: 0.79 | Val: 1.4943 (60.89%) | Best: 64.56%\n",
            "Epoch 149 | T: 22s | Train: 2.0726 (49.1%) | Phys: 0.79 | Val: 1.4693 (62.09%) | Best: 64.56%\n",
            "Epoch 150 | T: 21s | Train: 2.1632 (46.4%) | Phys: 0.79 | Val: 1.3965 (63.43%) | Best: 64.56%\n",
            "Epoch 151 | T: 22s | Train: 2.0819 (48.8%) | Phys: 0.79 | Val: 1.4465 (63.25%) | Best: 64.56%\n",
            "Epoch 152 | T: 22s | Train: 2.0518 (49.5%) | Phys: 0.79 | Val: 1.4631 (62.55%) | Best: 64.56%\n",
            "Epoch 153 | T: 22s | Train: 2.1148 (47.6%) | Phys: 0.79 | Val: 1.4497 (62.87%) | Best: 64.56%\n",
            "Epoch 154 | T: 21s | Train: 2.1189 (47.9%) | Phys: 0.79 | Val: 1.4353 (63.18%) | Best: 64.56%\n",
            "Epoch 155 | T: 22s | Train: 2.1230 (47.4%) | Phys: 0.79 | Val: 1.3511 (64.49%) | Best: 64.56%\n",
            "Epoch 156 | T: 22s | Train: 2.1233 (47.5%) | Phys: 0.79 | Val: 1.4447 (62.49%) | Best: 64.56%\n",
            "Epoch 157 | T: 22s | Train: 2.1197 (47.5%) | Phys: 0.79 | Val: 1.3661 (64.49%) | Best: 64.56%\n",
            "Epoch 158 | T: 21s | Train: 2.1101 (47.8%) | Phys: 0.79 | Val: 1.2934 (66.02%) | Best: 66.02%\n",
            "Epoch 159 | T: 22s | Train: 2.0781 (48.7%) | Phys: 0.79 | Val: 1.4176 (63.00%) | Best: 66.02%\n",
            "Epoch 160 | T: 22s | Train: 2.0933 (48.6%) | Phys: 0.79 | Val: 1.3698 (65.01%) | Best: 66.02%\n",
            "Epoch 161 | T: 22s | Train: 2.0575 (49.2%) | Phys: 0.79 | Val: 1.3782 (63.86%) | Best: 66.02%\n",
            "Epoch 162 | T: 22s | Train: 2.1220 (47.7%) | Phys: 0.79 | Val: 1.4297 (63.26%) | Best: 66.02%\n",
            "Epoch 163 | T: 22s | Train: 2.0816 (48.7%) | Phys: 0.79 | Val: 1.3495 (64.70%) | Best: 66.02%\n",
            "Epoch 164 | T: 22s | Train: 2.0301 (49.9%) | Phys: 0.79 | Val: 1.4182 (63.90%) | Best: 66.02%\n",
            "Epoch 165 | T: 22s | Train: 2.0637 (49.0%) | Phys: 0.79 | Val: 1.3705 (63.99%) | Best: 66.02%\n",
            "Epoch 166 | T: 22s | Train: 2.1204 (47.7%) | Phys: 0.79 | Val: 1.3772 (64.71%) | Best: 66.02%\n",
            "Epoch 167 | T: 22s | Train: 2.1240 (47.7%) | Phys: 0.79 | Val: 1.5010 (62.44%) | Best: 66.02%\n",
            "Epoch 168 | T: 22s | Train: 2.0939 (48.1%) | Phys: 0.79 | Val: 1.4457 (62.01%) | Best: 66.02%\n",
            "Epoch 169 | T: 22s | Train: 2.0849 (48.5%) | Phys: 0.79 | Val: 1.3960 (63.75%) | Best: 66.02%\n",
            "Epoch 170 | T: 22s | Train: 2.0760 (48.5%) | Phys: 0.79 | Val: 1.3095 (65.28%) | Best: 66.02%\n",
            "Epoch 171 | T: 22s | Train: 2.0811 (48.7%) | Phys: 0.79 | Val: 1.2866 (65.10%) | Best: 66.02%\n",
            "Epoch 172 | T: 22s | Train: 2.0322 (50.0%) | Phys: 0.79 | Val: 1.3353 (65.59%) | Best: 66.02%\n",
            "Epoch 173 | T: 22s | Train: 2.1275 (47.5%) | Phys: 0.79 | Val: 1.5096 (62.22%) | Best: 66.02%\n",
            "Epoch 174 | T: 22s | Train: 2.0747 (48.9%) | Phys: 0.79 | Val: 1.3869 (64.85%) | Best: 66.02%\n",
            "Epoch 175 | T: 22s | Train: 1.9964 (50.9%) | Phys: 0.79 | Val: 1.2622 (66.22%) | Best: 66.22%\n",
            "Epoch 176 | T: 22s | Train: 2.0750 (48.8%) | Phys: 0.79 | Val: 1.2782 (66.57%) | Best: 66.57%\n",
            "Epoch 177 | T: 22s | Train: 2.0494 (49.4%) | Phys: 0.79 | Val: 1.3535 (64.52%) | Best: 66.57%\n",
            "Epoch 178 | T: 22s | Train: 2.0510 (49.6%) | Phys: 0.79 | Val: 1.3755 (64.20%) | Best: 66.57%\n",
            "Epoch 179 | T: 22s | Train: 2.0324 (50.0%) | Phys: 0.79 | Val: 1.4028 (63.18%) | Best: 66.57%\n",
            "Epoch 180 | T: 22s | Train: 2.0794 (48.4%) | Phys: 0.79 | Val: 1.4567 (62.93%) | Best: 66.57%\n",
            "Epoch 181 | T: 22s | Train: 2.0532 (49.7%) | Phys: 0.79 | Val: 1.3121 (65.55%) | Best: 66.57%\n",
            "Epoch 182 | T: 22s | Train: 2.0450 (49.8%) | Phys: 0.79 | Val: 1.3000 (66.08%) | Best: 66.57%\n",
            "Epoch 183 | T: 21s | Train: 2.0651 (49.3%) | Phys: 0.79 | Val: 1.3222 (66.17%) | Best: 66.57%\n",
            "Epoch 184 | T: 21s | Train: 2.0370 (50.1%) | Phys: 0.79 | Val: 1.3968 (63.83%) | Best: 66.57%\n",
            "Epoch 185 | T: 22s | Train: 2.0359 (50.1%) | Phys: 0.79 | Val: 1.3617 (65.00%) | Best: 66.57%\n",
            "Epoch 186 | T: 22s | Train: 2.0850 (48.6%) | Phys: 0.79 | Val: 1.3755 (65.04%) | Best: 66.57%\n",
            "Epoch 187 | T: 22s | Train: 2.0278 (50.4%) | Phys: 0.79 | Val: 1.2904 (65.63%) | Best: 66.57%\n",
            "Epoch 188 | T: 22s | Train: 2.0329 (50.1%) | Phys: 0.79 | Val: 1.3115 (65.95%) | Best: 66.57%\n",
            "Epoch 189 | T: 22s | Train: 2.0268 (50.3%) | Phys: 0.79 | Val: 1.4232 (64.00%) | Best: 66.57%\n",
            "Epoch 190 | T: 22s | Train: 1.9971 (51.3%) | Phys: 0.79 | Val: 1.2967 (66.56%) | Best: 66.57%\n",
            "Epoch 191 | T: 22s | Train: 2.0617 (49.4%) | Phys: 0.79 | Val: 1.3203 (66.33%) | Best: 66.57%\n",
            "Epoch 192 | T: 21s | Train: 2.0581 (49.3%) | Phys: 0.79 | Val: 1.3480 (65.48%) | Best: 66.57%\n",
            "Epoch 193 | T: 22s | Train: 2.0185 (50.8%) | Phys: 0.79 | Val: 1.3749 (65.43%) | Best: 66.57%\n",
            "Epoch 194 | T: 22s | Train: 2.0099 (50.5%) | Phys: 0.79 | Val: 1.4025 (63.80%) | Best: 66.57%\n",
            "Epoch 195 | T: 22s | Train: 2.0235 (50.3%) | Phys: 0.79 | Val: 1.2131 (68.09%) | Best: 68.09%\n",
            "Epoch 196 | T: 22s | Train: 2.0375 (49.9%) | Phys: 0.79 | Val: 1.4026 (63.35%) | Best: 68.09%\n",
            "Epoch 197 | T: 22s | Train: 2.0036 (50.9%) | Phys: 0.79 | Val: 1.2971 (66.47%) | Best: 68.09%\n",
            "Epoch 198 | T: 22s | Train: 2.0041 (51.0%) | Phys: 0.79 | Val: 1.4134 (64.61%) | Best: 68.09%\n",
            "Epoch 199 | T: 22s | Train: 1.9978 (51.1%) | Phys: 0.79 | Val: 1.2053 (68.50%) | Best: 68.50%\n",
            "Epoch 200 | T: 22s | Train: 1.9941 (51.3%) | Phys: 0.79 | Val: 1.2517 (67.40%) | Best: 68.50%\n",
            "Epoch 201 | T: 22s | Train: 1.9884 (51.4%) | Phys: 0.79 | Val: 1.3403 (65.81%) | Best: 68.50%\n",
            "Epoch 202 | T: 22s | Train: 2.0324 (50.1%) | Phys: 0.79 | Val: 1.2244 (67.79%) | Best: 68.50%\n",
            "Epoch 203 | T: 22s | Train: 1.9765 (51.1%) | Phys: 0.79 | Val: 1.3428 (65.76%) | Best: 68.50%\n",
            "Epoch 204 | T: 22s | Train: 2.0107 (50.6%) | Phys: 0.79 | Val: 1.3436 (65.46%) | Best: 68.50%\n",
            "Epoch 205 | T: 22s | Train: 2.0181 (50.2%) | Phys: 0.79 | Val: 1.3183 (65.91%) | Best: 68.50%\n",
            "Epoch 206 | T: 22s | Train: 1.9873 (51.2%) | Phys: 0.79 | Val: 1.2868 (66.32%) | Best: 68.50%\n",
            "Epoch 207 | T: 21s | Train: 1.9394 (52.2%) | Phys: 0.79 | Val: 1.2964 (66.73%) | Best: 68.50%\n",
            "Epoch 208 | T: 22s | Train: 2.0235 (50.3%) | Phys: 0.79 | Val: 1.3162 (67.40%) | Best: 68.50%\n",
            "Epoch 209 | T: 22s | Train: 1.9735 (51.5%) | Phys: 0.79 | Val: 1.2531 (67.28%) | Best: 68.50%\n",
            "Epoch 210 | T: 22s | Train: 1.9683 (51.7%) | Phys: 0.79 | Val: 1.2499 (66.83%) | Best: 68.50%\n",
            "Epoch 211 | T: 22s | Train: 1.9251 (52.7%) | Phys: 0.79 | Val: 1.2226 (68.23%) | Best: 68.50%\n",
            "Epoch 212 | T: 22s | Train: 1.9040 (53.6%) | Phys: 0.79 | Val: 1.3171 (65.77%) | Best: 68.50%\n",
            "Epoch 213 | T: 22s | Train: 1.9689 (51.3%) | Phys: 0.79 | Val: 1.1997 (69.42%) | Best: 69.42%\n",
            "Epoch 214 | T: 22s | Train: 2.0009 (50.9%) | Phys: 0.79 | Val: 1.2263 (68.39%) | Best: 69.42%\n",
            "Epoch 215 | T: 22s | Train: 1.9328 (52.4%) | Phys: 0.79 | Val: 1.3162 (66.93%) | Best: 69.42%\n",
            "Epoch 216 | T: 22s | Train: 1.9824 (51.2%) | Phys: 0.79 | Val: 1.2359 (68.46%) | Best: 69.42%\n",
            "Epoch 217 | T: 22s | Train: 1.9460 (52.2%) | Phys: 0.79 | Val: 1.2482 (67.98%) | Best: 69.42%\n",
            "Epoch 218 | T: 22s | Train: 1.9693 (51.7%) | Phys: 0.79 | Val: 1.3383 (66.40%) | Best: 69.42%\n",
            "Epoch 219 | T: 22s | Train: 1.9791 (51.2%) | Phys: 0.79 | Val: 1.2617 (67.22%) | Best: 69.42%\n",
            "Epoch 220 | T: 22s | Train: 1.9424 (51.9%) | Phys: 0.79 | Val: 1.2676 (66.46%) | Best: 69.42%\n",
            "Epoch 221 | T: 21s | Train: 2.0142 (50.4%) | Phys: 0.79 | Val: 1.2413 (68.06%) | Best: 69.42%\n",
            "Epoch 222 | T: 22s | Train: 1.9096 (53.4%) | Phys: 0.79 | Val: 1.2890 (67.12%) | Best: 69.42%\n",
            "Epoch 223 | T: 22s | Train: 1.9520 (52.4%) | Phys: 0.79 | Val: 1.1442 (69.90%) | Best: 69.90%\n",
            "Epoch 224 | T: 22s | Train: 1.9639 (51.9%) | Phys: 0.79 | Val: 1.1632 (70.04%) | Best: 70.04%\n",
            "Epoch 225 | T: 22s | Train: 1.8747 (54.2%) | Phys: 0.79 | Val: 1.1693 (68.94%) | Best: 70.04%\n",
            "Epoch 226 | T: 22s | Train: 1.8754 (53.6%) | Phys: 0.79 | Val: 1.2453 (68.24%) | Best: 70.04%\n",
            "Epoch 227 | T: 22s | Train: 1.9573 (51.9%) | Phys: 0.79 | Val: 1.1796 (69.58%) | Best: 70.04%\n",
            "Epoch 228 | T: 22s | Train: 1.8812 (54.1%) | Phys: 0.79 | Val: 1.1590 (69.31%) | Best: 70.04%\n",
            "Epoch 229 | T: 22s | Train: 1.9078 (53.1%) | Phys: 0.79 | Val: 1.1261 (70.50%) | Best: 70.50%\n",
            "Epoch 230 | T: 22s | Train: 1.9259 (52.7%) | Phys: 0.79 | Val: 1.1856 (69.79%) | Best: 70.50%\n",
            "Epoch 231 | T: 22s | Train: 1.8963 (53.4%) | Phys: 0.79 | Val: 1.3415 (66.91%) | Best: 70.50%\n",
            "Epoch 232 | T: 21s | Train: 1.8699 (54.2%) | Phys: 0.79 | Val: 1.1777 (69.48%) | Best: 70.50%\n",
            "Epoch 233 | T: 22s | Train: 1.9422 (52.2%) | Phys: 0.79 | Val: 1.2292 (69.13%) | Best: 70.50%\n",
            "Epoch 234 | T: 22s | Train: 1.9000 (53.2%) | Phys: 0.79 | Val: 1.2052 (69.04%) | Best: 70.50%\n",
            "Epoch 235 | T: 22s | Train: 1.9342 (52.5%) | Phys: 0.79 | Val: 1.1101 (70.90%) | Best: 70.90%\n",
            "Epoch 236 | T: 22s | Train: 1.7968 (56.2%) | Phys: 0.79 | Val: 1.1573 (70.36%) | Best: 70.90%\n",
            "Epoch 237 | T: 22s | Train: 1.8595 (54.4%) | Phys: 0.79 | Val: 1.0989 (70.85%) | Best: 70.90%\n",
            "Epoch 238 | T: 22s | Train: 1.8822 (53.7%) | Phys: 0.79 | Val: 1.1435 (70.08%) | Best: 70.90%\n",
            "Epoch 239 | T: 22s | Train: 1.9051 (53.4%) | Phys: 0.79 | Val: 1.0959 (71.94%) | Best: 71.94%\n",
            "Epoch 240 | T: 22s | Train: 1.9073 (53.4%) | Phys: 0.79 | Val: 1.1023 (70.87%) | Best: 71.94%\n",
            "Epoch 241 | T: 22s | Train: 1.8858 (53.9%) | Phys: 0.79 | Val: 1.1706 (68.84%) | Best: 71.94%\n",
            "Epoch 242 | T: 21s | Train: 1.8541 (54.6%) | Phys: 0.79 | Val: 1.1801 (69.60%) | Best: 71.94%\n",
            "Epoch 243 | T: 22s | Train: 1.8162 (55.8%) | Phys: 0.79 | Val: 1.0662 (71.39%) | Best: 71.94%\n",
            "Epoch 244 | T: 21s | Train: 1.8551 (54.6%) | Phys: 0.79 | Val: 1.1526 (70.94%) | Best: 71.94%\n",
            "Epoch 245 | T: 22s | Train: 1.8600 (54.5%) | Phys: 0.79 | Val: 1.1537 (70.94%) | Best: 71.94%\n",
            "Epoch 246 | T: 22s | Train: 1.8580 (54.6%) | Phys: 0.79 | Val: 1.1448 (70.58%) | Best: 71.94%\n",
            "Epoch 247 | T: 22s | Train: 1.8825 (54.1%) | Phys: 0.79 | Val: 1.1238 (71.40%) | Best: 71.94%\n",
            "Epoch 248 | T: 22s | Train: 1.8100 (55.6%) | Phys: 0.79 | Val: 1.3141 (66.98%) | Best: 71.94%\n",
            "Epoch 249 | T: 22s | Train: 1.8281 (55.5%) | Phys: 0.79 | Val: 1.2934 (67.07%) | Best: 71.94%\n",
            "Epoch 250 | T: 21s | Train: 1.8178 (55.6%) | Phys: 0.79 | Val: 1.1010 (71.10%) | Best: 71.94%\n",
            "Epoch 251 | T: 22s | Train: 1.8645 (54.0%) | Phys: 0.79 | Val: 1.1094 (72.17%) | Best: 72.17%\n",
            "Epoch 252 | T: 22s | Train: 1.8098 (56.1%) | Phys: 0.79 | Val: 1.0917 (72.00%) | Best: 72.17%\n",
            "Epoch 253 | T: 22s | Train: 1.7484 (57.5%) | Phys: 0.79 | Val: 1.0990 (71.60%) | Best: 72.17%\n",
            "Epoch 254 | T: 22s | Train: 1.8368 (55.3%) | Phys: 0.79 | Val: 1.0926 (71.89%) | Best: 72.17%\n",
            "Epoch 255 | T: 22s | Train: 1.8496 (54.8%) | Phys: 0.79 | Val: 1.0913 (71.53%) | Best: 72.17%\n",
            "Epoch 256 | T: 22s | Train: 1.7850 (56.4%) | Phys: 0.79 | Val: 1.1588 (70.49%) | Best: 72.17%\n",
            "Epoch 257 | T: 22s | Train: 1.8110 (55.9%) | Phys: 0.79 | Val: 1.0589 (73.13%) | Best: 73.13%\n",
            "Epoch 258 | T: 22s | Train: 1.8444 (54.4%) | Phys: 0.79 | Val: 1.0981 (71.35%) | Best: 73.13%\n",
            "Epoch 259 | T: 22s | Train: 1.8618 (54.2%) | Phys: 0.79 | Val: 1.1695 (70.14%) | Best: 73.13%\n",
            "Epoch 260 | T: 22s | Train: 1.7313 (58.0%) | Phys: 0.79 | Val: 1.0096 (73.15%) | Best: 73.15%\n",
            "Epoch 261 | T: 22s | Train: 1.7527 (57.3%) | Phys: 0.79 | Val: 1.0238 (72.95%) | Best: 73.15%\n",
            "Epoch 262 | T: 22s | Train: 1.7747 (56.7%) | Phys: 0.79 | Val: 1.1170 (71.18%) | Best: 73.15%\n",
            "Epoch 263 | T: 22s | Train: 1.8318 (55.3%) | Phys: 0.79 | Val: 1.1544 (71.09%) | Best: 73.15%\n",
            "Epoch 264 | T: 22s | Train: 1.7099 (58.6%) | Phys: 0.79 | Val: 1.0686 (71.45%) | Best: 73.15%\n",
            "Epoch 265 | T: 21s | Train: 1.7630 (57.4%) | Phys: 0.79 | Val: 1.1115 (72.53%) | Best: 73.15%\n",
            "Epoch 266 | T: 22s | Train: 1.7593 (57.5%) | Phys: 0.79 | Val: 1.0436 (73.16%) | Best: 73.16%\n",
            "Epoch 267 | T: 22s | Train: 1.7152 (58.5%) | Phys: 0.79 | Val: 1.0545 (72.58%) | Best: 73.16%\n",
            "Epoch 268 | T: 21s | Train: 1.8167 (55.3%) | Phys: 0.79 | Val: 1.0609 (72.95%) | Best: 73.16%\n",
            "Epoch 269 | T: 21s | Train: 1.7175 (58.3%) | Phys: 0.79 | Val: 1.0598 (72.32%) | Best: 73.16%\n",
            "Epoch 270 | T: 22s | Train: 1.7896 (55.9%) | Phys: 0.79 | Val: 1.1503 (71.27%) | Best: 73.16%\n",
            "Epoch 271 | T: 22s | Train: 1.7191 (58.2%) | Phys: 0.79 | Val: 0.9916 (73.89%) | Best: 73.89%\n",
            "Epoch 272 | T: 22s | Train: 1.7662 (56.7%) | Phys: 0.79 | Val: 1.1025 (72.27%) | Best: 73.89%\n",
            "Epoch 273 | T: 22s | Train: 1.7700 (56.5%) | Phys: 0.79 | Val: 1.0294 (73.67%) | Best: 73.89%\n",
            "Epoch 274 | T: 22s | Train: 1.7209 (58.5%) | Phys: 0.79 | Val: 1.0656 (72.66%) | Best: 73.89%\n",
            "Epoch 275 | T: 21s | Train: 1.7032 (58.8%) | Phys: 0.79 | Val: 1.0626 (73.35%) | Best: 73.89%\n",
            "Epoch 276 | T: 22s | Train: 1.6561 (60.1%) | Phys: 0.79 | Val: 1.0783 (73.13%) | Best: 73.89%\n",
            "Epoch 277 | T: 22s | Train: 1.7363 (57.5%) | Phys: 0.79 | Val: 1.0846 (72.56%) | Best: 73.89%\n",
            "Epoch 278 | T: 22s | Train: 1.6712 (59.4%) | Phys: 0.79 | Val: 1.0560 (72.44%) | Best: 73.89%\n",
            "Epoch 279 | T: 22s | Train: 1.6915 (58.3%) | Phys: 0.79 | Val: 1.1035 (71.97%) | Best: 73.89%\n",
            "Epoch 280 | T: 22s | Train: 1.6655 (60.1%) | Phys: 0.79 | Val: 1.0272 (73.66%) | Best: 73.89%\n",
            "Epoch 281 | T: 22s | Train: 1.7413 (57.5%) | Phys: 0.79 | Val: 1.0332 (73.92%) | Best: 73.92%\n",
            "Epoch 282 | T: 22s | Train: 1.7183 (58.3%) | Phys: 0.79 | Val: 1.0274 (73.49%) | Best: 73.92%\n",
            "Epoch 283 | T: 22s | Train: 1.6299 (60.8%) | Phys: 0.79 | Val: 0.9844 (73.47%) | Best: 73.92%\n",
            "Epoch 284 | T: 22s | Train: 1.7098 (58.1%) | Phys: 0.79 | Val: 1.0552 (73.91%) | Best: 73.92%\n",
            "Epoch 285 | T: 22s | Train: 1.7003 (58.9%) | Phys: 0.79 | Val: 1.0332 (73.59%) | Best: 73.92%\n",
            "Epoch 286 | T: 22s | Train: 1.6819 (58.5%) | Phys: 0.79 | Val: 0.9965 (74.41%) | Best: 74.41%\n",
            "Epoch 287 | T: 21s | Train: 1.6620 (59.7%) | Phys: 0.79 | Val: 1.0087 (74.16%) | Best: 74.41%\n",
            "Epoch 288 | T: 22s | Train: 1.6257 (60.4%) | Phys: 0.79 | Val: 1.0358 (74.69%) | Best: 74.69%\n",
            "Epoch 289 | T: 22s | Train: 1.6423 (60.0%) | Phys: 0.79 | Val: 0.9916 (74.52%) | Best: 74.69%\n",
            "Epoch 290 | T: 22s | Train: 1.6659 (59.3%) | Phys: 0.79 | Val: 0.9544 (74.59%) | Best: 74.69%\n",
            "Epoch 291 | T: 22s | Train: 1.5968 (61.3%) | Phys: 0.79 | Val: 1.0656 (73.54%) | Best: 74.69%\n",
            "Epoch 292 | T: 22s | Train: 1.6130 (61.0%) | Phys: 0.79 | Val: 0.9670 (75.31%) | Best: 75.31%\n",
            "Epoch 293 | T: 22s | Train: 1.5516 (62.8%) | Phys: 0.79 | Val: 1.0284 (73.14%) | Best: 75.31%\n",
            "Epoch 294 | T: 22s | Train: 1.6492 (59.8%) | Phys: 0.79 | Val: 1.0143 (74.15%) | Best: 75.31%\n",
            "Epoch 295 | T: 22s | Train: 1.5992 (61.4%) | Phys: 0.79 | Val: 0.9750 (75.22%) | Best: 75.31%\n",
            "Epoch 296 | T: 22s | Train: 1.6062 (61.4%) | Phys: 0.79 | Val: 0.9681 (75.72%) | Best: 75.72%\n",
            "Epoch 297 | T: 22s | Train: 1.6401 (60.0%) | Phys: 0.79 | Val: 1.0118 (74.03%) | Best: 75.72%\n",
            "Epoch 298 | T: 22s | Train: 1.5629 (62.8%) | Phys: 0.79 | Val: 1.0362 (73.63%) | Best: 75.72%\n",
            "Epoch 299 | T: 22s | Train: 1.6153 (60.7%) | Phys: 0.79 | Val: 0.9945 (75.17%) | Best: 75.72%\n",
            "Epoch 300 | T: 22s | Train: 1.5909 (61.7%) | Phys: 0.79 | Val: 0.9814 (74.86%) | Best: 75.72%\n",
            "Epoch 301 | T: 22s | Train: 1.5705 (61.9%) | Phys: 0.79 | Val: 0.9525 (75.34%) | Best: 75.72%\n",
            "Epoch 302 | T: 22s | Train: 1.6246 (60.4%) | Phys: 0.79 | Val: 0.9291 (76.03%) | Best: 76.03%\n",
            "Epoch 303 | T: 22s | Train: 1.5661 (61.7%) | Phys: 0.79 | Val: 0.9662 (75.83%) | Best: 76.03%\n",
            "Epoch 304 | T: 22s | Train: 1.5677 (62.0%) | Phys: 0.79 | Val: 0.9064 (76.33%) | Best: 76.33%\n",
            "Epoch 305 | T: 22s | Train: 1.5768 (61.9%) | Phys: 0.79 | Val: 0.9551 (75.58%) | Best: 76.33%\n",
            "Epoch 306 | T: 22s | Train: 1.5782 (61.5%) | Phys: 0.79 | Val: 0.9833 (75.11%) | Best: 76.33%\n",
            "Epoch 307 | T: 22s | Train: 1.5888 (61.6%) | Phys: 0.79 | Val: 0.9352 (76.05%) | Best: 76.33%\n",
            "Epoch 308 | T: 22s | Train: 1.5653 (62.2%) | Phys: 0.79 | Val: 0.9153 (76.36%) | Best: 76.36%\n",
            "Epoch 309 | T: 22s | Train: 1.5136 (63.7%) | Phys: 0.79 | Val: 0.8928 (76.90%) | Best: 76.90%\n",
            "Epoch 310 | T: 22s | Train: 1.5681 (61.8%) | Phys: 0.79 | Val: 0.8961 (76.77%) | Best: 76.90%\n",
            "Epoch 311 | T: 21s | Train: 1.5521 (62.1%) | Phys: 0.79 | Val: 0.8818 (76.51%) | Best: 76.90%\n",
            "Epoch 312 | T: 22s | Train: 1.5548 (61.4%) | Phys: 0.79 | Val: 0.8838 (76.53%) | Best: 76.90%\n",
            "Epoch 313 | T: 22s | Train: 1.5290 (63.2%) | Phys: 0.79 | Val: 0.9124 (76.60%) | Best: 76.90%\n",
            "Epoch 314 | T: 21s | Train: 1.5420 (62.5%) | Phys: 0.79 | Val: 0.9021 (76.82%) | Best: 76.90%\n",
            "Epoch 315 | T: 22s | Train: 1.5710 (61.8%) | Phys: 0.79 | Val: 0.9814 (76.14%) | Best: 76.90%\n",
            "Epoch 316 | T: 22s | Train: 1.4899 (64.0%) | Phys: 0.79 | Val: 0.9176 (76.62%) | Best: 76.90%\n",
            "Epoch 317 | T: 22s | Train: 1.4665 (64.6%) | Phys: 0.79 | Val: 0.8811 (77.27%) | Best: 77.27%\n",
            "Epoch 318 | T: 22s | Train: 1.4568 (64.3%) | Phys: 0.79 | Val: 0.8924 (77.17%) | Best: 77.27%\n",
            "Epoch 319 | T: 23s | Train: 1.5013 (63.6%) | Phys: 0.79 | Val: 0.9226 (76.46%) | Best: 77.27%\n",
            "Epoch 320 | T: 22s | Train: 1.5005 (63.6%) | Phys: 0.79 | Val: 0.8699 (77.04%) | Best: 77.27%\n",
            "Epoch 321 | T: 22s | Train: 1.4545 (64.8%) | Phys: 0.79 | Val: 0.8870 (77.76%) | Best: 77.76%\n",
            "Epoch 322 | T: 22s | Train: 1.5010 (63.4%) | Phys: 0.79 | Val: 0.8494 (77.93%) | Best: 77.93%\n",
            "Epoch 323 | T: 22s | Train: 1.4935 (64.1%) | Phys: 0.79 | Val: 0.9442 (76.64%) | Best: 77.93%\n",
            "Epoch 324 | T: 22s | Train: 1.5027 (63.5%) | Phys: 0.79 | Val: 0.8925 (78.21%) | Best: 78.21%\n",
            "Epoch 325 | T: 22s | Train: 1.4949 (63.4%) | Phys: 0.79 | Val: 0.8553 (78.15%) | Best: 78.21%\n",
            "Epoch 326 | T: 22s | Train: 1.4810 (63.8%) | Phys: 0.79 | Val: 0.8618 (77.64%) | Best: 78.21%\n",
            "Epoch 327 | T: 22s | Train: 1.4705 (64.3%) | Phys: 0.79 | Val: 0.9089 (77.43%) | Best: 78.21%\n",
            "Epoch 328 | T: 22s | Train: 1.4309 (66.0%) | Phys: 0.79 | Val: 0.8677 (77.96%) | Best: 78.21%\n",
            "Epoch 329 | T: 22s | Train: 1.3876 (66.8%) | Phys: 0.79 | Val: 0.8891 (78.12%) | Best: 78.21%\n",
            "Epoch 330 | T: 22s | Train: 1.4321 (64.7%) | Phys: 0.79 | Val: 0.8673 (78.57%) | Best: 78.57%\n",
            "Epoch 331 | T: 22s | Train: 1.4227 (65.1%) | Phys: 0.79 | Val: 0.8190 (78.87%) | Best: 78.87%\n",
            "Epoch 332 | T: 22s | Train: 1.4085 (65.7%) | Phys: 0.79 | Val: 0.9343 (77.57%) | Best: 78.87%\n",
            "Epoch 333 | T: 22s | Train: 1.4247 (65.1%) | Phys: 0.79 | Val: 0.8290 (78.75%) | Best: 78.87%\n",
            "Epoch 334 | T: 22s | Train: 1.4229 (65.3%) | Phys: 0.79 | Val: 0.8820 (77.91%) | Best: 78.87%\n",
            "Epoch 335 | T: 22s | Train: 1.3882 (66.1%) | Phys: 0.79 | Val: 0.8798 (78.38%) | Best: 78.87%\n",
            "Epoch 336 | T: 22s | Train: 1.3543 (67.2%) | Phys: 0.79 | Val: 0.8957 (78.61%) | Best: 78.87%\n",
            "Epoch 337 | T: 22s | Train: 1.4238 (65.3%) | Phys: 0.79 | Val: 0.8474 (79.05%) | Best: 79.05%\n",
            "Epoch 338 | T: 22s | Train: 1.4229 (64.9%) | Phys: 0.79 | Val: 0.8338 (79.65%) | Best: 79.65%\n",
            "Epoch 339 | T: 22s | Train: 1.3525 (67.1%) | Phys: 0.79 | Val: 0.8755 (78.35%) | Best: 79.65%\n",
            "Epoch 340 | T: 22s | Train: 1.3996 (66.2%) | Phys: 0.79 | Val: 0.8598 (78.72%) | Best: 79.65%\n",
            "Epoch 341 | T: 22s | Train: 1.3672 (66.8%) | Phys: 0.79 | Val: 0.8124 (79.52%) | Best: 79.65%\n",
            "Epoch 342 | T: 21s | Train: 1.3275 (67.7%) | Phys: 0.79 | Val: 0.8375 (78.77%) | Best: 79.65%\n",
            "Epoch 343 | T: 22s | Train: 1.3401 (67.5%) | Phys: 0.79 | Val: 0.8318 (79.18%) | Best: 79.65%\n",
            "Epoch 344 | T: 22s | Train: 1.3479 (67.0%) | Phys: 0.79 | Val: 0.8431 (78.83%) | Best: 79.65%\n",
            "Epoch 345 | T: 21s | Train: 1.3595 (66.9%) | Phys: 0.79 | Val: 0.8268 (79.11%) | Best: 79.65%\n",
            "Epoch 346 | T: 22s | Train: 1.3741 (66.4%) | Phys: 0.79 | Val: 0.8360 (79.50%) | Best: 79.65%\n",
            "Epoch 347 | T: 22s | Train: 1.3890 (65.6%) | Phys: 0.79 | Val: 0.8727 (79.07%) | Best: 79.65%\n",
            "Epoch 348 | T: 22s | Train: 1.3120 (68.1%) | Phys: 0.79 | Val: 0.8015 (79.74%) | Best: 79.74%\n",
            "Epoch 349 | T: 22s | Train: 1.3565 (66.3%) | Phys: 0.79 | Val: 0.8323 (79.10%) | Best: 79.74%\n",
            "Epoch 350 | T: 23s | Train: 1.2573 (69.2%) | Phys: 0.79 | Val: 0.8057 (79.67%) | Best: 79.74%\n",
            "Epoch 351 | T: 22s | Train: 1.3250 (66.7%) | Phys: 0.79 | Val: 0.8152 (79.45%) | Best: 79.74%\n",
            "Epoch 352 | T: 22s | Train: 1.2945 (68.0%) | Phys: 0.79 | Val: 0.8089 (79.87%) | Best: 79.87%\n",
            "Epoch 353 | T: 22s | Train: 1.2985 (68.0%) | Phys: 0.79 | Val: 0.8142 (80.15%) | Best: 80.15%\n",
            "Epoch 354 | T: 22s | Train: 1.3314 (66.3%) | Phys: 0.79 | Val: 0.8672 (79.84%) | Best: 80.15%\n",
            "Epoch 355 | T: 22s | Train: 1.3003 (67.6%) | Phys: 0.79 | Val: 0.8128 (79.97%) | Best: 80.15%\n",
            "Epoch 356 | T: 22s | Train: 1.3281 (66.9%) | Phys: 0.79 | Val: 0.7965 (80.10%) | Best: 80.15%\n",
            "Epoch 357 | T: 22s | Train: 1.3122 (67.4%) | Phys: 0.79 | Val: 0.8071 (80.19%) | Best: 80.19%\n",
            "Epoch 358 | T: 22s | Train: 1.3066 (67.2%) | Phys: 0.79 | Val: 0.7761 (81.04%) | Best: 81.04%\n",
            "Epoch 359 | T: 22s | Train: 1.2760 (68.2%) | Phys: 0.79 | Val: 0.8117 (80.32%) | Best: 81.04%\n",
            "Epoch 360 | T: 22s | Train: 1.2216 (70.7%) | Phys: 0.79 | Val: 0.8063 (80.40%) | Best: 81.04%\n",
            "Epoch 361 | T: 22s | Train: 1.2788 (68.2%) | Phys: 0.79 | Val: 0.8405 (80.04%) | Best: 81.04%\n",
            "Epoch 362 | T: 22s | Train: 1.2867 (67.9%) | Phys: 0.79 | Val: 0.7833 (80.80%) | Best: 81.04%\n",
            "Epoch 363 | T: 22s | Train: 1.2924 (67.5%) | Phys: 0.79 | Val: 0.7512 (81.24%) | Best: 81.24%\n",
            "Epoch 364 | T: 22s | Train: 1.2689 (68.4%) | Phys: 0.79 | Val: 0.8000 (80.89%) | Best: 81.24%\n",
            "Epoch 365 | T: 22s | Train: 1.2360 (69.0%) | Phys: 0.79 | Val: 0.7941 (80.82%) | Best: 81.24%\n",
            "Epoch 366 | T: 22s | Train: 1.2395 (69.1%) | Phys: 0.79 | Val: 0.7769 (80.62%) | Best: 81.24%\n",
            "Epoch 367 | T: 22s | Train: 1.2165 (69.5%) | Phys: 0.79 | Val: 0.7580 (81.50%) | Best: 81.50%\n",
            "Epoch 368 | T: 22s | Train: 1.1943 (70.8%) | Phys: 0.79 | Val: 0.7571 (81.26%) | Best: 81.50%\n",
            "Epoch 369 | T: 22s | Train: 1.2706 (68.0%) | Phys: 0.79 | Val: 0.7711 (81.50%) | Best: 81.50%\n",
            "Epoch 370 | T: 22s | Train: 1.1845 (70.7%) | Phys: 0.79 | Val: 0.7532 (81.47%) | Best: 81.50%\n",
            "Epoch 371 | T: 22s | Train: 1.2269 (68.8%) | Phys: 0.79 | Val: 0.7567 (81.30%) | Best: 81.50%\n",
            "Epoch 372 | T: 22s | Train: 1.1942 (70.5%) | Phys: 0.79 | Val: 0.7939 (81.19%) | Best: 81.50%\n",
            "Epoch 373 | T: 22s | Train: 1.1988 (69.7%) | Phys: 0.79 | Val: 0.7930 (81.13%) | Best: 81.50%\n",
            "Epoch 374 | T: 22s | Train: 1.1867 (70.2%) | Phys: 0.79 | Val: 0.7621 (81.32%) | Best: 81.50%\n",
            "Epoch 375 | T: 22s | Train: 1.2184 (69.0%) | Phys: 0.79 | Val: 0.7747 (81.13%) | Best: 81.50%\n",
            "Epoch 376 | T: 22s | Train: 1.1511 (71.1%) | Phys: 0.79 | Val: 0.7491 (81.65%) | Best: 81.65%\n",
            "Epoch 377 | T: 22s | Train: 1.2116 (69.1%) | Phys: 0.79 | Val: 0.7405 (81.85%) | Best: 81.85%\n",
            "Epoch 378 | T: 22s | Train: 1.2072 (69.4%) | Phys: 0.79 | Val: 0.7755 (81.42%) | Best: 81.85%\n",
            "Epoch 379 | T: 22s | Train: 1.2103 (68.7%) | Phys: 0.79 | Val: 0.7209 (81.77%) | Best: 81.85%\n",
            "Epoch 380 | T: 22s | Train: 1.1866 (69.6%) | Phys: 0.79 | Val: 0.7857 (81.18%) | Best: 81.85%\n",
            "Epoch 381 | T: 22s | Train: 1.2018 (68.8%) | Phys: 0.79 | Val: 0.7644 (81.33%) | Best: 81.85%\n",
            "Epoch 382 | T: 22s | Train: 1.2216 (68.8%) | Phys: 0.79 | Val: 0.7607 (81.79%) | Best: 81.85%\n",
            "Epoch 383 | T: 22s | Train: 1.1444 (71.4%) | Phys: 0.79 | Val: 0.7433 (81.85%) | Best: 81.85%\n",
            "Epoch 384 | T: 22s | Train: 1.1626 (70.5%) | Phys: 0.79 | Val: 0.7356 (81.90%) | Best: 81.90%\n",
            "Epoch 385 | T: 22s | Train: 1.1718 (70.3%) | Phys: 0.79 | Val: 0.7966 (81.45%) | Best: 81.90%\n",
            "Epoch 386 | T: 22s | Train: 1.1673 (70.5%) | Phys: 0.79 | Val: 0.7550 (81.75%) | Best: 81.90%\n",
            "Epoch 387 | T: 22s | Train: 1.1621 (70.7%) | Phys: 0.79 | Val: 0.7639 (81.69%) | Best: 81.90%\n",
            "Epoch 388 | T: 22s | Train: 1.1982 (68.7%) | Phys: 0.79 | Val: 0.7500 (81.81%) | Best: 81.90%\n",
            "Epoch 389 | T: 22s | Train: 1.1731 (70.4%) | Phys: 0.79 | Val: 0.7517 (81.99%) | Best: 81.99%\n",
            "Epoch 390 | T: 22s | Train: 1.1719 (70.1%) | Phys: 0.79 | Val: 0.7501 (82.00%) | Best: 82.00%\n",
            "Epoch 391 | T: 22s | Train: 1.1660 (70.2%) | Phys: 0.79 | Val: 0.7864 (81.51%) | Best: 82.00%\n",
            "Epoch 392 | T: 22s | Train: 1.2241 (68.5%) | Phys: 0.79 | Val: 0.7854 (81.63%) | Best: 82.00%\n",
            "Epoch 393 | T: 22s | Train: 1.2168 (68.9%) | Phys: 0.79 | Val: 0.7483 (82.06%) | Best: 82.06%\n",
            "Epoch 394 | T: 22s | Train: 1.1875 (69.8%) | Phys: 0.79 | Val: 0.7559 (81.80%) | Best: 82.06%\n",
            "Epoch 395 | T: 22s | Train: 1.2223 (67.6%) | Phys: 0.79 | Val: 0.7693 (81.75%) | Best: 82.06%\n",
            "Epoch 396 | T: 22s | Train: 1.1021 (72.3%) | Phys: 0.79 | Val: 0.7195 (82.10%) | Best: 82.10%\n",
            "Epoch 397 | T: 22s | Train: 1.1673 (70.4%) | Phys: 0.79 | Val: 0.7283 (82.06%) | Best: 82.10%\n",
            "Epoch 398 | T: 22s | Train: 1.1560 (70.8%) | Phys: 0.79 | Val: 0.7805 (81.68%) | Best: 82.10%\n",
            "Epoch 399 | T: 22s | Train: 1.1692 (70.3%) | Phys: 0.79 | Val: 0.7221 (82.13%) | Best: 82.13%\n",
            "Epoch 400 | T: 22s | Train: 1.2122 (69.1%) | Phys: 0.79 | Val: 0.8087 (81.34%) | Best: 82.13%\n",
            "Final Best: 82.13%\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": [],
      "name": "CIFAR-100 SIGReg Weak 1600 EP",
      "gpuType": "L4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}