{
  "cells": [
    {
      "cell_type": "code",
      "id": "3amcbKc5hLopVnnXSD9Wt1yi",
      "metadata": {
        "tags": [],
        "id": "3amcbKc5hLopVnnXSD9Wt1yi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "648647b0-dd68-4324-f44b-5d220d0de0f1"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "\n",
        "# ==========================================\n",
        "# 1. Configuration & Physics\n",
        "# ==========================================\n",
        "# OPTIONS: 'baseline', 'weak' (Covariance), 'strong' (LeJEPA/Epps-Pulley)\n",
        "REG_MODE = 'strong'\n",
        "SIGR_ALPHA = 0.01   # Strength of the physics constraint\n",
        "SKETCH_DIM = 64    # Dimension of the random observer\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "LEARNING_RATE = 0.1\n",
        "EPOCHS = 400\n",
        "WEIGHT_DECAY = 5e-4\n",
        "MOMENTUM = 0.9\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "MIXUP_ALPHA = 0.8\n",
        "CUTMIX_ALPHA = 1.0\n",
        "\n",
        "# Check for Apple Silicon (MPS)\n",
        "if torch.backends.mps.is_available():\n",
        "    DEVICE = 'mps'\n",
        "\n",
        "print(f\"Training on: {DEVICE} | Mode: {REG_MODE} | Alpha: {SIGR_ALPHA}\")\n",
        "\n",
        "def rand_bbox(size, lam):\n",
        "    W = size[2]\n",
        "    H = size[3]\n",
        "    cut_rat = np.sqrt(1. - lam)\n",
        "    cut_w = int(W * cut_rat)\n",
        "    cut_h = int(H * cut_rat)\n",
        "\n",
        "    cx = np.random.randint(W)\n",
        "    cy = np.random.randint(H)\n",
        "\n",
        "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
        "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
        "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
        "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
        "\n",
        "    return bbx1, bby1, bbx2, bby2\n",
        "\n",
        "# ------------------------------------------\n",
        "# Physics Engine: The Regularizers\n",
        "# ------------------------------------------\n",
        "\n",
        "def sigreg_weak_loss(x, sketch_dim=64):\n",
        "    \"\"\"\n",
        "    Forces Covariance(x) ~ Identity.\n",
        "    Matches the 2nd Moment (Spherical Cloud).\n",
        "    \"\"\"\n",
        "    N, C = x.size()\n",
        "    # 1. Sketching (Optional for C=512, but good for consistency)\n",
        "    if C > sketch_dim:\n",
        "        S = torch.randn(sketch_dim, C, device=x.device) / (C ** 0.5)\n",
        "        x = x @ S.T  # [N, sketch_dim]\n",
        "    else:\n",
        "        sketch_dim = C\n",
        "\n",
        "    # 2. Centering & Covariance\n",
        "    x = x - x.mean(dim=0, keepdim=True)\n",
        "    cov = (x.T @ x) / (N - 1 + 1e-6)\n",
        "\n",
        "    # 3. Target Identity\n",
        "    target = torch.eye(sketch_dim, device=x.device)\n",
        "\n",
        "    # 4. Off-diagonal suppression + Diagonal maintenance\n",
        "    return torch.norm(cov - target, p='fro')\n",
        "\n",
        "def sigreg_strong_loss(x, sketch_dim=64):\n",
        "    \"\"\"\n",
        "    Forces ECF(x) ~ ECF(Gaussian).\n",
        "    Matches ALL Moments (Maximum Entropy Cloud).\n",
        "    Exact implementation of LeJEPA Algorithm 1.\n",
        "    \"\"\"\n",
        "    N, C = x.size()\n",
        "\n",
        "    # 1. Projection (The Observer)\n",
        "    # Project channels down to sketch_dim\n",
        "    A = torch.randn(C, sketch_dim, device=x.device)\n",
        "    A = A / (A.norm(p=2, dim=0, keepdim=True) + 1e-6)\n",
        "\n",
        "    # 2. Integration Points\n",
        "    t = torch.linspace(-5, 5, 17, device=x.device)\n",
        "\n",
        "    # 3. Theoretical Gaussian CF\n",
        "    exp_f = torch.exp(-0.5 * t**2)\n",
        "\n",
        "    # 4. Empirical CF\n",
        "    # proj: [N, sketch_dim]\n",
        "    proj = x @ A\n",
        "\n",
        "    # args: [N, sketch_dim, T]\n",
        "    args = proj.unsqueeze(2) * t.view(1, 1, -1)\n",
        "\n",
        "    # ecf: [sketch_dim, T] (Mean over batch)\n",
        "    ecf = torch.exp(1j * args).mean(dim=0)\n",
        "\n",
        "    # 5. Weighted L2 Distance\n",
        "    # |ecf - gauss|^2 * gauss_weight\n",
        "    diff_sq = (ecf - exp_f.unsqueeze(0)).abs().square()\n",
        "    err = diff_sq * exp_f.unsqueeze(0)\n",
        "\n",
        "    # 6. Integrate\n",
        "    loss = torch.trapz(err, t, dim=1) * N\n",
        "\n",
        "    return loss.mean()\n",
        "\n",
        "# ==========================================\n",
        "# 2. Thermodynamic ResNet-18\n",
        "# ==========================================\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "\n",
        "        # --- PHYSICS INJECTION ---\n",
        "        # We calculate loss here, but we need to return it to the main model.\n",
        "        # We use Global Average Pooling to get a (N, C) vector for regularization\n",
        "        # This forces the semantic features to be isotropic.\n",
        "        reg_loss = torch.tensor(0.0, device=x.device)\n",
        "\n",
        "        if REG_MODE != 'baseline':\n",
        "            # Pool spatial dims: [N, C, H, W] -> [N, C]\n",
        "            flat = F.adaptive_avg_pool2d(out, (1, 1)).view(out.size(0), -1)\n",
        "\n",
        "            if REG_MODE == 'weak':\n",
        "                reg_loss = sigreg_weak_loss(flat, SKETCH_DIM)\n",
        "            elif REG_MODE == 'strong':\n",
        "                reg_loss = sigreg_strong_loss(flat, SKETCH_DIM)\n",
        "\n",
        "        return out, reg_loss\n",
        "\n",
        "class ThermoResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=100):\n",
        "        super(ThermoResNet, self).__init__()\n",
        "        self.in_planes = 64\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.ModuleList(layers) # Changed to ModuleList to iterate manually\n",
        "\n",
        "    def forward(self, x):\n",
        "        total_phys_loss = 0.0\n",
        "\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "\n",
        "        # Manually iterate through layers to collect physics losses\n",
        "        for layer_group in [self.layer1, self.layer2, self.layer3, self.layer4]:\n",
        "            for block in layer_group:\n",
        "                out, l_loss = block(out)\n",
        "                total_phys_loss += l_loss\n",
        "\n",
        "        out = F.avg_pool2d(out, 4)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "\n",
        "        # Normalize p_loss by number of layers to keep scale consistent\n",
        "        return out, (total_phys_loss / 8.0)\n",
        "\n",
        "def ResNet18():\n",
        "    return ThermoResNet(BasicBlock, [2, 2, 2, 2])\n",
        "\n",
        "# ==========================================\n",
        "# 3. Data Preparation\n",
        "# ==========================================\n",
        "def get_data_loaders():\n",
        "    print('==> Preparing data...')\n",
        "    mean = (0.5070751592371323, 0.48654887331495095, 0.4409178433670343)\n",
        "    std = (0.2673342858792401, 0.2564384629170883, 0.27615047132568404)\n",
        "\n",
        "    transform_train = transforms.Compose([\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean, std),\n",
        "    ])\n",
        "\n",
        "    transform_test = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean, std),\n",
        "    ])\n",
        "\n",
        "    trainset = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=transform_train)\n",
        "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
        "\n",
        "    testset = torchvision.datasets.CIFAR100(root='./data', train=False, download=True, transform=transform_test)\n",
        "    testloader = torch.utils.data.DataLoader(testset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
        "\n",
        "    return trainloader, testloader\n",
        "\n",
        "# ==========================================\n",
        "# 4. Training Engine\n",
        "# ==========================================\n",
        "def train(epoch, net, trainloader, optimizer, criterion):\n",
        "    net.train()\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    phys_loss_meter = 0\n",
        "\n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
        "\n",
        "        # Apply Mixup/CutMix\n",
        "        r = np.random.rand(1)\n",
        "        if r < 0.5: # Mixup\n",
        "            lam = np.random.beta(MIXUP_ALPHA, MIXUP_ALPHA)\n",
        "            index = torch.randperm(inputs.size(0)).to(DEVICE)\n",
        "            inputs = lam * inputs + (1 - lam) * inputs[index, :]\n",
        "            targets_a, targets_b = targets, targets[index]\n",
        "        else: # CutMix\n",
        "            lam = np.random.beta(CUTMIX_ALPHA, CUTMIX_ALPHA)\n",
        "            rand_index = torch.randperm(inputs.size(0)).to(DEVICE)\n",
        "            target_a = targets\n",
        "            target_b = targets[rand_index]\n",
        "            bbx1, bby1, bbx2, bby2 = rand_bbox(inputs.size(), lam)\n",
        "            inputs[:, :, bbx1:bbx2, bby1:bby2] = inputs[rand_index, :, bbx1:bbx2, bby1:bby2]\n",
        "            lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (inputs.size()[-1] * inputs.size()[-2]))\n",
        "            targets_a, targets_b = target_a, target_b\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward\n",
        "        outputs, p_loss = net(inputs)\n",
        "\n",
        "        # Task Loss\n",
        "        c_loss = criterion(outputs, targets_a) * lam + criterion(outputs, targets_b) * (1. - lam)\n",
        "\n",
        "        # Total Loss\n",
        "        loss = (1 - SIGR_ALPHA) * c_loss + (SIGR_ALPHA * p_loss)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += ((1 - SIGR_ALPHA) * c_loss).item() # Log only task loss for comparison\n",
        "        phys_loss_meter += (SIGR_ALPHA * p_loss).item()\n",
        "\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += (lam * predicted.eq(targets_a).float() + (1 - lam) * predicted.eq(targets_b).float()).sum().item()\n",
        "\n",
        "    acc = 100. * correct / total\n",
        "    return train_loss / (batch_idx + 1), acc, phys_loss_meter / (batch_idx + 1)\n",
        "\n",
        "def test(epoch, net, testloader, criterion):\n",
        "    net.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "            inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
        "            outputs, _ = net(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "    acc = 100. * correct / total\n",
        "    return test_loss / (batch_idx + 1), acc\n",
        "\n",
        "# ==========================================\n",
        "# 5. Main Execution\n",
        "# ==========================================\n",
        "if __name__ == '__main__':\n",
        "    trainloader, testloader = get_data_loaders()\n",
        "\n",
        "    print(f'==> Building model (Mode: {REG_MODE})...')\n",
        "    net = ResNet18()\n",
        "    net = net.to(DEVICE)\n",
        "\n",
        "    if DEVICE == 'cuda':\n",
        "        net = torch.nn.DataParallel(net)\n",
        "        cudnn.benchmark = True\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(net.parameters(), lr=LEARNING_RATE, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
        "\n",
        "    print(f\"Starting training for {EPOCHS} epochs...\")\n",
        "    best_acc = 0\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        start_time = time.time()\n",
        "\n",
        "        tr_loss, tr_acc, phys_loss = train(epoch, net, trainloader, optimizer, criterion)\n",
        "        te_loss, te_acc = test(epoch, net, testloader, criterion)\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        if te_acc > best_acc:\n",
        "            best_acc = te_acc\n",
        "            # torch.save(net.state_dict(), f'thermo_resnet_{REG_MODE}.pth')\n",
        "\n",
        "        epoch_time = time.time() - start_time\n",
        "\n",
        "        print(f\"Epoch {epoch+1:03d} | T: {epoch_time:.0f}s | \"\n",
        "              f\"Train: {tr_loss:.4f} ({tr_acc:.1f}%) | \"\n",
        "              f\"Phys: {phys_loss:.2f} | \"\n",
        "              f\"Val: {te_loss:.4f} ({te_acc:.2f}%) | \"\n",
        "              f\"Best: {best_acc:.2f}%\")\n",
        "\n",
        "    print(f\"Final Best: {best_acc:.2f}%\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on: cuda | Mode: strong | Alpha: 0.01\n",
            "==> Preparing data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/datasets/cifar.py:83: VisibleDeprecationWarning: dtype(): align should be passed as Python or NumPy boolean but got `align=0`. Did you mean to pass a tuple to create a subarray type? (Deprecated NumPy 2.4)\n",
            "  entry = pickle.load(f, encoding=\"latin1\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==> Building model (Mode: strong)...\n",
            "Starting training for 400 epochs...\n",
            "Epoch 001 | T: 24s | Train: 4.4491 (3.2%) | Phys: 0.38 | Val: 4.1227 (5.03%) | Best: 5.03%\n",
            "Epoch 002 | T: 23s | Train: 4.2122 (5.2%) | Phys: 0.32 | Val: 3.9445 (7.92%) | Best: 7.92%\n",
            "Epoch 003 | T: 23s | Train: 4.1147 (7.2%) | Phys: 0.32 | Val: 3.7519 (12.22%) | Best: 12.22%\n",
            "Epoch 004 | T: 23s | Train: 4.0154 (9.0%) | Phys: 0.32 | Val: 3.6440 (13.81%) | Best: 13.81%\n",
            "Epoch 005 | T: 23s | Train: 3.9552 (10.1%) | Phys: 0.32 | Val: 3.4771 (16.93%) | Best: 16.93%\n",
            "Epoch 006 | T: 23s | Train: 3.8342 (12.1%) | Phys: 0.33 | Val: 3.3444 (19.03%) | Best: 19.03%\n",
            "Epoch 007 | T: 23s | Train: 3.8018 (13.1%) | Phys: 0.33 | Val: 3.4722 (18.52%) | Best: 19.03%\n",
            "Epoch 008 | T: 24s | Train: 3.7277 (14.6%) | Phys: 0.34 | Val: 3.1741 (23.45%) | Best: 23.45%\n",
            "Epoch 009 | T: 23s | Train: 3.6949 (15.4%) | Phys: 0.35 | Val: 3.0692 (23.69%) | Best: 23.69%\n",
            "Epoch 010 | T: 23s | Train: 3.6194 (17.0%) | Phys: 0.35 | Val: 3.0290 (25.25%) | Best: 25.25%\n",
            "Epoch 011 | T: 23s | Train: 3.5919 (17.4%) | Phys: 0.36 | Val: 2.9679 (26.21%) | Best: 26.21%\n",
            "Epoch 012 | T: 23s | Train: 3.5543 (18.2%) | Phys: 0.36 | Val: 2.9411 (26.95%) | Best: 26.95%\n",
            "Epoch 013 | T: 23s | Train: 3.5047 (19.3%) | Phys: 0.37 | Val: 2.9107 (28.57%) | Best: 28.57%\n",
            "Epoch 014 | T: 23s | Train: 3.4733 (20.3%) | Phys: 0.38 | Val: 2.7693 (30.90%) | Best: 30.90%\n",
            "Epoch 015 | T: 23s | Train: 3.3991 (21.7%) | Phys: 0.39 | Val: 2.7306 (31.45%) | Best: 31.45%\n",
            "Epoch 016 | T: 23s | Train: 3.3582 (22.4%) | Phys: 0.39 | Val: 2.7579 (31.08%) | Best: 31.45%\n",
            "Epoch 017 | T: 23s | Train: 3.3231 (23.5%) | Phys: 0.40 | Val: 2.5752 (34.79%) | Best: 34.79%\n",
            "Epoch 018 | T: 23s | Train: 3.2544 (24.7%) | Phys: 0.41 | Val: 2.7678 (30.85%) | Best: 34.79%\n",
            "Epoch 019 | T: 23s | Train: 3.2164 (25.5%) | Phys: 0.41 | Val: 2.3831 (38.49%) | Best: 38.49%\n",
            "Epoch 020 | T: 23s | Train: 3.2189 (26.0%) | Phys: 0.42 | Val: 2.3911 (38.90%) | Best: 38.90%\n",
            "Epoch 021 | T: 23s | Train: 3.2055 (26.4%) | Phys: 0.43 | Val: 2.5462 (35.26%) | Best: 38.90%\n",
            "Epoch 022 | T: 23s | Train: 3.1653 (27.4%) | Phys: 0.43 | Val: 2.4110 (38.22%) | Best: 38.90%\n",
            "Epoch 023 | T: 23s | Train: 3.1463 (28.0%) | Phys: 0.43 | Val: 2.2839 (41.48%) | Best: 41.48%\n",
            "Epoch 024 | T: 23s | Train: 3.0612 (29.5%) | Phys: 0.43 | Val: 2.3426 (39.46%) | Best: 41.48%\n",
            "Epoch 025 | T: 23s | Train: 3.1038 (28.9%) | Phys: 0.43 | Val: 2.2329 (42.04%) | Best: 42.04%\n",
            "Epoch 026 | T: 23s | Train: 3.0321 (30.4%) | Phys: 0.44 | Val: 2.1682 (44.04%) | Best: 44.04%\n",
            "Epoch 027 | T: 23s | Train: 3.0871 (29.4%) | Phys: 0.44 | Val: 2.3010 (40.92%) | Best: 44.04%\n",
            "Epoch 028 | T: 23s | Train: 3.0134 (30.8%) | Phys: 0.44 | Val: 2.2493 (42.03%) | Best: 44.04%\n",
            "Epoch 029 | T: 23s | Train: 2.9956 (31.4%) | Phys: 0.44 | Val: 2.0669 (46.01%) | Best: 46.01%\n",
            "Epoch 030 | T: 23s | Train: 2.9648 (32.7%) | Phys: 0.44 | Val: 2.1227 (44.64%) | Best: 46.01%\n",
            "Epoch 031 | T: 23s | Train: 3.0037 (31.2%) | Phys: 0.45 | Val: 2.0349 (48.08%) | Best: 48.08%\n",
            "Epoch 032 | T: 23s | Train: 2.9472 (32.6%) | Phys: 0.45 | Val: 1.9720 (48.23%) | Best: 48.23%\n",
            "Epoch 033 | T: 23s | Train: 2.9382 (32.7%) | Phys: 0.45 | Val: 2.0936 (45.36%) | Best: 48.23%\n",
            "Epoch 034 | T: 23s | Train: 2.9148 (33.4%) | Phys: 0.45 | Val: 2.1197 (45.49%) | Best: 48.23%\n",
            "Epoch 035 | T: 23s | Train: 2.8540 (35.1%) | Phys: 0.45 | Val: 1.9108 (50.03%) | Best: 50.03%\n",
            "Epoch 036 | T: 23s | Train: 2.8997 (34.2%) | Phys: 0.46 | Val: 2.2211 (44.07%) | Best: 50.03%\n",
            "Epoch 037 | T: 23s | Train: 2.8998 (33.9%) | Phys: 0.46 | Val: 2.1033 (46.16%) | Best: 50.03%\n",
            "Epoch 038 | T: 23s | Train: 2.8691 (34.7%) | Phys: 0.46 | Val: 2.0155 (47.67%) | Best: 50.03%\n",
            "Epoch 039 | T: 23s | Train: 2.8665 (34.9%) | Phys: 0.46 | Val: 2.0801 (46.57%) | Best: 50.03%\n",
            "Epoch 040 | T: 23s | Train: 2.8645 (35.1%) | Phys: 0.47 | Val: 2.0367 (47.36%) | Best: 50.03%\n",
            "Epoch 041 | T: 23s | Train: 2.7925 (36.6%) | Phys: 0.47 | Val: 2.0053 (47.86%) | Best: 50.03%\n",
            "Epoch 042 | T: 23s | Train: 2.8067 (36.1%) | Phys: 0.47 | Val: 1.9766 (48.41%) | Best: 50.03%\n",
            "Epoch 043 | T: 23s | Train: 2.8703 (34.6%) | Phys: 0.48 | Val: 1.8526 (51.44%) | Best: 51.44%\n",
            "Epoch 044 | T: 23s | Train: 2.7728 (36.8%) | Phys: 0.48 | Val: 1.8299 (52.51%) | Best: 52.51%\n",
            "Epoch 045 | T: 23s | Train: 2.7240 (38.2%) | Phys: 0.48 | Val: 1.7844 (52.17%) | Best: 52.51%\n",
            "Epoch 046 | T: 23s | Train: 2.8092 (36.3%) | Phys: 0.48 | Val: 1.9929 (49.03%) | Best: 52.51%\n",
            "Epoch 047 | T: 23s | Train: 2.8231 (35.8%) | Phys: 0.49 | Val: 1.9436 (50.64%) | Best: 52.51%\n",
            "Epoch 048 | T: 23s | Train: 2.7278 (38.1%) | Phys: 0.49 | Val: 1.9509 (49.46%) | Best: 52.51%\n",
            "Epoch 049 | T: 23s | Train: 2.7776 (37.1%) | Phys: 0.49 | Val: 1.8787 (51.94%) | Best: 52.51%\n",
            "Epoch 050 | T: 23s | Train: 2.7445 (37.9%) | Phys: 0.49 | Val: 1.8301 (51.98%) | Best: 52.51%\n",
            "Epoch 051 | T: 23s | Train: 2.7192 (38.5%) | Phys: 0.50 | Val: 1.7881 (52.84%) | Best: 52.84%\n",
            "Epoch 052 | T: 23s | Train: 2.7703 (37.4%) | Phys: 0.50 | Val: 1.8452 (52.19%) | Best: 52.84%\n",
            "Epoch 053 | T: 23s | Train: 2.7376 (38.0%) | Phys: 0.50 | Val: 1.7190 (54.56%) | Best: 54.56%\n",
            "Epoch 054 | T: 23s | Train: 2.7817 (37.2%) | Phys: 0.50 | Val: 1.7776 (53.17%) | Best: 54.56%\n",
            "Epoch 055 | T: 23s | Train: 2.7800 (36.8%) | Phys: 0.51 | Val: 2.0895 (46.45%) | Best: 54.56%\n",
            "Epoch 056 | T: 23s | Train: 2.7608 (37.4%) | Phys: 0.51 | Val: 1.7723 (53.84%) | Best: 54.56%\n",
            "Epoch 057 | T: 23s | Train: 2.7143 (38.8%) | Phys: 0.51 | Val: 1.9362 (50.08%) | Best: 54.56%\n",
            "Epoch 058 | T: 23s | Train: 2.6282 (40.6%) | Phys: 0.51 | Val: 1.8824 (50.58%) | Best: 54.56%\n",
            "Epoch 059 | T: 23s | Train: 2.6464 (40.3%) | Phys: 0.51 | Val: 1.7109 (54.48%) | Best: 54.56%\n",
            "Epoch 060 | T: 23s | Train: 2.6803 (39.3%) | Phys: 0.51 | Val: 1.7150 (54.75%) | Best: 54.75%\n",
            "Epoch 061 | T: 23s | Train: 2.7367 (37.9%) | Phys: 0.51 | Val: 1.6475 (57.15%) | Best: 57.15%\n",
            "Epoch 062 | T: 23s | Train: 2.6293 (40.6%) | Phys: 0.51 | Val: 1.7292 (54.59%) | Best: 57.15%\n",
            "Epoch 063 | T: 23s | Train: 2.6696 (39.8%) | Phys: 0.52 | Val: 1.8186 (52.62%) | Best: 57.15%\n",
            "Epoch 064 | T: 23s | Train: 2.6733 (39.7%) | Phys: 0.52 | Val: 1.8520 (52.18%) | Best: 57.15%\n",
            "Epoch 065 | T: 23s | Train: 2.6040 (41.3%) | Phys: 0.52 | Val: 1.8087 (52.92%) | Best: 57.15%\n",
            "Epoch 066 | T: 23s | Train: 2.6240 (41.2%) | Phys: 0.52 | Val: 1.7629 (54.28%) | Best: 57.15%\n",
            "Epoch 067 | T: 23s | Train: 2.6388 (40.3%) | Phys: 0.52 | Val: 1.7613 (54.68%) | Best: 57.15%\n",
            "Epoch 068 | T: 23s | Train: 2.6388 (40.3%) | Phys: 0.52 | Val: 1.7641 (54.33%) | Best: 57.15%\n",
            "Epoch 069 | T: 23s | Train: 2.6573 (39.7%) | Phys: 0.52 | Val: 1.7469 (54.07%) | Best: 57.15%\n",
            "Epoch 070 | T: 23s | Train: 2.6437 (40.7%) | Phys: 0.52 | Val: 1.6871 (55.77%) | Best: 57.15%\n",
            "Epoch 071 | T: 23s | Train: 2.6589 (40.2%) | Phys: 0.52 | Val: 1.7431 (55.67%) | Best: 57.15%\n",
            "Epoch 072 | T: 23s | Train: 2.5597 (42.0%) | Phys: 0.52 | Val: 1.8968 (51.71%) | Best: 57.15%\n",
            "Epoch 073 | T: 23s | Train: 2.5763 (41.7%) | Phys: 0.52 | Val: 1.7834 (53.55%) | Best: 57.15%\n",
            "Epoch 074 | T: 23s | Train: 2.5331 (43.0%) | Phys: 0.52 | Val: 1.7200 (56.12%) | Best: 57.15%\n",
            "Epoch 075 | T: 23s | Train: 2.6100 (41.1%) | Phys: 0.52 | Val: 1.6401 (57.43%) | Best: 57.43%\n",
            "Epoch 076 | T: 23s | Train: 2.5877 (41.2%) | Phys: 0.52 | Val: 1.7497 (54.30%) | Best: 57.43%\n",
            "Epoch 077 | T: 23s | Train: 2.5774 (42.0%) | Phys: 0.52 | Val: 1.8293 (53.27%) | Best: 57.43%\n",
            "Epoch 078 | T: 23s | Train: 2.5421 (42.9%) | Phys: 0.52 | Val: 1.5945 (58.64%) | Best: 58.64%\n",
            "Epoch 079 | T: 23s | Train: 2.5606 (41.9%) | Phys: 0.52 | Val: 1.7134 (55.91%) | Best: 58.64%\n",
            "Epoch 080 | T: 23s | Train: 2.6269 (40.8%) | Phys: 0.52 | Val: 1.7205 (56.00%) | Best: 58.64%\n",
            "Epoch 081 | T: 23s | Train: 2.6032 (41.4%) | Phys: 0.52 | Val: 1.5587 (59.56%) | Best: 59.56%\n",
            "Epoch 082 | T: 23s | Train: 2.5862 (41.7%) | Phys: 0.52 | Val: 1.6889 (56.08%) | Best: 59.56%\n",
            "Epoch 083 | T: 23s | Train: 2.5787 (42.0%) | Phys: 0.52 | Val: 1.6991 (56.16%) | Best: 59.56%\n",
            "Epoch 084 | T: 23s | Train: 2.5343 (43.0%) | Phys: 0.52 | Val: 1.9788 (49.89%) | Best: 59.56%\n",
            "Epoch 085 | T: 23s | Train: 2.5496 (42.4%) | Phys: 0.52 | Val: 1.6918 (56.77%) | Best: 59.56%\n",
            "Epoch 086 | T: 23s | Train: 2.5260 (43.0%) | Phys: 0.52 | Val: 1.8389 (52.47%) | Best: 59.56%\n",
            "Epoch 087 | T: 23s | Train: 2.5600 (42.3%) | Phys: 0.52 | Val: 1.6947 (56.29%) | Best: 59.56%\n",
            "Epoch 088 | T: 23s | Train: 2.6086 (41.4%) | Phys: 0.52 | Val: 1.7822 (53.66%) | Best: 59.56%\n",
            "Epoch 089 | T: 23s | Train: 2.5544 (42.5%) | Phys: 0.52 | Val: 1.6798 (56.32%) | Best: 59.56%\n",
            "Epoch 090 | T: 23s | Train: 2.5255 (43.2%) | Phys: 0.52 | Val: 1.9604 (51.18%) | Best: 59.56%\n",
            "Epoch 091 | T: 23s | Train: 2.6101 (41.1%) | Phys: 0.52 | Val: 1.7587 (54.06%) | Best: 59.56%\n",
            "Epoch 092 | T: 23s | Train: 2.5988 (41.6%) | Phys: 0.52 | Val: 1.7731 (54.62%) | Best: 59.56%\n",
            "Epoch 093 | T: 23s | Train: 2.4957 (43.6%) | Phys: 0.52 | Val: 1.6160 (57.85%) | Best: 59.56%\n",
            "Epoch 094 | T: 23s | Train: 2.4802 (44.3%) | Phys: 0.52 | Val: 1.7500 (55.18%) | Best: 59.56%\n",
            "Epoch 095 | T: 23s | Train: 2.4849 (43.9%) | Phys: 0.52 | Val: 1.6017 (57.32%) | Best: 59.56%\n",
            "Epoch 096 | T: 23s | Train: 2.5366 (43.0%) | Phys: 0.52 | Val: 1.5252 (59.93%) | Best: 59.93%\n",
            "Epoch 097 | T: 23s | Train: 2.5042 (43.6%) | Phys: 0.52 | Val: 1.6177 (58.29%) | Best: 59.93%\n",
            "Epoch 098 | T: 23s | Train: 2.5050 (43.4%) | Phys: 0.52 | Val: 1.6057 (56.79%) | Best: 59.93%\n",
            "Epoch 099 | T: 23s | Train: 2.5532 (42.8%) | Phys: 0.52 | Val: 1.5698 (58.46%) | Best: 59.93%\n",
            "Epoch 100 | T: 23s | Train: 2.5273 (43.5%) | Phys: 0.52 | Val: 1.5656 (59.34%) | Best: 59.93%\n",
            "Epoch 101 | T: 23s | Train: 2.5188 (43.1%) | Phys: 0.52 | Val: 1.9743 (51.27%) | Best: 59.93%\n",
            "Epoch 102 | T: 23s | Train: 2.5286 (42.8%) | Phys: 0.52 | Val: 1.7552 (54.94%) | Best: 59.93%\n",
            "Epoch 103 | T: 23s | Train: 2.4495 (44.7%) | Phys: 0.52 | Val: 1.5979 (57.90%) | Best: 59.93%\n",
            "Epoch 104 | T: 23s | Train: 2.4919 (44.0%) | Phys: 0.52 | Val: 1.7060 (55.00%) | Best: 59.93%\n",
            "Epoch 105 | T: 23s | Train: 2.5111 (43.6%) | Phys: 0.52 | Val: 1.6636 (57.20%) | Best: 59.93%\n",
            "Epoch 106 | T: 23s | Train: 2.5186 (43.2%) | Phys: 0.52 | Val: 1.5995 (57.87%) | Best: 59.93%\n",
            "Epoch 107 | T: 23s | Train: 2.4876 (44.0%) | Phys: 0.52 | Val: 1.5219 (59.12%) | Best: 59.93%\n",
            "Epoch 108 | T: 23s | Train: 2.4757 (44.7%) | Phys: 0.52 | Val: 1.6416 (57.01%) | Best: 59.93%\n",
            "Epoch 109 | T: 23s | Train: 2.5158 (43.3%) | Phys: 0.52 | Val: 1.8200 (53.89%) | Best: 59.93%\n",
            "Epoch 110 | T: 23s | Train: 2.5144 (43.6%) | Phys: 0.52 | Val: 1.6479 (56.81%) | Best: 59.93%\n",
            "Epoch 111 | T: 23s | Train: 2.4767 (44.4%) | Phys: 0.52 | Val: 1.6758 (56.50%) | Best: 59.93%\n",
            "Epoch 112 | T: 23s | Train: 2.5313 (43.2%) | Phys: 0.52 | Val: 1.6165 (58.39%) | Best: 59.93%\n",
            "Epoch 113 | T: 23s | Train: 2.5488 (42.4%) | Phys: 0.52 | Val: 1.5220 (59.46%) | Best: 59.93%\n",
            "Epoch 114 | T: 23s | Train: 2.4760 (44.4%) | Phys: 0.52 | Val: 1.5253 (59.17%) | Best: 59.93%\n",
            "Epoch 115 | T: 23s | Train: 2.5526 (42.4%) | Phys: 0.52 | Val: 1.6283 (57.75%) | Best: 59.93%\n",
            "Epoch 116 | T: 23s | Train: 2.4850 (43.8%) | Phys: 0.52 | Val: 1.5084 (59.89%) | Best: 59.93%\n",
            "Epoch 117 | T: 23s | Train: 2.4624 (44.4%) | Phys: 0.52 | Val: 1.5526 (60.12%) | Best: 60.12%\n",
            "Epoch 118 | T: 24s | Train: 2.4964 (43.5%) | Phys: 0.52 | Val: 1.5989 (58.99%) | Best: 60.12%\n",
            "Epoch 119 | T: 23s | Train: 2.4497 (45.3%) | Phys: 0.52 | Val: 1.5539 (59.48%) | Best: 60.12%\n",
            "Epoch 120 | T: 23s | Train: 2.4961 (43.8%) | Phys: 0.52 | Val: 1.6108 (59.06%) | Best: 60.12%\n",
            "Epoch 121 | T: 23s | Train: 2.5187 (43.2%) | Phys: 0.52 | Val: 1.4900 (60.31%) | Best: 60.31%\n",
            "Epoch 122 | T: 23s | Train: 2.5044 (43.8%) | Phys: 0.52 | Val: 1.4540 (61.01%) | Best: 61.01%\n",
            "Epoch 123 | T: 23s | Train: 2.4743 (44.4%) | Phys: 0.52 | Val: 1.5610 (59.04%) | Best: 61.01%\n",
            "Epoch 124 | T: 23s | Train: 2.4482 (44.9%) | Phys: 0.52 | Val: 1.5674 (59.01%) | Best: 61.01%\n",
            "Epoch 125 | T: 23s | Train: 2.4597 (44.7%) | Phys: 0.52 | Val: 1.5205 (59.93%) | Best: 61.01%\n",
            "Epoch 126 | T: 23s | Train: 2.4400 (44.9%) | Phys: 0.52 | Val: 1.5091 (59.86%) | Best: 61.01%\n",
            "Epoch 127 | T: 23s | Train: 2.4400 (44.9%) | Phys: 0.52 | Val: 1.4486 (62.00%) | Best: 62.00%\n",
            "Epoch 128 | T: 23s | Train: 2.4413 (45.2%) | Phys: 0.52 | Val: 1.5748 (60.20%) | Best: 62.00%\n",
            "Epoch 129 | T: 23s | Train: 2.4600 (45.0%) | Phys: 0.52 | Val: 1.5015 (61.11%) | Best: 62.00%\n",
            "Epoch 130 | T: 23s | Train: 2.4520 (45.0%) | Phys: 0.52 | Val: 1.6181 (58.88%) | Best: 62.00%\n",
            "Epoch 131 | T: 23s | Train: 2.4239 (45.5%) | Phys: 0.52 | Val: 1.6590 (57.72%) | Best: 62.00%\n",
            "Epoch 132 | T: 23s | Train: 2.4671 (44.5%) | Phys: 0.52 | Val: 1.7075 (55.59%) | Best: 62.00%\n",
            "Epoch 133 | T: 23s | Train: 2.4193 (45.5%) | Phys: 0.52 | Val: 1.6300 (59.24%) | Best: 62.00%\n",
            "Epoch 134 | T: 23s | Train: 2.4528 (44.9%) | Phys: 0.52 | Val: 1.4991 (60.66%) | Best: 62.00%\n",
            "Epoch 135 | T: 23s | Train: 2.4142 (46.0%) | Phys: 0.52 | Val: 1.6291 (57.92%) | Best: 62.00%\n",
            "Epoch 136 | T: 23s | Train: 2.4302 (45.4%) | Phys: 0.52 | Val: 1.4062 (62.67%) | Best: 62.67%\n",
            "Epoch 137 | T: 23s | Train: 2.4468 (45.0%) | Phys: 0.52 | Val: 1.4670 (61.38%) | Best: 62.67%\n",
            "Epoch 138 | T: 23s | Train: 2.4147 (45.8%) | Phys: 0.52 | Val: 1.4979 (59.97%) | Best: 62.67%\n",
            "Epoch 139 | T: 23s | Train: 2.4051 (46.1%) | Phys: 0.52 | Val: 1.7213 (56.93%) | Best: 62.67%\n",
            "Epoch 140 | T: 23s | Train: 2.4703 (44.5%) | Phys: 0.52 | Val: 1.7270 (55.56%) | Best: 62.67%\n",
            "Epoch 141 | T: 23s | Train: 2.4924 (44.1%) | Phys: 0.52 | Val: 1.5519 (59.83%) | Best: 62.67%\n",
            "Epoch 142 | T: 23s | Train: 2.4108 (45.7%) | Phys: 0.52 | Val: 1.8823 (52.34%) | Best: 62.67%\n",
            "Epoch 143 | T: 23s | Train: 2.4438 (44.9%) | Phys: 0.52 | Val: 1.4088 (63.42%) | Best: 63.42%\n",
            "Epoch 144 | T: 23s | Train: 2.3918 (46.1%) | Phys: 0.52 | Val: 1.5754 (58.14%) | Best: 63.42%\n",
            "Epoch 145 | T: 23s | Train: 2.4417 (45.2%) | Phys: 0.52 | Val: 1.4595 (61.77%) | Best: 63.42%\n",
            "Epoch 146 | T: 23s | Train: 2.3904 (46.0%) | Phys: 0.52 | Val: 1.3880 (62.52%) | Best: 63.42%\n",
            "Epoch 147 | T: 23s | Train: 2.3760 (47.1%) | Phys: 0.52 | Val: 1.4444 (61.91%) | Best: 63.42%\n",
            "Epoch 148 | T: 23s | Train: 2.4299 (45.5%) | Phys: 0.52 | Val: 1.5369 (60.25%) | Best: 63.42%\n",
            "Epoch 149 | T: 23s | Train: 2.3532 (47.3%) | Phys: 0.52 | Val: 1.5146 (60.61%) | Best: 63.42%\n",
            "Epoch 150 | T: 23s | Train: 2.4552 (44.8%) | Phys: 0.52 | Val: 1.4930 (60.86%) | Best: 63.42%\n",
            "Epoch 151 | T: 23s | Train: 2.4770 (44.2%) | Phys: 0.52 | Val: 1.4570 (62.44%) | Best: 63.42%\n",
            "Epoch 152 | T: 23s | Train: 2.3933 (46.4%) | Phys: 0.52 | Val: 1.4551 (61.70%) | Best: 63.42%\n",
            "Epoch 153 | T: 23s | Train: 2.4061 (45.7%) | Phys: 0.52 | Val: 1.6408 (58.24%) | Best: 63.42%\n",
            "Epoch 154 | T: 23s | Train: 2.3330 (48.1%) | Phys: 0.52 | Val: 1.5318 (59.79%) | Best: 63.42%\n",
            "Epoch 155 | T: 23s | Train: 2.4523 (44.9%) | Phys: 0.52 | Val: 1.5330 (59.96%) | Best: 63.42%\n",
            "Epoch 156 | T: 23s | Train: 2.3624 (46.9%) | Phys: 0.52 | Val: 1.5733 (59.10%) | Best: 63.42%\n",
            "Epoch 157 | T: 23s | Train: 2.3354 (47.5%) | Phys: 0.52 | Val: 1.4146 (62.68%) | Best: 63.42%\n",
            "Epoch 158 | T: 23s | Train: 2.4059 (46.2%) | Phys: 0.52 | Val: 1.4458 (62.88%) | Best: 63.42%\n",
            "Epoch 159 | T: 23s | Train: 2.3969 (46.2%) | Phys: 0.52 | Val: 1.6094 (58.27%) | Best: 63.42%\n",
            "Epoch 160 | T: 23s | Train: 2.3608 (47.1%) | Phys: 0.52 | Val: 1.5365 (59.77%) | Best: 63.42%\n",
            "Epoch 161 | T: 23s | Train: 2.3677 (47.0%) | Phys: 0.52 | Val: 1.4805 (61.32%) | Best: 63.42%\n",
            "Epoch 162 | T: 23s | Train: 2.3774 (46.8%) | Phys: 0.52 | Val: 1.5529 (60.19%) | Best: 63.42%\n",
            "Epoch 163 | T: 23s | Train: 2.3897 (46.4%) | Phys: 0.52 | Val: 1.6157 (58.46%) | Best: 63.42%\n",
            "Epoch 164 | T: 23s | Train: 2.3429 (47.3%) | Phys: 0.52 | Val: 1.4660 (61.10%) | Best: 63.42%\n",
            "Epoch 165 | T: 23s | Train: 2.3862 (46.1%) | Phys: 0.52 | Val: 1.4399 (62.67%) | Best: 63.42%\n",
            "Epoch 166 | T: 23s | Train: 2.4006 (46.1%) | Phys: 0.52 | Val: 1.3899 (63.01%) | Best: 63.42%\n",
            "Epoch 167 | T: 23s | Train: 2.3552 (47.2%) | Phys: 0.52 | Val: 1.4353 (63.46%) | Best: 63.46%\n",
            "Epoch 168 | T: 23s | Train: 2.3030 (48.4%) | Phys: 0.51 | Val: 1.5331 (60.44%) | Best: 63.46%\n",
            "Epoch 169 | T: 23s | Train: 2.3314 (48.1%) | Phys: 0.52 | Val: 1.4706 (61.36%) | Best: 63.46%\n",
            "Epoch 170 | T: 23s | Train: 2.4064 (46.1%) | Phys: 0.52 | Val: 1.5607 (59.36%) | Best: 63.46%\n",
            "Epoch 171 | T: 23s | Train: 2.2919 (48.9%) | Phys: 0.51 | Val: 1.3739 (63.87%) | Best: 63.87%\n",
            "Epoch 172 | T: 23s | Train: 2.3445 (47.3%) | Phys: 0.51 | Val: 1.3498 (64.50%) | Best: 64.50%\n",
            "Epoch 173 | T: 23s | Train: 2.3236 (48.0%) | Phys: 0.51 | Val: 1.4074 (63.00%) | Best: 64.50%\n",
            "Epoch 174 | T: 23s | Train: 2.3119 (47.9%) | Phys: 0.51 | Val: 1.3445 (64.69%) | Best: 64.69%\n",
            "Epoch 175 | T: 23s | Train: 2.3444 (47.6%) | Phys: 0.51 | Val: 1.5525 (59.31%) | Best: 64.69%\n",
            "Epoch 176 | T: 23s | Train: 2.3581 (47.3%) | Phys: 0.52 | Val: 1.4255 (63.19%) | Best: 64.69%\n",
            "Epoch 177 | T: 23s | Train: 2.3059 (48.3%) | Phys: 0.51 | Val: 1.4336 (63.17%) | Best: 64.69%\n",
            "Epoch 178 | T: 23s | Train: 2.3371 (48.0%) | Phys: 0.51 | Val: 1.3667 (64.90%) | Best: 64.90%\n",
            "Epoch 179 | T: 23s | Train: 2.3163 (48.2%) | Phys: 0.51 | Val: 1.4263 (63.80%) | Best: 64.90%\n",
            "Epoch 180 | T: 23s | Train: 2.2509 (49.6%) | Phys: 0.51 | Val: 1.3862 (64.47%) | Best: 64.90%\n",
            "Epoch 181 | T: 23s | Train: 2.2637 (49.4%) | Phys: 0.51 | Val: 1.4744 (61.98%) | Best: 64.90%\n",
            "Epoch 182 | T: 23s | Train: 2.3172 (48.2%) | Phys: 0.51 | Val: 1.4164 (63.73%) | Best: 64.90%\n",
            "Epoch 183 | T: 23s | Train: 2.2749 (48.9%) | Phys: 0.51 | Val: 1.3682 (64.29%) | Best: 64.90%\n",
            "Epoch 184 | T: 23s | Train: 2.2984 (48.6%) | Phys: 0.51 | Val: 1.6087 (59.46%) | Best: 64.90%\n",
            "Epoch 185 | T: 23s | Train: 2.2869 (48.9%) | Phys: 0.51 | Val: 1.5223 (61.66%) | Best: 64.90%\n",
            "Epoch 186 | T: 23s | Train: 2.3127 (48.7%) | Phys: 0.51 | Val: 1.3972 (64.30%) | Best: 64.90%\n",
            "Epoch 187 | T: 23s | Train: 2.2557 (49.8%) | Phys: 0.51 | Val: 1.4146 (63.41%) | Best: 64.90%\n",
            "Epoch 188 | T: 23s | Train: 2.3026 (48.4%) | Phys: 0.51 | Val: 1.3819 (64.37%) | Best: 64.90%\n",
            "Epoch 189 | T: 23s | Train: 2.2449 (49.8%) | Phys: 0.51 | Val: 1.2655 (65.76%) | Best: 65.76%\n",
            "Epoch 190 | T: 23s | Train: 2.2666 (49.4%) | Phys: 0.51 | Val: 1.4307 (63.64%) | Best: 65.76%\n",
            "Epoch 191 | T: 23s | Train: 2.2883 (48.7%) | Phys: 0.51 | Val: 1.3723 (64.69%) | Best: 65.76%\n",
            "Epoch 192 | T: 23s | Train: 2.2834 (49.1%) | Phys: 0.51 | Val: 1.3290 (65.66%) | Best: 65.76%\n",
            "Epoch 193 | T: 23s | Train: 2.2533 (49.5%) | Phys: 0.51 | Val: 1.3500 (64.32%) | Best: 65.76%\n",
            "Epoch 194 | T: 23s | Train: 2.2597 (49.4%) | Phys: 0.51 | Val: 1.3531 (64.14%) | Best: 65.76%\n",
            "Epoch 195 | T: 23s | Train: 2.2579 (49.6%) | Phys: 0.51 | Val: 1.3201 (66.13%) | Best: 66.13%\n",
            "Epoch 196 | T: 23s | Train: 2.2424 (50.0%) | Phys: 0.51 | Val: 1.2977 (66.00%) | Best: 66.13%\n",
            "Epoch 197 | T: 23s | Train: 2.2434 (50.2%) | Phys: 0.51 | Val: 1.3062 (65.97%) | Best: 66.13%\n",
            "Epoch 198 | T: 23s | Train: 2.2109 (50.7%) | Phys: 0.51 | Val: 1.3843 (64.55%) | Best: 66.13%\n",
            "Epoch 199 | T: 23s | Train: 2.2265 (50.5%) | Phys: 0.51 | Val: 1.3683 (64.04%) | Best: 66.13%\n",
            "Epoch 200 | T: 23s | Train: 2.2366 (50.2%) | Phys: 0.51 | Val: 1.4173 (63.09%) | Best: 66.13%\n",
            "Epoch 201 | T: 23s | Train: 2.2488 (49.7%) | Phys: 0.51 | Val: 1.4096 (63.20%) | Best: 66.13%\n",
            "Epoch 202 | T: 23s | Train: 2.2232 (50.5%) | Phys: 0.51 | Val: 1.2584 (66.47%) | Best: 66.47%\n",
            "Epoch 203 | T: 23s | Train: 2.2973 (48.5%) | Phys: 0.51 | Val: 1.3493 (65.39%) | Best: 66.47%\n",
            "Epoch 204 | T: 23s | Train: 2.2374 (50.0%) | Phys: 0.51 | Val: 1.2674 (67.39%) | Best: 67.39%\n",
            "Epoch 205 | T: 23s | Train: 2.2075 (51.3%) | Phys: 0.51 | Val: 1.3733 (65.23%) | Best: 67.39%\n",
            "Epoch 206 | T: 23s | Train: 2.2065 (51.0%) | Phys: 0.51 | Val: 1.2449 (66.57%) | Best: 67.39%\n",
            "Epoch 207 | T: 23s | Train: 2.2213 (50.8%) | Phys: 0.51 | Val: 1.2741 (67.44%) | Best: 67.44%\n",
            "Epoch 208 | T: 23s | Train: 2.2195 (50.5%) | Phys: 0.51 | Val: 1.3234 (65.26%) | Best: 67.44%\n",
            "Epoch 209 | T: 23s | Train: 2.2221 (50.5%) | Phys: 0.51 | Val: 1.2452 (66.65%) | Best: 67.44%\n",
            "Epoch 210 | T: 23s | Train: 2.2327 (50.0%) | Phys: 0.51 | Val: 1.2681 (66.33%) | Best: 67.44%\n",
            "Epoch 211 | T: 23s | Train: 2.1916 (51.4%) | Phys: 0.51 | Val: 1.3069 (66.00%) | Best: 67.44%\n",
            "Epoch 212 | T: 23s | Train: 2.1954 (51.5%) | Phys: 0.51 | Val: 1.3588 (65.95%) | Best: 67.44%\n",
            "Epoch 213 | T: 23s | Train: 2.2044 (50.3%) | Phys: 0.51 | Val: 1.3577 (66.58%) | Best: 67.44%\n",
            "Epoch 214 | T: 23s | Train: 2.1739 (51.7%) | Phys: 0.51 | Val: 1.2655 (67.74%) | Best: 67.74%\n",
            "Epoch 215 | T: 23s | Train: 2.0908 (53.8%) | Phys: 0.51 | Val: 1.4159 (65.10%) | Best: 67.74%\n",
            "Epoch 216 | T: 23s | Train: 2.1700 (52.0%) | Phys: 0.51 | Val: 1.3658 (64.76%) | Best: 67.74%\n",
            "Epoch 217 | T: 23s | Train: 2.2139 (50.5%) | Phys: 0.51 | Val: 1.2771 (67.17%) | Best: 67.74%\n",
            "Epoch 218 | T: 23s | Train: 2.1602 (52.3%) | Phys: 0.51 | Val: 1.2359 (67.39%) | Best: 67.74%\n",
            "Epoch 219 | T: 23s | Train: 2.1498 (52.1%) | Phys: 0.51 | Val: 1.2564 (66.90%) | Best: 67.74%\n",
            "Epoch 220 | T: 23s | Train: 2.2387 (50.0%) | Phys: 0.51 | Val: 1.2278 (68.12%) | Best: 68.12%\n",
            "Epoch 221 | T: 23s | Train: 2.1708 (51.4%) | Phys: 0.51 | Val: 1.3059 (65.51%) | Best: 68.12%\n",
            "Epoch 222 | T: 23s | Train: 2.2001 (50.8%) | Phys: 0.51 | Val: 1.2857 (66.48%) | Best: 68.12%\n",
            "Epoch 223 | T: 23s | Train: 2.1416 (52.3%) | Phys: 0.51 | Val: 1.2233 (67.52%) | Best: 68.12%\n",
            "Epoch 224 | T: 23s | Train: 2.1087 (53.0%) | Phys: 0.51 | Val: 1.1886 (68.95%) | Best: 68.95%\n",
            "Epoch 225 | T: 23s | Train: 2.1505 (52.3%) | Phys: 0.51 | Val: 1.2179 (68.02%) | Best: 68.95%\n",
            "Epoch 226 | T: 23s | Train: 2.2316 (49.9%) | Phys: 0.51 | Val: 1.2395 (67.24%) | Best: 68.95%\n",
            "Epoch 227 | T: 23s | Train: 2.1775 (51.7%) | Phys: 0.51 | Val: 1.1926 (69.13%) | Best: 69.13%\n",
            "Epoch 228 | T: 23s | Train: 2.1615 (51.5%) | Phys: 0.51 | Val: 1.2008 (68.89%) | Best: 69.13%\n",
            "Epoch 229 | T: 23s | Train: 2.1288 (52.5%) | Phys: 0.51 | Val: 1.1891 (67.91%) | Best: 69.13%\n",
            "Epoch 230 | T: 23s | Train: 2.1380 (52.5%) | Phys: 0.51 | Val: 1.2086 (69.78%) | Best: 69.78%\n",
            "Epoch 231 | T: 23s | Train: 2.1376 (52.3%) | Phys: 0.51 | Val: 1.2548 (68.35%) | Best: 69.78%\n",
            "Epoch 232 | T: 23s | Train: 2.0874 (53.6%) | Phys: 0.51 | Val: 1.3020 (67.11%) | Best: 69.78%\n",
            "Epoch 233 | T: 23s | Train: 2.0836 (53.6%) | Phys: 0.51 | Val: 1.1650 (68.95%) | Best: 69.78%\n",
            "Epoch 234 | T: 23s | Train: 2.0811 (54.1%) | Phys: 0.51 | Val: 1.2728 (67.13%) | Best: 69.78%\n",
            "Epoch 235 | T: 23s | Train: 2.0544 (54.6%) | Phys: 0.51 | Val: 1.2385 (67.98%) | Best: 69.78%\n",
            "Epoch 236 | T: 23s | Train: 2.1215 (53.1%) | Phys: 0.51 | Val: 1.2244 (67.58%) | Best: 69.78%\n",
            "Epoch 237 | T: 23s | Train: 2.0865 (54.0%) | Phys: 0.51 | Val: 1.2441 (68.19%) | Best: 69.78%\n",
            "Epoch 238 | T: 23s | Train: 2.1522 (51.9%) | Phys: 0.51 | Val: 1.2832 (67.83%) | Best: 69.78%\n",
            "Epoch 239 | T: 23s | Train: 2.0832 (53.8%) | Phys: 0.51 | Val: 1.1657 (69.19%) | Best: 69.78%\n",
            "Epoch 240 | T: 23s | Train: 2.1091 (53.7%) | Phys: 0.51 | Val: 1.1353 (70.34%) | Best: 70.34%\n",
            "Epoch 241 | T: 23s | Train: 2.0785 (54.0%) | Phys: 0.51 | Val: 1.2759 (67.80%) | Best: 70.34%\n",
            "Epoch 242 | T: 23s | Train: 2.0883 (53.8%) | Phys: 0.51 | Val: 1.1795 (69.10%) | Best: 70.34%\n",
            "Epoch 243 | T: 23s | Train: 2.0746 (54.0%) | Phys: 0.51 | Val: 1.1966 (69.70%) | Best: 70.34%\n",
            "Epoch 244 | T: 23s | Train: 2.0842 (54.0%) | Phys: 0.51 | Val: 1.3066 (67.28%) | Best: 70.34%\n",
            "Epoch 245 | T: 23s | Train: 2.0314 (55.2%) | Phys: 0.51 | Val: 1.2571 (67.19%) | Best: 70.34%\n",
            "Epoch 246 | T: 23s | Train: 2.0581 (54.1%) | Phys: 0.51 | Val: 1.2112 (68.70%) | Best: 70.34%\n",
            "Epoch 247 | T: 23s | Train: 2.0513 (55.1%) | Phys: 0.51 | Val: 1.2031 (69.72%) | Best: 70.34%\n",
            "Epoch 248 | T: 23s | Train: 2.0965 (53.2%) | Phys: 0.51 | Val: 1.1663 (69.72%) | Best: 70.34%\n",
            "Epoch 249 | T: 23s | Train: 2.0657 (54.3%) | Phys: 0.51 | Val: 1.2008 (69.73%) | Best: 70.34%\n",
            "Epoch 250 | T: 23s | Train: 1.9938 (56.2%) | Phys: 0.51 | Val: 1.1963 (69.40%) | Best: 70.34%\n",
            "Epoch 251 | T: 23s | Train: 2.0169 (55.8%) | Phys: 0.51 | Val: 1.1260 (69.79%) | Best: 70.34%\n",
            "Epoch 252 | T: 23s | Train: 2.0935 (53.5%) | Phys: 0.51 | Val: 1.0519 (71.75%) | Best: 71.75%\n",
            "Epoch 253 | T: 23s | Train: 2.0411 (55.0%) | Phys: 0.51 | Val: 1.1497 (70.37%) | Best: 71.75%\n",
            "Epoch 254 | T: 23s | Train: 2.0370 (55.2%) | Phys: 0.51 | Val: 1.1994 (70.27%) | Best: 71.75%\n",
            "Epoch 255 | T: 23s | Train: 2.0028 (55.5%) | Phys: 0.51 | Val: 1.1943 (69.45%) | Best: 71.75%\n",
            "Epoch 256 | T: 23s | Train: 1.9895 (56.0%) | Phys: 0.51 | Val: 1.2251 (68.98%) | Best: 71.75%\n",
            "Epoch 257 | T: 23s | Train: 1.9700 (56.3%) | Phys: 0.51 | Val: 1.1303 (71.12%) | Best: 71.75%\n",
            "Epoch 258 | T: 23s | Train: 2.0104 (55.8%) | Phys: 0.51 | Val: 1.1725 (69.57%) | Best: 71.75%\n",
            "Epoch 259 | T: 23s | Train: 1.9673 (57.0%) | Phys: 0.51 | Val: 1.1191 (71.65%) | Best: 71.75%\n",
            "Epoch 260 | T: 23s | Train: 1.9467 (57.4%) | Phys: 0.51 | Val: 1.1536 (70.71%) | Best: 71.75%\n",
            "Epoch 261 | T: 23s | Train: 2.0127 (55.4%) | Phys: 0.51 | Val: 1.1447 (69.84%) | Best: 71.75%\n",
            "Epoch 262 | T: 23s | Train: 2.0168 (55.0%) | Phys: 0.51 | Val: 1.1969 (69.95%) | Best: 71.75%\n",
            "Epoch 263 | T: 23s | Train: 1.9359 (57.4%) | Phys: 0.51 | Val: 1.0819 (71.60%) | Best: 71.75%\n",
            "Epoch 264 | T: 23s | Train: 1.9919 (55.9%) | Phys: 0.51 | Val: 1.1780 (70.42%) | Best: 71.75%\n",
            "Epoch 265 | T: 23s | Train: 1.9671 (56.6%) | Phys: 0.51 | Val: 1.1440 (71.24%) | Best: 71.75%\n",
            "Epoch 266 | T: 23s | Train: 2.0624 (54.4%) | Phys: 0.51 | Val: 1.1503 (71.24%) | Best: 71.75%\n",
            "Epoch 267 | T: 23s | Train: 1.9550 (56.5%) | Phys: 0.51 | Val: 1.0955 (71.87%) | Best: 71.87%\n",
            "Epoch 268 | T: 23s | Train: 1.9643 (56.2%) | Phys: 0.51 | Val: 1.0933 (72.09%) | Best: 72.09%\n",
            "Epoch 269 | T: 23s | Train: 1.9491 (57.4%) | Phys: 0.51 | Val: 1.0997 (71.84%) | Best: 72.09%\n",
            "Epoch 270 | T: 23s | Train: 1.9989 (55.6%) | Phys: 0.51 | Val: 1.0722 (71.77%) | Best: 72.09%\n",
            "Epoch 271 | T: 23s | Train: 1.9126 (58.1%) | Phys: 0.51 | Val: 1.1278 (70.96%) | Best: 72.09%\n",
            "Epoch 272 | T: 23s | Train: 1.8976 (58.5%) | Phys: 0.51 | Val: 1.0703 (71.82%) | Best: 72.09%\n",
            "Epoch 273 | T: 23s | Train: 1.9370 (57.4%) | Phys: 0.51 | Val: 1.0890 (72.24%) | Best: 72.24%\n",
            "Epoch 274 | T: 23s | Train: 1.9410 (57.6%) | Phys: 0.51 | Val: 1.1589 (70.36%) | Best: 72.24%\n",
            "Epoch 275 | T: 23s | Train: 1.9350 (56.9%) | Phys: 0.51 | Val: 1.0897 (72.50%) | Best: 72.50%\n",
            "Epoch 276 | T: 23s | Train: 1.9007 (58.5%) | Phys: 0.51 | Val: 1.0210 (73.14%) | Best: 73.14%\n",
            "Epoch 277 | T: 23s | Train: 1.9042 (57.9%) | Phys: 0.51 | Val: 1.0288 (73.03%) | Best: 73.14%\n",
            "Epoch 278 | T: 23s | Train: 1.8734 (58.9%) | Phys: 0.51 | Val: 1.0579 (72.70%) | Best: 73.14%\n",
            "Epoch 279 | T: 23s | Train: 1.8925 (58.3%) | Phys: 0.51 | Val: 1.0749 (72.31%) | Best: 73.14%\n",
            "Epoch 280 | T: 23s | Train: 1.9192 (57.4%) | Phys: 0.51 | Val: 1.2361 (68.65%) | Best: 73.14%\n",
            "Epoch 281 | T: 23s | Train: 1.8383 (60.1%) | Phys: 0.51 | Val: 1.0297 (73.43%) | Best: 73.43%\n",
            "Epoch 282 | T: 23s | Train: 1.8979 (57.8%) | Phys: 0.51 | Val: 1.0600 (72.01%) | Best: 73.43%\n",
            "Epoch 283 | T: 23s | Train: 1.8399 (60.0%) | Phys: 0.51 | Val: 1.0335 (73.34%) | Best: 73.43%\n",
            "Epoch 284 | T: 23s | Train: 1.8586 (58.9%) | Phys: 0.51 | Val: 1.0345 (73.34%) | Best: 73.43%\n",
            "Epoch 285 | T: 23s | Train: 1.8486 (59.0%) | Phys: 0.51 | Val: 1.0293 (73.36%) | Best: 73.43%\n",
            "Epoch 286 | T: 23s | Train: 1.8650 (59.7%) | Phys: 0.51 | Val: 1.0935 (73.09%) | Best: 73.43%\n",
            "Epoch 287 | T: 23s | Train: 1.8334 (59.9%) | Phys: 0.51 | Val: 1.0468 (73.34%) | Best: 73.43%\n",
            "Epoch 288 | T: 23s | Train: 1.8804 (58.4%) | Phys: 0.51 | Val: 1.1226 (72.78%) | Best: 73.43%\n",
            "Epoch 289 | T: 23s | Train: 1.8414 (60.0%) | Phys: 0.51 | Val: 1.0648 (73.20%) | Best: 73.43%\n",
            "Epoch 290 | T: 23s | Train: 1.8336 (60.1%) | Phys: 0.51 | Val: 1.0729 (72.59%) | Best: 73.43%\n",
            "Epoch 291 | T: 23s | Train: 1.8358 (59.6%) | Phys: 0.51 | Val: 1.0117 (74.54%) | Best: 74.54%\n",
            "Epoch 292 | T: 23s | Train: 1.8306 (60.2%) | Phys: 0.51 | Val: 1.1361 (72.20%) | Best: 74.54%\n",
            "Epoch 293 | T: 23s | Train: 1.8727 (58.2%) | Phys: 0.51 | Val: 0.9849 (74.56%) | Best: 74.56%\n",
            "Epoch 294 | T: 23s | Train: 1.7617 (61.8%) | Phys: 0.51 | Val: 1.0013 (74.30%) | Best: 74.56%\n",
            "Epoch 295 | T: 23s | Train: 1.8904 (58.2%) | Phys: 0.51 | Val: 1.0350 (74.29%) | Best: 74.56%\n",
            "Epoch 296 | T: 23s | Train: 1.8575 (59.0%) | Phys: 0.51 | Val: 0.9927 (74.40%) | Best: 74.56%\n",
            "Epoch 297 | T: 23s | Train: 1.7582 (61.7%) | Phys: 0.51 | Val: 1.0589 (73.41%) | Best: 74.56%\n",
            "Epoch 298 | T: 24s | Train: 1.7790 (60.9%) | Phys: 0.51 | Val: 0.9887 (74.91%) | Best: 74.91%\n",
            "Epoch 299 | T: 23s | Train: 1.7649 (61.5%) | Phys: 0.51 | Val: 0.9870 (74.68%) | Best: 74.91%\n",
            "Epoch 300 | T: 23s | Train: 1.8052 (60.2%) | Phys: 0.51 | Val: 1.0070 (73.92%) | Best: 74.91%\n",
            "Epoch 301 | T: 23s | Train: 1.7681 (61.1%) | Phys: 0.51 | Val: 1.0065 (74.25%) | Best: 74.91%\n",
            "Epoch 302 | T: 23s | Train: 1.7449 (62.1%) | Phys: 0.51 | Val: 1.0464 (74.13%) | Best: 74.91%\n",
            "Epoch 303 | T: 23s | Train: 1.7895 (60.9%) | Phys: 0.51 | Val: 0.9636 (75.54%) | Best: 75.54%\n",
            "Epoch 304 | T: 23s | Train: 1.7417 (62.1%) | Phys: 0.51 | Val: 0.9727 (75.78%) | Best: 75.78%\n",
            "Epoch 305 | T: 23s | Train: 1.7462 (62.3%) | Phys: 0.51 | Val: 1.0186 (74.83%) | Best: 75.78%\n",
            "Epoch 306 | T: 23s | Train: 1.7242 (62.1%) | Phys: 0.51 | Val: 0.9769 (75.29%) | Best: 75.78%\n",
            "Epoch 307 | T: 23s | Train: 1.7192 (62.9%) | Phys: 0.51 | Val: 1.0274 (74.22%) | Best: 75.78%\n",
            "Epoch 308 | T: 23s | Train: 1.7789 (60.9%) | Phys: 0.51 | Val: 1.0058 (74.38%) | Best: 75.78%\n",
            "Epoch 309 | T: 23s | Train: 1.7132 (62.8%) | Phys: 0.51 | Val: 0.9848 (75.43%) | Best: 75.78%\n",
            "Epoch 310 | T: 23s | Train: 1.7056 (62.7%) | Phys: 0.51 | Val: 0.9994 (74.91%) | Best: 75.78%\n",
            "Epoch 311 | T: 23s | Train: 1.7244 (62.3%) | Phys: 0.51 | Val: 0.9634 (75.66%) | Best: 75.78%\n",
            "Epoch 312 | T: 23s | Train: 1.7807 (60.6%) | Phys: 0.51 | Val: 0.9365 (76.11%) | Best: 76.11%\n",
            "Epoch 313 | T: 23s | Train: 1.7030 (62.5%) | Phys: 0.51 | Val: 0.9430 (76.02%) | Best: 76.11%\n",
            "Epoch 314 | T: 23s | Train: 1.6446 (64.1%) | Phys: 0.51 | Val: 0.9754 (75.38%) | Best: 76.11%\n",
            "Epoch 315 | T: 24s | Train: 1.6885 (63.4%) | Phys: 0.51 | Val: 0.9087 (76.75%) | Best: 76.75%\n",
            "Epoch 316 | T: 23s | Train: 1.7400 (61.7%) | Phys: 0.51 | Val: 0.9432 (76.58%) | Best: 76.75%\n",
            "Epoch 317 | T: 23s | Train: 1.6272 (64.7%) | Phys: 0.51 | Val: 0.9773 (75.69%) | Best: 76.75%\n",
            "Epoch 318 | T: 23s | Train: 1.7147 (62.4%) | Phys: 0.51 | Val: 0.9811 (75.32%) | Best: 76.75%\n",
            "Epoch 319 | T: 23s | Train: 1.6656 (63.7%) | Phys: 0.51 | Val: 0.9187 (76.51%) | Best: 76.75%\n",
            "Epoch 320 | T: 23s | Train: 1.6963 (63.1%) | Phys: 0.51 | Val: 0.9666 (75.21%) | Best: 76.75%\n",
            "Epoch 321 | T: 23s | Train: 1.5927 (66.0%) | Phys: 0.51 | Val: 0.8971 (77.25%) | Best: 77.25%\n",
            "Epoch 322 | T: 23s | Train: 1.6249 (64.3%) | Phys: 0.51 | Val: 0.9434 (76.51%) | Best: 77.25%\n",
            "Epoch 323 | T: 23s | Train: 1.6475 (64.0%) | Phys: 0.51 | Val: 0.8933 (77.63%) | Best: 77.63%\n",
            "Epoch 324 | T: 23s | Train: 1.6502 (63.3%) | Phys: 0.51 | Val: 0.9339 (76.87%) | Best: 77.63%\n",
            "Epoch 325 | T: 23s | Train: 1.6256 (64.7%) | Phys: 0.51 | Val: 0.9340 (76.90%) | Best: 77.63%\n",
            "Epoch 326 | T: 23s | Train: 1.5549 (66.3%) | Phys: 0.51 | Val: 0.9125 (76.78%) | Best: 77.63%\n",
            "Epoch 327 | T: 23s | Train: 1.6278 (64.6%) | Phys: 0.51 | Val: 0.9460 (77.11%) | Best: 77.63%\n",
            "Epoch 328 | T: 23s | Train: 1.6123 (64.8%) | Phys: 0.51 | Val: 0.8827 (77.80%) | Best: 77.80%\n",
            "Epoch 329 | T: 23s | Train: 1.5439 (66.9%) | Phys: 0.51 | Val: 0.9124 (77.15%) | Best: 77.80%\n",
            "Epoch 330 | T: 23s | Train: 1.5715 (65.7%) | Phys: 0.51 | Val: 0.9361 (77.20%) | Best: 77.80%\n",
            "Epoch 331 | T: 23s | Train: 1.6539 (63.0%) | Phys: 0.51 | Val: 0.9273 (77.10%) | Best: 77.80%\n",
            "Epoch 332 | T: 23s | Train: 1.6175 (64.5%) | Phys: 0.51 | Val: 0.9071 (77.26%) | Best: 77.80%\n",
            "Epoch 333 | T: 23s | Train: 1.5611 (66.2%) | Phys: 0.51 | Val: 0.9086 (78.45%) | Best: 78.45%\n",
            "Epoch 334 | T: 23s | Train: 1.6003 (64.6%) | Phys: 0.51 | Val: 0.8756 (77.90%) | Best: 78.45%\n",
            "Epoch 335 | T: 23s | Train: 1.5773 (66.3%) | Phys: 0.51 | Val: 0.9297 (77.93%) | Best: 78.45%\n",
            "Epoch 336 | T: 23s | Train: 1.5186 (67.7%) | Phys: 0.51 | Val: 0.8952 (77.30%) | Best: 78.45%\n",
            "Epoch 337 | T: 23s | Train: 1.5999 (64.8%) | Phys: 0.51 | Val: 0.8941 (78.26%) | Best: 78.45%\n",
            "Epoch 338 | T: 23s | Train: 1.6098 (64.5%) | Phys: 0.51 | Val: 0.8675 (78.23%) | Best: 78.45%\n",
            "Epoch 339 | T: 23s | Train: 1.5902 (65.0%) | Phys: 0.51 | Val: 0.9180 (78.22%) | Best: 78.45%\n",
            "Epoch 340 | T: 23s | Train: 1.5009 (67.5%) | Phys: 0.51 | Val: 0.8650 (78.42%) | Best: 78.45%\n",
            "Epoch 341 | T: 23s | Train: 1.5396 (66.4%) | Phys: 0.51 | Val: 0.9199 (78.48%) | Best: 78.48%\n",
            "Epoch 342 | T: 23s | Train: 1.5496 (65.9%) | Phys: 0.51 | Val: 0.8506 (78.58%) | Best: 78.58%\n",
            "Epoch 343 | T: 23s | Train: 1.4698 (67.9%) | Phys: 0.51 | Val: 0.8693 (78.68%) | Best: 78.68%\n",
            "Epoch 344 | T: 23s | Train: 1.5281 (66.2%) | Phys: 0.51 | Val: 0.8128 (79.32%) | Best: 79.32%\n",
            "Epoch 345 | T: 23s | Train: 1.5930 (64.4%) | Phys: 0.51 | Val: 0.9142 (77.54%) | Best: 79.32%\n",
            "Epoch 346 | T: 23s | Train: 1.4575 (68.3%) | Phys: 0.51 | Val: 0.8887 (78.41%) | Best: 79.32%\n",
            "Epoch 347 | T: 23s | Train: 1.5620 (65.6%) | Phys: 0.51 | Val: 0.8252 (78.92%) | Best: 79.32%\n",
            "Epoch 348 | T: 23s | Train: 1.5365 (65.5%) | Phys: 0.51 | Val: 0.9300 (78.16%) | Best: 79.32%\n",
            "Epoch 349 | T: 23s | Train: 1.4728 (67.2%) | Phys: 0.51 | Val: 0.8767 (78.67%) | Best: 79.32%\n",
            "Epoch 350 | T: 23s | Train: 1.5267 (65.9%) | Phys: 0.51 | Val: 0.8779 (78.81%) | Best: 79.32%\n",
            "Epoch 351 | T: 23s | Train: 1.4732 (67.6%) | Phys: 0.51 | Val: 0.8095 (79.38%) | Best: 79.38%\n",
            "Epoch 352 | T: 23s | Train: 1.4677 (67.7%) | Phys: 0.51 | Val: 0.8572 (79.31%) | Best: 79.38%\n",
            "Epoch 353 | T: 24s | Train: 1.4161 (69.0%) | Phys: 0.51 | Val: 0.8295 (79.64%) | Best: 79.64%\n",
            "Epoch 354 | T: 23s | Train: 1.4390 (68.2%) | Phys: 0.51 | Val: 0.8719 (79.08%) | Best: 79.64%\n",
            "Epoch 355 | T: 23s | Train: 1.4621 (67.5%) | Phys: 0.51 | Val: 0.8331 (79.02%) | Best: 79.64%\n",
            "Epoch 356 | T: 23s | Train: 1.4544 (67.7%) | Phys: 0.51 | Val: 0.8283 (79.53%) | Best: 79.64%\n",
            "Epoch 357 | T: 23s | Train: 1.4579 (67.9%) | Phys: 0.51 | Val: 0.8268 (79.48%) | Best: 79.64%\n",
            "Epoch 358 | T: 23s | Train: 1.5376 (65.4%) | Phys: 0.51 | Val: 0.8718 (79.23%) | Best: 79.64%\n",
            "Epoch 359 | T: 23s | Train: 1.4551 (67.5%) | Phys: 0.51 | Val: 0.8449 (79.39%) | Best: 79.64%\n",
            "Epoch 360 | T: 23s | Train: 1.3888 (69.3%) | Phys: 0.51 | Val: 0.8320 (79.47%) | Best: 79.64%\n",
            "Epoch 361 | T: 23s | Train: 1.3928 (69.1%) | Phys: 0.51 | Val: 0.8567 (79.27%) | Best: 79.64%\n",
            "Epoch 362 | T: 23s | Train: 1.4061 (68.6%) | Phys: 0.51 | Val: 0.8129 (80.19%) | Best: 80.19%\n",
            "Epoch 363 | T: 23s | Train: 1.4260 (68.0%) | Phys: 0.51 | Val: 0.8786 (79.24%) | Best: 80.19%\n",
            "Epoch 364 | T: 23s | Train: 1.4692 (66.4%) | Phys: 0.51 | Val: 0.8253 (79.90%) | Best: 80.19%\n",
            "Epoch 365 | T: 23s | Train: 1.4031 (68.5%) | Phys: 0.51 | Val: 0.7793 (80.34%) | Best: 80.34%\n",
            "Epoch 366 | T: 23s | Train: 1.4568 (67.3%) | Phys: 0.51 | Val: 0.8476 (79.91%) | Best: 80.34%\n",
            "Epoch 367 | T: 23s | Train: 1.4065 (69.2%) | Phys: 0.51 | Val: 0.8020 (80.26%) | Best: 80.34%\n",
            "Epoch 368 | T: 23s | Train: 1.3738 (69.1%) | Phys: 0.51 | Val: 0.7929 (80.76%) | Best: 80.76%\n",
            "Epoch 369 | T: 23s | Train: 1.3097 (71.3%) | Phys: 0.51 | Val: 0.8423 (79.82%) | Best: 80.76%\n",
            "Epoch 370 | T: 23s | Train: 1.3893 (68.7%) | Phys: 0.51 | Val: 0.8270 (80.25%) | Best: 80.76%\n",
            "Epoch 371 | T: 23s | Train: 1.3745 (69.3%) | Phys: 0.51 | Val: 0.8353 (79.97%) | Best: 80.76%\n",
            "Epoch 372 | T: 23s | Train: 1.3834 (69.3%) | Phys: 0.51 | Val: 0.8065 (80.43%) | Best: 80.76%\n",
            "Epoch 373 | T: 24s | Train: 1.4078 (68.1%) | Phys: 0.51 | Val: 0.8508 (80.59%) | Best: 80.76%\n",
            "Epoch 374 | T: 23s | Train: 1.3748 (68.5%) | Phys: 0.51 | Val: 0.8222 (80.44%) | Best: 80.76%\n",
            "Epoch 375 | T: 23s | Train: 1.3739 (68.8%) | Phys: 0.51 | Val: 0.7624 (80.78%) | Best: 80.78%\n",
            "Epoch 376 | T: 23s | Train: 1.3642 (69.0%) | Phys: 0.51 | Val: 0.8211 (80.58%) | Best: 80.78%\n",
            "Epoch 377 | T: 23s | Train: 1.3333 (70.0%) | Phys: 0.51 | Val: 0.7857 (81.11%) | Best: 81.11%\n",
            "Epoch 378 | T: 23s | Train: 1.3736 (68.8%) | Phys: 0.51 | Val: 0.7890 (80.77%) | Best: 81.11%\n",
            "Epoch 379 | T: 23s | Train: 1.3461 (69.5%) | Phys: 0.51 | Val: 0.8148 (80.64%) | Best: 81.11%\n",
            "Epoch 380 | T: 23s | Train: 1.3691 (68.7%) | Phys: 0.51 | Val: 0.8017 (80.68%) | Best: 81.11%\n",
            "Epoch 381 | T: 23s | Train: 1.3946 (68.2%) | Phys: 0.51 | Val: 0.8003 (80.73%) | Best: 81.11%\n",
            "Epoch 382 | T: 23s | Train: 1.3478 (69.0%) | Phys: 0.51 | Val: 0.8220 (80.42%) | Best: 81.11%\n",
            "Epoch 383 | T: 24s | Train: 1.3560 (69.2%) | Phys: 0.51 | Val: 0.7886 (80.80%) | Best: 81.11%\n",
            "Epoch 384 | T: 23s | Train: 1.3291 (70.0%) | Phys: 0.51 | Val: 0.8109 (80.65%) | Best: 81.11%\n",
            "Epoch 385 | T: 23s | Train: 1.3542 (68.4%) | Phys: 0.51 | Val: 0.7977 (80.80%) | Best: 81.11%\n",
            "Epoch 386 | T: 23s | Train: 1.3330 (69.3%) | Phys: 0.51 | Val: 0.7911 (80.87%) | Best: 81.11%\n",
            "Epoch 387 | T: 23s | Train: 1.3601 (68.2%) | Phys: 0.51 | Val: 0.7758 (80.85%) | Best: 81.11%\n",
            "Epoch 388 | T: 23s | Train: 1.3497 (69.1%) | Phys: 0.51 | Val: 0.7810 (81.02%) | Best: 81.11%\n",
            "Epoch 389 | T: 24s | Train: 1.2807 (71.4%) | Phys: 0.51 | Val: 0.7788 (80.80%) | Best: 81.11%\n",
            "Epoch 390 | T: 23s | Train: 1.3048 (70.6%) | Phys: 0.51 | Val: 0.8292 (80.66%) | Best: 81.11%\n",
            "Epoch 391 | T: 23s | Train: 1.3209 (69.6%) | Phys: 0.51 | Val: 0.7792 (80.94%) | Best: 81.11%\n",
            "Epoch 392 | T: 23s | Train: 1.3201 (70.3%) | Phys: 0.51 | Val: 0.8545 (80.50%) | Best: 81.11%\n",
            "Epoch 393 | T: 23s | Train: 1.3422 (68.6%) | Phys: 0.51 | Val: 0.7574 (81.07%) | Best: 81.11%\n",
            "Epoch 394 | T: 23s | Train: 1.3198 (69.8%) | Phys: 0.51 | Val: 0.8322 (80.68%) | Best: 81.11%\n",
            "Epoch 395 | T: 23s | Train: 1.2971 (70.2%) | Phys: 0.51 | Val: 0.7926 (81.00%) | Best: 81.11%\n",
            "Epoch 396 | T: 23s | Train: 1.3071 (70.4%) | Phys: 0.51 | Val: 0.7613 (81.05%) | Best: 81.11%\n",
            "Epoch 397 | T: 23s | Train: 1.4025 (67.1%) | Phys: 0.51 | Val: 0.8139 (80.93%) | Best: 81.11%\n",
            "Epoch 398 | T: 23s | Train: 1.3010 (70.3%) | Phys: 0.51 | Val: 0.8096 (80.83%) | Best: 81.11%\n",
            "Epoch 399 | T: 23s | Train: 1.3274 (70.1%) | Phys: 0.51 | Val: 0.7830 (80.98%) | Best: 81.11%\n",
            "Epoch 400 | T: 23s | Train: 1.2767 (71.3%) | Phys: 0.51 | Val: 0.7792 (81.18%) | Best: 81.18%\n",
            "Final Best: 81.18%\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": [],
      "name": "CIFAR-100 SIGReg Strong 1600 EP",
      "gpuType": "L4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}