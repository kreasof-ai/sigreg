{
  "cells": [
    {
      "cell_type": "code",
      "id": "pBxX3I1kYwojaamntSxiNZTK",
      "metadata": {
        "tags": [],
        "id": "pBxX3I1kYwojaamntSxiNZTK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "537de807-b0c0-4ebb-cb26-8a1724765d05"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "import math\n",
        "import time\n",
        "\n",
        "REG_MODE = 'weak'\n",
        "SIGR_ALPHA = 0.1   # Strength of the physics constraint\n",
        "SKETCH_DIM = 64    # Dimension of the random observer\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "LEARNING_RATE = 0.1\n",
        "EPOCHS = 400\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "if torch.backends.mps.is_available(): DEVICE = 'mps'\n",
        "\n",
        "print(f\"Training on device: {DEVICE}\")\n",
        "\n",
        "def get_data_loaders():\n",
        "    print('==> Preparing data with Strong Augmentation...')\n",
        "\n",
        "    mean = (0.5071, 0.4867, 0.4408)\n",
        "    std = (0.2675, 0.2565, 0.2761)\n",
        "\n",
        "    # FIX 1: Add RandAugment\n",
        "    transform_train = transforms.Compose([\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean, std),\n",
        "    ])\n",
        "\n",
        "    transform_test = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean, std),\n",
        "    ])\n",
        "\n",
        "    trainset = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=transform_train)\n",
        "    # Increase workers to handle augmentation load\n",
        "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
        "\n",
        "    testset = torchvision.datasets.CIFAR100(root='./data', train=False, download=True, transform=transform_test)\n",
        "    testloader = torch.utils.data.DataLoader(testset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
        "\n",
        "    return trainloader, testloader\n",
        "\n",
        "\n",
        "# ------------------------------------------\n",
        "# Physics Engine: The Regularizers\n",
        "# ------------------------------------------\n",
        "\n",
        "def sigreg_weak_loss(x, sketch_dim=64):\n",
        "    \"\"\"\n",
        "    Forces Covariance(x) ~ Identity.\n",
        "    Matches the 2nd Moment (Spherical Cloud).\n",
        "    \"\"\"\n",
        "    N, C = x.size()\n",
        "    # 1. Sketching (Optional for C=512, but good for consistency)\n",
        "    if C > sketch_dim:\n",
        "        S = torch.randn(sketch_dim, C, device=x.device) / (C ** 0.5)\n",
        "        x = x @ S.T  # [N, sketch_dim]\n",
        "    else:\n",
        "        sketch_dim = C\n",
        "\n",
        "    # 2. Centering & Covariance\n",
        "    x = x - x.mean(dim=0, keepdim=True)\n",
        "    cov = (x.T @ x) / (N - 1 + 1e-6)\n",
        "\n",
        "    # 3. Target Identity\n",
        "    target = torch.eye(sketch_dim, device=x.device)\n",
        "\n",
        "    # 4. Off-diagonal suppression + Diagonal maintenance\n",
        "    return torch.norm(cov - target, p='fro')\n",
        "\n",
        "def sigreg_strong_loss(x, sketch_dim=64):\n",
        "    \"\"\"\n",
        "    Forces ECF(x) ~ ECF(Gaussian).\n",
        "    Matches ALL Moments (Maximum Entropy Cloud).\n",
        "    Exact implementation of LeJEPA Algorithm 1.\n",
        "    \"\"\"\n",
        "    N, C = x.size()\n",
        "\n",
        "    # 1. Projection (The Observer)\n",
        "    # Project channels down to sketch_dim\n",
        "    A = torch.randn(C, sketch_dim, device=x.device)\n",
        "    A = A / (A.norm(p=2, dim=0, keepdim=True) + 1e-6)\n",
        "\n",
        "    # 2. Integration Points\n",
        "    t = torch.linspace(-5, 5, 17, device=x.device)\n",
        "\n",
        "    # 3. Theoretical Gaussian CF\n",
        "    exp_f = torch.exp(-0.5 * t**2)\n",
        "\n",
        "    # 4. Empirical CF\n",
        "    # proj: [N, sketch_dim]\n",
        "    proj = x @ A\n",
        "\n",
        "    # args: [N, sketch_dim, T]\n",
        "    args = proj.unsqueeze(2) * t.view(1, 1, -1)\n",
        "\n",
        "    # ecf: [sketch_dim, T] (Mean over batch)\n",
        "    ecf = torch.exp(1j * args).mean(dim=0)\n",
        "\n",
        "    # 5. Weighted L2 Distance\n",
        "    # |ecf - gauss|^2 * gauss_weight\n",
        "    diff_sq = (ecf - exp_f.unsqueeze(0)).abs().square()\n",
        "    err = diff_sq * exp_f.unsqueeze(0)\n",
        "\n",
        "    # 6. Integrate\n",
        "    loss = torch.trapz(err, t, dim=1) * N\n",
        "\n",
        "    return loss.mean()\n",
        "\n",
        "class LinearBlock(nn.Module):\n",
        "    def __init__(self, dim, hidden_dim, reg_mode='baseline', sketch_dim=64):\n",
        "        super().__init__()\n",
        "        self.fc = nn.Linear(dim, hidden_dim)\n",
        "        # Note: NO BATCH NORM. We rely purely on SIGReg.\n",
        "        self.reg_mode = reg_mode\n",
        "        self.sketch_dim = sketch_dim\n",
        "\n",
        "    def forward(self, x):\n",
        "        pre_act = self.fc(x)\n",
        "\n",
        "        reg_loss = torch.tensor(0.0, device=x.device)\n",
        "        if self.reg_mode != 'baseline':\n",
        "            if self.reg_mode == 'weak':\n",
        "                reg_loss = sigreg_weak_loss(pre_act, self.sketch_dim)\n",
        "            elif self.reg_mode == 'strong':\n",
        "                reg_loss = sigreg_strong_loss(pre_act, self.sketch_dim)\n",
        "\n",
        "        out = F.relu(pre_act)\n",
        "\n",
        "        return out, reg_loss\n",
        "\n",
        "class ThermoMLP(nn.Module):\n",
        "    def __init__(self, input_dim=3072, hidden_dim=1024, num_classes=100, depth=6, reg_mode='weak', sketch_dim=64):\n",
        "        super().__init__()\n",
        "\n",
        "        layers = []\n",
        "        # Input Layer\n",
        "        layers.append(LinearBlock(input_dim, hidden_dim, reg_mode))\n",
        "\n",
        "        # Deep Layers (No Residuals!)\n",
        "        for _ in range(depth - 2):\n",
        "            layers.append(LinearBlock(hidden_dim, hidden_dim, reg_mode, sketch_dim))\n",
        "\n",
        "        self.layers = nn.ModuleList(layers)\n",
        "        self.classifier = nn.Linear(hidden_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Flatten: [B, C, H, W] -> [B, 3072]\n",
        "        x = x.flatten(1)\n",
        "\n",
        "        total_phys_loss = 0.0\n",
        "\n",
        "        for layer in self.layers:\n",
        "            x, l_loss = layer(x)\n",
        "            total_phys_loss += l_loss\n",
        "\n",
        "        out = self.classifier(x)\n",
        "\n",
        "        # Normalize loss scale\n",
        "        return out, (total_phys_loss / len(self.layers))\n",
        "\n",
        "# ==========================================\n",
        "# 5. Training Engine (Updated for Mixup/CutMix)\n",
        "# ==========================================\n",
        "def train(epoch, net, trainloader, optimizer, criterion):\n",
        "    net.train()\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    phys_loss_meter = 0\n",
        "\n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward\n",
        "        outputs, p_loss = net(inputs)\n",
        "\n",
        "        # Task Loss\n",
        "        c_loss = criterion(outputs, targets)\n",
        "\n",
        "        # Total Loss\n",
        "        loss = (1 - SIGR_ALPHA) * c_loss + (SIGR_ALPHA * p_loss)\n",
        "\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(net.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += ((1 - SIGR_ALPHA) * c_loss).item()\n",
        "        phys_loss_meter += (SIGR_ALPHA * p_loss).item()\n",
        "\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "    acc = 100. * correct / total\n",
        "    return train_loss / (batch_idx + 1), acc, phys_loss_meter / (batch_idx + 1)\n",
        "\n",
        "def test(epoch, net, testloader, criterion):\n",
        "    net.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "            inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
        "            outputs, _ = net(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "    acc = 100. * correct / total\n",
        "    return test_loss / (batch_idx + 1), acc\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    trainloader, testloader = get_data_loaders()\n",
        "\n",
        "    net = ThermoMLP(reg_mode=REG_MODE, sketch_dim=SKETCH_DIM).to(DEVICE)\n",
        "    net = net.to(DEVICE)\n",
        "\n",
        "    # Standard CrossEntropy for final eval, SoftLabel for training is handled by Mixup logic\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    optimizer = optim.SGD(net.parameters(), lr=LEARNING_RATE, momentum=0.0, weight_decay=0.0)\n",
        "\n",
        "    print(f\"Starting training for {EPOCHS} epochs with RandAugment + Mixup/CutMix...\")\n",
        "\n",
        "    best_acc = 0\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        start_time = time.time()\n",
        "\n",
        "        train_loss, train_acc, physics_loss = train(epoch, net, trainloader, optimizer, criterion)\n",
        "        test_loss, test_acc = test(epoch, net, testloader, criterion)\n",
        "\n",
        "        if test_acc > best_acc:\n",
        "            best_acc = test_acc\n",
        "            # torch.save(net.state_dict(), f'thermo_resnet_{REG_MODE}.pth')\n",
        "\n",
        "        epoch_time = time.time() - start_time\n",
        "\n",
        "        print(f\"Epoch {epoch+1} | T: {epoch_time:.0f}s | \"\n",
        "              f\"Train: {train_loss:.4f} ({train_acc:.1f}%) | \"\n",
        "              f\"Phys: {physics_loss:.2f} | \"\n",
        "              f\"Val: {test_loss:.4f} ({test_acc:.2f}%) | \"\n",
        "              f\"Best: {best_acc:.2f}%\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on device: cuda\n",
            "==> Preparing data with Strong Augmentation...\n",
            "Starting training for 400 epochs with RandAugment + Mixup/CutMix...\n",
            "Epoch 1 | T: 11s | Train: 4.1171 (1.9%) | Phys: 0.77 | Val: 4.4405 (2.89%) | Best: 2.89%\n",
            "Epoch 2 | T: 12s | Train: 3.7617 (5.6%) | Phys: 0.76 | Val: 4.1134 (6.60%) | Best: 6.60%\n",
            "Epoch 3 | T: 10s | Train: 3.5795 (8.4%) | Phys: 0.75 | Val: 3.9829 (8.06%) | Best: 8.06%\n",
            "Epoch 4 | T: 10s | Train: 3.4900 (10.2%) | Phys: 0.75 | Val: 3.8813 (10.52%) | Best: 10.52%\n",
            "Epoch 5 | T: 10s | Train: 3.4141 (11.6%) | Phys: 0.75 | Val: 3.8105 (11.47%) | Best: 11.47%\n",
            "Epoch 6 | T: 10s | Train: 3.3546 (12.9%) | Phys: 0.75 | Val: 3.7651 (11.89%) | Best: 11.89%\n",
            "Epoch 7 | T: 10s | Train: 3.3030 (13.7%) | Phys: 0.75 | Val: 3.7044 (13.27%) | Best: 13.27%\n",
            "Epoch 8 | T: 10s | Train: 3.2630 (14.6%) | Phys: 0.75 | Val: 3.6844 (13.37%) | Best: 13.37%\n",
            "Epoch 9 | T: 10s | Train: 3.2224 (15.1%) | Phys: 0.75 | Val: 3.6331 (14.29%) | Best: 14.29%\n",
            "Epoch 10 | T: 10s | Train: 3.1851 (16.0%) | Phys: 0.75 | Val: 3.6036 (14.82%) | Best: 14.82%\n",
            "Epoch 11 | T: 10s | Train: 3.1491 (16.6%) | Phys: 0.75 | Val: 3.5477 (15.97%) | Best: 15.97%\n",
            "Epoch 12 | T: 10s | Train: 3.1146 (17.2%) | Phys: 0.75 | Val: 3.4940 (16.74%) | Best: 16.74%\n",
            "Epoch 13 | T: 10s | Train: 3.0843 (18.0%) | Phys: 0.75 | Val: 3.5042 (17.04%) | Best: 17.04%\n",
            "Epoch 14 | T: 10s | Train: 3.0597 (18.4%) | Phys: 0.74 | Val: 3.4961 (16.94%) | Best: 17.04%\n",
            "Epoch 15 | T: 10s | Train: 3.0286 (19.0%) | Phys: 0.74 | Val: 3.4409 (18.10%) | Best: 18.10%\n",
            "Epoch 16 | T: 10s | Train: 3.0084 (19.4%) | Phys: 0.74 | Val: 3.4416 (17.80%) | Best: 18.10%\n",
            "Epoch 17 | T: 10s | Train: 2.9875 (19.9%) | Phys: 0.74 | Val: 3.4222 (18.62%) | Best: 18.62%\n",
            "Epoch 18 | T: 10s | Train: 2.9651 (20.2%) | Phys: 0.74 | Val: 3.3894 (19.24%) | Best: 19.24%\n",
            "Epoch 19 | T: 10s | Train: 2.9445 (20.7%) | Phys: 0.74 | Val: 3.3565 (19.64%) | Best: 19.64%\n",
            "Epoch 20 | T: 10s | Train: 2.9307 (20.9%) | Phys: 0.74 | Val: 3.3648 (19.64%) | Best: 19.64%\n",
            "Epoch 21 | T: 10s | Train: 2.9149 (21.5%) | Phys: 0.74 | Val: 3.3387 (20.35%) | Best: 20.35%\n",
            "Epoch 22 | T: 10s | Train: 2.8971 (21.9%) | Phys: 0.74 | Val: 3.3516 (19.53%) | Best: 20.35%\n",
            "Epoch 23 | T: 10s | Train: 2.8848 (22.1%) | Phys: 0.74 | Val: 3.3035 (20.44%) | Best: 20.44%\n",
            "Epoch 24 | T: 10s | Train: 2.8717 (21.8%) | Phys: 0.74 | Val: 3.2826 (20.85%) | Best: 20.85%\n",
            "Epoch 25 | T: 10s | Train: 2.8522 (22.7%) | Phys: 0.74 | Val: 3.3152 (20.10%) | Best: 20.85%\n",
            "Epoch 26 | T: 10s | Train: 2.8414 (22.9%) | Phys: 0.74 | Val: 3.2723 (20.88%) | Best: 20.88%\n",
            "Epoch 27 | T: 10s | Train: 2.8250 (23.3%) | Phys: 0.74 | Val: 3.2441 (21.68%) | Best: 21.68%\n",
            "Epoch 28 | T: 10s | Train: 2.8155 (23.3%) | Phys: 0.74 | Val: 3.2311 (22.02%) | Best: 22.02%\n",
            "Epoch 29 | T: 10s | Train: 2.7992 (23.7%) | Phys: 0.74 | Val: 3.2246 (22.07%) | Best: 22.07%\n",
            "Epoch 30 | T: 10s | Train: 2.7867 (24.1%) | Phys: 0.74 | Val: 3.1657 (23.25%) | Best: 23.25%\n",
            "Epoch 31 | T: 10s | Train: 2.7739 (24.4%) | Phys: 0.74 | Val: 3.1838 (22.63%) | Best: 23.25%\n",
            "Epoch 32 | T: 10s | Train: 2.7631 (24.7%) | Phys: 0.74 | Val: 3.1593 (23.32%) | Best: 23.32%\n",
            "Epoch 33 | T: 10s | Train: 2.7498 (24.7%) | Phys: 0.74 | Val: 3.1752 (23.27%) | Best: 23.32%\n",
            "Epoch 34 | T: 10s | Train: 2.7386 (24.9%) | Phys: 0.74 | Val: 3.1794 (22.18%) | Best: 23.32%\n",
            "Epoch 35 | T: 10s | Train: 2.7301 (25.3%) | Phys: 0.74 | Val: 3.1268 (23.69%) | Best: 23.69%\n",
            "Epoch 36 | T: 10s | Train: 2.7161 (25.6%) | Phys: 0.74 | Val: 3.0861 (24.65%) | Best: 24.65%\n",
            "Epoch 37 | T: 10s | Train: 2.7019 (25.6%) | Phys: 0.74 | Val: 3.0719 (25.10%) | Best: 25.10%\n",
            "Epoch 38 | T: 10s | Train: 2.6913 (25.9%) | Phys: 0.74 | Val: 3.1145 (24.69%) | Best: 25.10%\n",
            "Epoch 39 | T: 10s | Train: 2.6799 (26.3%) | Phys: 0.74 | Val: 3.0556 (25.34%) | Best: 25.34%\n",
            "Epoch 40 | T: 10s | Train: 2.6745 (26.3%) | Phys: 0.73 | Val: 3.0520 (25.50%) | Best: 25.50%\n",
            "Epoch 41 | T: 10s | Train: 2.6668 (26.5%) | Phys: 0.73 | Val: 3.0506 (25.42%) | Best: 25.50%\n",
            "Epoch 42 | T: 10s | Train: 2.6476 (26.9%) | Phys: 0.73 | Val: 3.0254 (25.73%) | Best: 25.73%\n",
            "Epoch 43 | T: 10s | Train: 2.6394 (27.2%) | Phys: 0.73 | Val: 3.0337 (25.75%) | Best: 25.75%\n",
            "Epoch 44 | T: 10s | Train: 2.6316 (27.4%) | Phys: 0.73 | Val: 3.0279 (25.63%) | Best: 25.75%\n",
            "Epoch 45 | T: 10s | Train: 2.6194 (27.7%) | Phys: 0.73 | Val: 3.0310 (25.53%) | Best: 25.75%\n",
            "Epoch 46 | T: 10s | Train: 2.6100 (27.7%) | Phys: 0.73 | Val: 2.9849 (26.90%) | Best: 26.90%\n",
            "Epoch 47 | T: 10s | Train: 2.6018 (27.9%) | Phys: 0.73 | Val: 2.9912 (26.59%) | Best: 26.90%\n",
            "Epoch 48 | T: 10s | Train: 2.5925 (28.2%) | Phys: 0.73 | Val: 2.9711 (26.85%) | Best: 26.90%\n",
            "Epoch 49 | T: 10s | Train: 2.5789 (28.5%) | Phys: 0.73 | Val: 2.9886 (26.37%) | Best: 26.90%\n",
            "Epoch 50 | T: 10s | Train: 2.5785 (28.4%) | Phys: 0.73 | Val: 2.9354 (27.40%) | Best: 27.40%\n",
            "Epoch 51 | T: 10s | Train: 2.5629 (28.8%) | Phys: 0.73 | Val: 2.9727 (26.21%) | Best: 27.40%\n",
            "Epoch 52 | T: 10s | Train: 2.5556 (28.8%) | Phys: 0.73 | Val: 2.9775 (26.48%) | Best: 27.40%\n",
            "Epoch 53 | T: 10s | Train: 2.5448 (29.1%) | Phys: 0.73 | Val: 2.9528 (26.65%) | Best: 27.40%\n",
            "Epoch 54 | T: 10s | Train: 2.5355 (29.3%) | Phys: 0.73 | Val: 2.9331 (27.48%) | Best: 27.48%\n",
            "Epoch 55 | T: 10s | Train: 2.5281 (29.5%) | Phys: 0.73 | Val: 2.8902 (28.68%) | Best: 28.68%\n",
            "Epoch 56 | T: 10s | Train: 2.5195 (29.7%) | Phys: 0.73 | Val: 2.8869 (28.80%) | Best: 28.80%\n",
            "Epoch 57 | T: 10s | Train: 2.5133 (29.6%) | Phys: 0.73 | Val: 2.8995 (28.39%) | Best: 28.80%\n",
            "Epoch 58 | T: 10s | Train: 2.4984 (30.3%) | Phys: 0.73 | Val: 2.8871 (28.57%) | Best: 28.80%\n",
            "Epoch 59 | T: 10s | Train: 2.4932 (30.3%) | Phys: 0.73 | Val: 2.9139 (27.98%) | Best: 28.80%\n",
            "Epoch 60 | T: 10s | Train: 2.4887 (30.5%) | Phys: 0.73 | Val: 2.8539 (29.71%) | Best: 29.71%\n",
            "Epoch 61 | T: 10s | Train: 2.4785 (30.8%) | Phys: 0.73 | Val: 2.8738 (28.60%) | Best: 29.71%\n",
            "Epoch 62 | T: 10s | Train: 2.4698 (30.9%) | Phys: 0.73 | Val: 2.8545 (29.02%) | Best: 29.71%\n",
            "Epoch 63 | T: 10s | Train: 2.4599 (31.2%) | Phys: 0.73 | Val: 2.8514 (28.96%) | Best: 29.71%\n",
            "Epoch 64 | T: 10s | Train: 2.4558 (31.1%) | Phys: 0.73 | Val: 2.8394 (29.56%) | Best: 29.71%\n",
            "Epoch 65 | T: 10s | Train: 2.4485 (31.4%) | Phys: 0.73 | Val: 2.8401 (29.01%) | Best: 29.71%\n",
            "Epoch 66 | T: 10s | Train: 2.4395 (31.5%) | Phys: 0.73 | Val: 2.8264 (29.84%) | Best: 29.84%\n",
            "Epoch 67 | T: 10s | Train: 2.4317 (31.8%) | Phys: 0.73 | Val: 2.8165 (30.06%) | Best: 30.06%\n",
            "Epoch 68 | T: 10s | Train: 2.4269 (32.0%) | Phys: 0.73 | Val: 2.8305 (29.59%) | Best: 30.06%\n",
            "Epoch 69 | T: 10s | Train: 2.4155 (31.9%) | Phys: 0.73 | Val: 2.7849 (30.77%) | Best: 30.77%\n",
            "Epoch 70 | T: 10s | Train: 2.4068 (32.4%) | Phys: 0.73 | Val: 2.7813 (30.71%) | Best: 30.77%\n",
            "Epoch 71 | T: 10s | Train: 2.4031 (32.4%) | Phys: 0.73 | Val: 2.7938 (30.52%) | Best: 30.77%\n",
            "Epoch 72 | T: 10s | Train: 2.3956 (32.7%) | Phys: 0.73 | Val: 2.7811 (30.03%) | Best: 30.77%\n",
            "Epoch 73 | T: 10s | Train: 2.3879 (32.5%) | Phys: 0.73 | Val: 2.7746 (30.82%) | Best: 30.82%\n",
            "Epoch 74 | T: 10s | Train: 2.3759 (33.0%) | Phys: 0.73 | Val: 2.7717 (30.61%) | Best: 30.82%\n",
            "Epoch 75 | T: 10s | Train: 2.3703 (32.7%) | Phys: 0.73 | Val: 2.7625 (30.76%) | Best: 30.82%\n",
            "Epoch 76 | T: 10s | Train: 2.3628 (33.3%) | Phys: 0.73 | Val: 2.7707 (30.87%) | Best: 30.87%\n",
            "Epoch 77 | T: 10s | Train: 2.3593 (33.2%) | Phys: 0.73 | Val: 2.7940 (30.42%) | Best: 30.87%\n",
            "Epoch 78 | T: 10s | Train: 2.3501 (33.6%) | Phys: 0.73 | Val: 2.7590 (31.34%) | Best: 31.34%\n",
            "Epoch 79 | T: 10s | Train: 2.3409 (33.6%) | Phys: 0.73 | Val: 2.7352 (31.59%) | Best: 31.59%\n",
            "Epoch 80 | T: 10s | Train: 2.3373 (34.0%) | Phys: 0.73 | Val: 2.7602 (31.11%) | Best: 31.59%\n",
            "Epoch 81 | T: 10s | Train: 2.3265 (34.0%) | Phys: 0.73 | Val: 2.7521 (31.09%) | Best: 31.59%\n",
            "Epoch 82 | T: 10s | Train: 2.3223 (34.1%) | Phys: 0.73 | Val: 2.7516 (31.23%) | Best: 31.59%\n",
            "Epoch 83 | T: 10s | Train: 2.3166 (34.3%) | Phys: 0.73 | Val: 2.7378 (31.63%) | Best: 31.63%\n",
            "Epoch 84 | T: 10s | Train: 2.3103 (34.5%) | Phys: 0.73 | Val: 2.7285 (31.55%) | Best: 31.63%\n",
            "Epoch 85 | T: 10s | Train: 2.3061 (34.5%) | Phys: 0.73 | Val: 2.7511 (31.24%) | Best: 31.63%\n",
            "Epoch 86 | T: 10s | Train: 2.2988 (34.5%) | Phys: 0.72 | Val: 2.6839 (32.92%) | Best: 32.92%\n",
            "Epoch 87 | T: 10s | Train: 2.2870 (34.9%) | Phys: 0.73 | Val: 2.7304 (31.78%) | Best: 32.92%\n",
            "Epoch 88 | T: 10s | Train: 2.2819 (35.0%) | Phys: 0.73 | Val: 2.6759 (32.86%) | Best: 32.92%\n",
            "Epoch 89 | T: 10s | Train: 2.2741 (35.2%) | Phys: 0.73 | Val: 2.6804 (32.37%) | Best: 32.92%\n",
            "Epoch 90 | T: 10s | Train: 2.2633 (35.4%) | Phys: 0.72 | Val: 2.7009 (32.03%) | Best: 32.92%\n",
            "Epoch 91 | T: 10s | Train: 2.2640 (35.6%) | Phys: 0.72 | Val: 2.6804 (33.04%) | Best: 33.04%\n",
            "Epoch 92 | T: 10s | Train: 2.2555 (35.8%) | Phys: 0.72 | Val: 2.6902 (32.56%) | Best: 33.04%\n",
            "Epoch 93 | T: 10s | Train: 2.2562 (35.9%) | Phys: 0.72 | Val: 2.6997 (32.65%) | Best: 33.04%\n",
            "Epoch 94 | T: 10s | Train: 2.2406 (36.0%) | Phys: 0.73 | Val: 2.6707 (32.73%) | Best: 33.04%\n",
            "Epoch 95 | T: 10s | Train: 2.2392 (36.2%) | Phys: 0.72 | Val: 2.6736 (32.64%) | Best: 33.04%\n",
            "Epoch 96 | T: 10s | Train: 2.2322 (36.3%) | Phys: 0.72 | Val: 2.6778 (32.70%) | Best: 33.04%\n",
            "Epoch 97 | T: 10s | Train: 2.2209 (36.4%) | Phys: 0.72 | Val: 2.6497 (33.74%) | Best: 33.74%\n",
            "Epoch 98 | T: 10s | Train: 2.2194 (36.5%) | Phys: 0.72 | Val: 2.6445 (33.48%) | Best: 33.74%\n",
            "Epoch 99 | T: 10s | Train: 2.2144 (36.7%) | Phys: 0.72 | Val: 2.6258 (33.94%) | Best: 33.94%\n",
            "Epoch 100 | T: 10s | Train: 2.2041 (37.1%) | Phys: 0.72 | Val: 2.6382 (33.96%) | Best: 33.96%\n",
            "Epoch 101 | T: 10s | Train: 2.2020 (36.8%) | Phys: 0.72 | Val: 2.6422 (33.77%) | Best: 33.96%\n",
            "Epoch 102 | T: 10s | Train: 2.1967 (37.0%) | Phys: 0.72 | Val: 2.6136 (34.36%) | Best: 34.36%\n",
            "Epoch 103 | T: 10s | Train: 2.1907 (37.1%) | Phys: 0.72 | Val: 2.6225 (33.84%) | Best: 34.36%\n",
            "Epoch 104 | T: 10s | Train: 2.1846 (37.4%) | Phys: 0.72 | Val: 2.6087 (34.48%) | Best: 34.48%\n",
            "Epoch 105 | T: 10s | Train: 2.1742 (37.3%) | Phys: 0.72 | Val: 2.6265 (34.55%) | Best: 34.55%\n",
            "Epoch 106 | T: 10s | Train: 2.1742 (37.9%) | Phys: 0.72 | Val: 2.6158 (34.03%) | Best: 34.55%\n",
            "Epoch 107 | T: 10s | Train: 2.1638 (37.8%) | Phys: 0.72 | Val: 2.6075 (34.43%) | Best: 34.55%\n",
            "Epoch 108 | T: 10s | Train: 2.1590 (38.0%) | Phys: 0.72 | Val: 2.5949 (34.76%) | Best: 34.76%\n",
            "Epoch 109 | T: 10s | Train: 2.1488 (38.2%) | Phys: 0.72 | Val: 2.6010 (34.71%) | Best: 34.76%\n",
            "Epoch 110 | T: 10s | Train: 2.1479 (38.1%) | Phys: 0.72 | Val: 2.6331 (33.99%) | Best: 34.76%\n",
            "Epoch 111 | T: 10s | Train: 2.1419 (38.5%) | Phys: 0.72 | Val: 2.6203 (34.07%) | Best: 34.76%\n",
            "Epoch 112 | T: 10s | Train: 2.1315 (38.4%) | Phys: 0.72 | Val: 2.6107 (34.85%) | Best: 34.85%\n",
            "Epoch 113 | T: 10s | Train: 2.1314 (38.8%) | Phys: 0.72 | Val: 2.5834 (35.29%) | Best: 35.29%\n",
            "Epoch 114 | T: 10s | Train: 2.1241 (38.6%) | Phys: 0.72 | Val: 2.5814 (35.23%) | Best: 35.29%\n",
            "Epoch 115 | T: 10s | Train: 2.1164 (39.1%) | Phys: 0.72 | Val: 2.6233 (34.06%) | Best: 35.29%\n",
            "Epoch 116 | T: 10s | Train: 2.1154 (38.9%) | Phys: 0.72 | Val: 2.5794 (35.15%) | Best: 35.29%\n",
            "Epoch 117 | T: 10s | Train: 2.1084 (39.3%) | Phys: 0.72 | Val: 2.5995 (34.50%) | Best: 35.29%\n",
            "Epoch 118 | T: 10s | Train: 2.1016 (39.2%) | Phys: 0.72 | Val: 2.5775 (35.00%) | Best: 35.29%\n",
            "Epoch 119 | T: 10s | Train: 2.0972 (39.5%) | Phys: 0.72 | Val: 2.5896 (34.72%) | Best: 35.29%\n",
            "Epoch 120 | T: 10s | Train: 2.0884 (39.8%) | Phys: 0.72 | Val: 2.5476 (35.57%) | Best: 35.57%\n",
            "Epoch 121 | T: 10s | Train: 2.0883 (39.4%) | Phys: 0.72 | Val: 2.5568 (35.51%) | Best: 35.57%\n",
            "Epoch 122 | T: 10s | Train: 2.0833 (39.9%) | Phys: 0.72 | Val: 2.5537 (36.01%) | Best: 36.01%\n",
            "Epoch 123 | T: 10s | Train: 2.0786 (39.9%) | Phys: 0.72 | Val: 2.5612 (34.80%) | Best: 36.01%\n",
            "Epoch 124 | T: 10s | Train: 2.0674 (40.0%) | Phys: 0.72 | Val: 2.5648 (35.72%) | Best: 36.01%\n",
            "Epoch 125 | T: 10s | Train: 2.0658 (40.4%) | Phys: 0.72 | Val: 2.5327 (36.23%) | Best: 36.23%\n",
            "Epoch 126 | T: 10s | Train: 2.0565 (40.3%) | Phys: 0.72 | Val: 2.5616 (35.80%) | Best: 36.23%\n",
            "Epoch 127 | T: 10s | Train: 2.0538 (40.8%) | Phys: 0.72 | Val: 2.5742 (35.26%) | Best: 36.23%\n",
            "Epoch 128 | T: 10s | Train: 2.0493 (40.8%) | Phys: 0.72 | Val: 2.5521 (35.80%) | Best: 36.23%\n",
            "Epoch 129 | T: 10s | Train: 2.0389 (40.7%) | Phys: 0.72 | Val: 2.5502 (35.96%) | Best: 36.23%\n",
            "Epoch 130 | T: 10s | Train: 2.0388 (40.7%) | Phys: 0.72 | Val: 2.5627 (35.26%) | Best: 36.23%\n",
            "Epoch 131 | T: 10s | Train: 2.0366 (40.8%) | Phys: 0.72 | Val: 2.5362 (35.88%) | Best: 36.23%\n",
            "Epoch 132 | T: 10s | Train: 2.0296 (41.0%) | Phys: 0.72 | Val: 2.5173 (36.81%) | Best: 36.81%\n",
            "Epoch 133 | T: 10s | Train: 2.0239 (41.4%) | Phys: 0.72 | Val: 2.5186 (36.81%) | Best: 36.81%\n",
            "Epoch 134 | T: 10s | Train: 2.0165 (41.3%) | Phys: 0.72 | Val: 2.5340 (36.01%) | Best: 36.81%\n",
            "Epoch 135 | T: 10s | Train: 2.0105 (41.4%) | Phys: 0.72 | Val: 2.5300 (36.05%) | Best: 36.81%\n",
            "Epoch 136 | T: 10s | Train: 2.0032 (41.7%) | Phys: 0.72 | Val: 2.4993 (37.02%) | Best: 37.02%\n",
            "Epoch 137 | T: 10s | Train: 2.0001 (41.7%) | Phys: 0.72 | Val: 2.5201 (36.35%) | Best: 37.02%\n",
            "Epoch 138 | T: 10s | Train: 1.9961 (41.8%) | Phys: 0.72 | Val: 2.5184 (36.27%) | Best: 37.02%\n",
            "Epoch 139 | T: 10s | Train: 1.9886 (41.9%) | Phys: 0.72 | Val: 2.5199 (36.50%) | Best: 37.02%\n",
            "Epoch 140 | T: 10s | Train: 1.9853 (42.2%) | Phys: 0.72 | Val: 2.5067 (36.84%) | Best: 37.02%\n",
            "Epoch 141 | T: 10s | Train: 1.9811 (42.2%) | Phys: 0.72 | Val: 2.5064 (36.78%) | Best: 37.02%\n",
            "Epoch 142 | T: 10s | Train: 1.9733 (42.6%) | Phys: 0.72 | Val: 2.5156 (36.63%) | Best: 37.02%\n",
            "Epoch 143 | T: 10s | Train: 1.9686 (42.6%) | Phys: 0.72 | Val: 2.5086 (36.75%) | Best: 37.02%\n",
            "Epoch 144 | T: 10s | Train: 1.9610 (42.6%) | Phys: 0.72 | Val: 2.5243 (36.77%) | Best: 37.02%\n",
            "Epoch 145 | T: 10s | Train: 1.9558 (42.9%) | Phys: 0.72 | Val: 2.4973 (36.91%) | Best: 37.02%\n",
            "Epoch 146 | T: 10s | Train: 1.9525 (43.0%) | Phys: 0.72 | Val: 2.4995 (36.94%) | Best: 37.02%\n",
            "Epoch 147 | T: 10s | Train: 1.9501 (42.8%) | Phys: 0.72 | Val: 2.4760 (38.10%) | Best: 38.10%\n",
            "Epoch 148 | T: 10s | Train: 1.9436 (42.9%) | Phys: 0.72 | Val: 2.4995 (36.83%) | Best: 38.10%\n",
            "Epoch 149 | T: 10s | Train: 1.9391 (43.2%) | Phys: 0.72 | Val: 2.4945 (37.06%) | Best: 38.10%\n",
            "Epoch 150 | T: 10s | Train: 1.9311 (43.4%) | Phys: 0.72 | Val: 2.4835 (37.13%) | Best: 38.10%\n",
            "Epoch 151 | T: 10s | Train: 1.9294 (43.5%) | Phys: 0.72 | Val: 2.5187 (36.57%) | Best: 38.10%\n",
            "Epoch 152 | T: 10s | Train: 1.9258 (43.6%) | Phys: 0.72 | Val: 2.4800 (37.50%) | Best: 38.10%\n",
            "Epoch 153 | T: 10s | Train: 1.9185 (43.8%) | Phys: 0.72 | Val: 2.5065 (36.55%) | Best: 38.10%\n",
            "Epoch 154 | T: 10s | Train: 1.9097 (43.9%) | Phys: 0.72 | Val: 2.4881 (37.10%) | Best: 38.10%\n",
            "Epoch 155 | T: 10s | Train: 1.9094 (44.0%) | Phys: 0.72 | Val: 2.4699 (37.61%) | Best: 38.10%\n",
            "Epoch 156 | T: 10s | Train: 1.9055 (44.0%) | Phys: 0.72 | Val: 2.4901 (36.76%) | Best: 38.10%\n",
            "Epoch 157 | T: 10s | Train: 1.8967 (44.3%) | Phys: 0.72 | Val: 2.4767 (37.97%) | Best: 38.10%\n",
            "Epoch 158 | T: 10s | Train: 1.8879 (44.3%) | Phys: 0.72 | Val: 2.5046 (36.68%) | Best: 38.10%\n",
            "Epoch 159 | T: 10s | Train: 1.8880 (44.8%) | Phys: 0.72 | Val: 2.4656 (37.98%) | Best: 38.10%\n",
            "Epoch 160 | T: 10s | Train: 1.8808 (44.6%) | Phys: 0.72 | Val: 2.4725 (37.48%) | Best: 38.10%\n",
            "Epoch 161 | T: 10s | Train: 1.8771 (45.0%) | Phys: 0.72 | Val: 2.4450 (37.88%) | Best: 38.10%\n",
            "Epoch 162 | T: 10s | Train: 1.8685 (44.9%) | Phys: 0.72 | Val: 2.4568 (38.16%) | Best: 38.16%\n",
            "Epoch 163 | T: 10s | Train: 1.8686 (44.8%) | Phys: 0.72 | Val: 2.4451 (39.17%) | Best: 39.17%\n",
            "Epoch 164 | T: 10s | Train: 1.8650 (45.0%) | Phys: 0.72 | Val: 2.4689 (37.53%) | Best: 39.17%\n",
            "Epoch 165 | T: 10s | Train: 1.8600 (45.2%) | Phys: 0.72 | Val: 2.4693 (37.69%) | Best: 39.17%\n",
            "Epoch 166 | T: 10s | Train: 1.8533 (45.4%) | Phys: 0.72 | Val: 2.4573 (38.08%) | Best: 39.17%\n",
            "Epoch 167 | T: 10s | Train: 1.8476 (45.5%) | Phys: 0.72 | Val: 2.4381 (38.19%) | Best: 39.17%\n",
            "Epoch 168 | T: 10s | Train: 1.8405 (45.6%) | Phys: 0.72 | Val: 2.4526 (38.68%) | Best: 39.17%\n",
            "Epoch 169 | T: 10s | Train: 1.8373 (45.8%) | Phys: 0.72 | Val: 2.4405 (38.47%) | Best: 39.17%\n",
            "Epoch 170 | T: 10s | Train: 1.8333 (46.0%) | Phys: 0.72 | Val: 2.4483 (37.72%) | Best: 39.17%\n",
            "Epoch 171 | T: 10s | Train: 1.8258 (46.1%) | Phys: 0.72 | Val: 2.4445 (37.97%) | Best: 39.17%\n",
            "Epoch 172 | T: 10s | Train: 1.8261 (46.1%) | Phys: 0.72 | Val: 2.4385 (38.31%) | Best: 39.17%\n",
            "Epoch 173 | T: 10s | Train: 1.8170 (46.3%) | Phys: 0.72 | Val: 2.4412 (38.65%) | Best: 39.17%\n",
            "Epoch 174 | T: 10s | Train: 1.8133 (46.6%) | Phys: 0.72 | Val: 2.4621 (37.86%) | Best: 39.17%\n",
            "Epoch 175 | T: 10s | Train: 1.8087 (46.5%) | Phys: 0.72 | Val: 2.4524 (38.28%) | Best: 39.17%\n",
            "Epoch 176 | T: 10s | Train: 1.7990 (46.8%) | Phys: 0.72 | Val: 2.4605 (38.23%) | Best: 39.17%\n",
            "Epoch 177 | T: 10s | Train: 1.8027 (46.8%) | Phys: 0.72 | Val: 2.4270 (38.61%) | Best: 39.17%\n",
            "Epoch 178 | T: 10s | Train: 1.7975 (47.0%) | Phys: 0.72 | Val: 2.4268 (38.51%) | Best: 39.17%\n",
            "Epoch 179 | T: 10s | Train: 1.7891 (47.2%) | Phys: 0.72 | Val: 2.4230 (38.76%) | Best: 39.17%\n",
            "Epoch 180 | T: 10s | Train: 1.7772 (47.4%) | Phys: 0.72 | Val: 2.4413 (38.70%) | Best: 39.17%\n",
            "Epoch 181 | T: 10s | Train: 1.7750 (47.4%) | Phys: 0.72 | Val: 2.4322 (38.60%) | Best: 39.17%\n",
            "Epoch 182 | T: 10s | Train: 1.7740 (47.6%) | Phys: 0.72 | Val: 2.4551 (38.10%) | Best: 39.17%\n",
            "Epoch 183 | T: 10s | Train: 1.7643 (47.7%) | Phys: 0.72 | Val: 2.4220 (38.44%) | Best: 39.17%\n",
            "Epoch 184 | T: 10s | Train: 1.7626 (47.7%) | Phys: 0.72 | Val: 2.4269 (38.49%) | Best: 39.17%\n",
            "Epoch 185 | T: 10s | Train: 1.7627 (47.7%) | Phys: 0.72 | Val: 2.4112 (39.20%) | Best: 39.20%\n",
            "Epoch 186 | T: 10s | Train: 1.7556 (47.8%) | Phys: 0.72 | Val: 2.4243 (38.90%) | Best: 39.20%\n",
            "Epoch 187 | T: 10s | Train: 1.7539 (48.0%) | Phys: 0.72 | Val: 2.4189 (39.54%) | Best: 39.54%\n",
            "Epoch 188 | T: 10s | Train: 1.7430 (48.1%) | Phys: 0.72 | Val: 2.4260 (38.97%) | Best: 39.54%\n",
            "Epoch 189 | T: 10s | Train: 1.7360 (48.2%) | Phys: 0.72 | Val: 2.4111 (39.03%) | Best: 39.54%\n",
            "Epoch 190 | T: 10s | Train: 1.7367 (48.3%) | Phys: 0.72 | Val: 2.4282 (38.94%) | Best: 39.54%\n",
            "Epoch 191 | T: 10s | Train: 1.7337 (48.1%) | Phys: 0.72 | Val: 2.4331 (38.59%) | Best: 39.54%\n",
            "Epoch 192 | T: 10s | Train: 1.7201 (48.7%) | Phys: 0.72 | Val: 2.4033 (39.75%) | Best: 39.75%\n",
            "Epoch 193 | T: 10s | Train: 1.7210 (48.7%) | Phys: 0.72 | Val: 2.4134 (39.55%) | Best: 39.75%\n",
            "Epoch 194 | T: 10s | Train: 1.7165 (48.8%) | Phys: 0.72 | Val: 2.4234 (38.47%) | Best: 39.75%\n",
            "Epoch 195 | T: 10s | Train: 1.7058 (49.1%) | Phys: 0.72 | Val: 2.4301 (38.87%) | Best: 39.75%\n",
            "Epoch 196 | T: 10s | Train: 1.7000 (49.2%) | Phys: 0.72 | Val: 2.4213 (39.37%) | Best: 39.75%\n",
            "Epoch 197 | T: 10s | Train: 1.6981 (49.3%) | Phys: 0.72 | Val: 2.4213 (39.20%) | Best: 39.75%\n",
            "Epoch 198 | T: 10s | Train: 1.6904 (49.6%) | Phys: 0.72 | Val: 2.3940 (39.37%) | Best: 39.75%\n",
            "Epoch 199 | T: 10s | Train: 1.6928 (49.5%) | Phys: 0.72 | Val: 2.4223 (38.95%) | Best: 39.75%\n",
            "Epoch 200 | T: 10s | Train: 1.6890 (49.6%) | Phys: 0.72 | Val: 2.4141 (39.42%) | Best: 39.75%\n",
            "Epoch 201 | T: 10s | Train: 1.6795 (49.5%) | Phys: 0.72 | Val: 2.4093 (39.57%) | Best: 39.75%\n",
            "Epoch 202 | T: 10s | Train: 1.6772 (49.9%) | Phys: 0.72 | Val: 2.3993 (39.30%) | Best: 39.75%\n",
            "Epoch 203 | T: 10s | Train: 1.6727 (50.1%) | Phys: 0.72 | Val: 2.4185 (39.07%) | Best: 39.75%\n",
            "Epoch 204 | T: 10s | Train: 1.6610 (50.2%) | Phys: 0.72 | Val: 2.4040 (39.87%) | Best: 39.87%\n",
            "Epoch 205 | T: 10s | Train: 1.6598 (50.3%) | Phys: 0.72 | Val: 2.4148 (39.19%) | Best: 39.87%\n",
            "Epoch 206 | T: 10s | Train: 1.6539 (50.5%) | Phys: 0.72 | Val: 2.4198 (39.84%) | Best: 39.87%\n",
            "Epoch 207 | T: 10s | Train: 1.6501 (50.6%) | Phys: 0.72 | Val: 2.4037 (39.83%) | Best: 39.87%\n",
            "Epoch 208 | T: 10s | Train: 1.6468 (50.8%) | Phys: 0.72 | Val: 2.3933 (40.11%) | Best: 40.11%\n",
            "Epoch 209 | T: 10s | Train: 1.6344 (51.0%) | Phys: 0.72 | Val: 2.4224 (39.55%) | Best: 40.11%\n",
            "Epoch 210 | T: 10s | Train: 1.6351 (51.0%) | Phys: 0.72 | Val: 2.3989 (39.74%) | Best: 40.11%\n",
            "Epoch 211 | T: 10s | Train: 1.6294 (51.0%) | Phys: 0.72 | Val: 2.3949 (39.64%) | Best: 40.11%\n",
            "Epoch 212 | T: 10s | Train: 1.6285 (51.1%) | Phys: 0.72 | Val: 2.4033 (40.11%) | Best: 40.11%\n",
            "Epoch 213 | T: 10s | Train: 1.6185 (51.3%) | Phys: 0.72 | Val: 2.4382 (38.76%) | Best: 40.11%\n",
            "Epoch 214 | T: 10s | Train: 1.6164 (51.5%) | Phys: 0.72 | Val: 2.3802 (40.10%) | Best: 40.11%\n",
            "Epoch 215 | T: 10s | Train: 1.6150 (51.8%) | Phys: 0.72 | Val: 2.3791 (40.69%) | Best: 40.69%\n",
            "Epoch 216 | T: 10s | Train: 1.6040 (51.8%) | Phys: 0.72 | Val: 2.4122 (39.76%) | Best: 40.69%\n",
            "Epoch 217 | T: 10s | Train: 1.5986 (52.1%) | Phys: 0.72 | Val: 2.4104 (39.90%) | Best: 40.69%\n",
            "Epoch 218 | T: 10s | Train: 1.5955 (52.0%) | Phys: 0.72 | Val: 2.4002 (39.71%) | Best: 40.69%\n",
            "Epoch 219 | T: 10s | Train: 1.5914 (52.2%) | Phys: 0.72 | Val: 2.3996 (40.17%) | Best: 40.69%\n",
            "Epoch 220 | T: 10s | Train: 1.5809 (52.3%) | Phys: 0.72 | Val: 2.4221 (39.33%) | Best: 40.69%\n",
            "Epoch 221 | T: 10s | Train: 1.5831 (52.0%) | Phys: 0.72 | Val: 2.4010 (40.25%) | Best: 40.69%\n",
            "Epoch 222 | T: 10s | Train: 1.5788 (52.5%) | Phys: 0.72 | Val: 2.4261 (39.73%) | Best: 40.69%\n",
            "Epoch 223 | T: 10s | Train: 1.5701 (52.8%) | Phys: 0.72 | Val: 2.4023 (40.50%) | Best: 40.69%\n",
            "Epoch 224 | T: 10s | Train: 1.5723 (52.5%) | Phys: 0.72 | Val: 2.4062 (40.14%) | Best: 40.69%\n",
            "Epoch 225 | T: 10s | Train: 1.5635 (52.6%) | Phys: 0.72 | Val: 2.4198 (39.54%) | Best: 40.69%\n",
            "Epoch 226 | T: 10s | Train: 1.5585 (53.0%) | Phys: 0.72 | Val: 2.4164 (39.76%) | Best: 40.69%\n",
            "Epoch 227 | T: 10s | Train: 1.5578 (53.2%) | Phys: 0.72 | Val: 2.4165 (39.98%) | Best: 40.69%\n",
            "Epoch 228 | T: 10s | Train: 1.5502 (53.3%) | Phys: 0.72 | Val: 2.4200 (40.20%) | Best: 40.69%\n",
            "Epoch 229 | T: 10s | Train: 1.5395 (53.5%) | Phys: 0.72 | Val: 2.4304 (39.42%) | Best: 40.69%\n",
            "Epoch 230 | T: 10s | Train: 1.5388 (53.4%) | Phys: 0.73 | Val: 2.3873 (40.12%) | Best: 40.69%\n",
            "Epoch 231 | T: 10s | Train: 1.5281 (53.8%) | Phys: 0.72 | Val: 2.4120 (40.07%) | Best: 40.69%\n",
            "Epoch 232 | T: 10s | Train: 1.5305 (53.5%) | Phys: 0.72 | Val: 2.4007 (40.64%) | Best: 40.69%\n",
            "Epoch 233 | T: 10s | Train: 1.5252 (53.8%) | Phys: 0.72 | Val: 2.3825 (40.75%) | Best: 40.75%\n",
            "Epoch 234 | T: 10s | Train: 1.5165 (54.0%) | Phys: 0.72 | Val: 2.3851 (40.15%) | Best: 40.75%\n",
            "Epoch 235 | T: 10s | Train: 1.5143 (54.1%) | Phys: 0.72 | Val: 2.4045 (40.49%) | Best: 40.75%\n",
            "Epoch 236 | T: 10s | Train: 1.5084 (54.4%) | Phys: 0.72 | Val: 2.4112 (40.09%) | Best: 40.75%\n",
            "Epoch 237 | T: 10s | Train: 1.5045 (54.3%) | Phys: 0.72 | Val: 2.4090 (40.20%) | Best: 40.75%\n",
            "Epoch 238 | T: 11s | Train: 1.5057 (54.6%) | Phys: 0.73 | Val: 2.4020 (39.96%) | Best: 40.75%\n",
            "Epoch 239 | T: 10s | Train: 1.4959 (54.7%) | Phys: 0.73 | Val: 2.3977 (40.50%) | Best: 40.75%\n",
            "Epoch 240 | T: 10s | Train: 1.4923 (54.9%) | Phys: 0.73 | Val: 2.3989 (40.98%) | Best: 40.98%\n",
            "Epoch 241 | T: 10s | Train: 1.4868 (55.0%) | Phys: 0.72 | Val: 2.4100 (40.51%) | Best: 40.98%\n",
            "Epoch 242 | T: 10s | Train: 1.4804 (55.2%) | Phys: 0.72 | Val: 2.4006 (40.60%) | Best: 40.98%\n",
            "Epoch 243 | T: 10s | Train: 1.4718 (55.4%) | Phys: 0.72 | Val: 2.4315 (39.84%) | Best: 40.98%\n",
            "Epoch 244 | T: 11s | Train: 1.4707 (55.3%) | Phys: 0.73 | Val: 2.4078 (40.51%) | Best: 40.98%\n",
            "Epoch 245 | T: 10s | Train: 1.4650 (55.4%) | Phys: 0.73 | Val: 2.3978 (40.44%) | Best: 40.98%\n",
            "Epoch 246 | T: 10s | Train: 1.4606 (55.4%) | Phys: 0.72 | Val: 2.3759 (41.36%) | Best: 41.36%\n",
            "Epoch 247 | T: 11s | Train: 1.4504 (56.0%) | Phys: 0.72 | Val: 2.4275 (39.55%) | Best: 41.36%\n",
            "Epoch 248 | T: 10s | Train: 1.4517 (56.0%) | Phys: 0.73 | Val: 2.3980 (40.26%) | Best: 41.36%\n",
            "Epoch 249 | T: 10s | Train: 1.4462 (56.0%) | Phys: 0.73 | Val: 2.3939 (40.69%) | Best: 41.36%\n",
            "Epoch 250 | T: 10s | Train: 1.4428 (56.1%) | Phys: 0.73 | Val: 2.4116 (40.21%) | Best: 41.36%\n",
            "Epoch 251 | T: 10s | Train: 1.4408 (56.3%) | Phys: 0.73 | Val: 2.4000 (40.43%) | Best: 41.36%\n",
            "Epoch 252 | T: 10s | Train: 1.4285 (56.4%) | Phys: 0.73 | Val: 2.3893 (41.32%) | Best: 41.36%\n",
            "Epoch 253 | T: 10s | Train: 1.4291 (56.4%) | Phys: 0.72 | Val: 2.4293 (40.15%) | Best: 41.36%\n",
            "Epoch 254 | T: 10s | Train: 1.4260 (56.6%) | Phys: 0.73 | Val: 2.4047 (40.63%) | Best: 41.36%\n",
            "Epoch 255 | T: 10s | Train: 1.4170 (57.0%) | Phys: 0.73 | Val: 2.4176 (40.73%) | Best: 41.36%\n",
            "Epoch 256 | T: 10s | Train: 1.4190 (56.7%) | Phys: 0.73 | Val: 2.3897 (41.04%) | Best: 41.36%\n",
            "Epoch 257 | T: 10s | Train: 1.4109 (57.0%) | Phys: 0.73 | Val: 2.3918 (41.09%) | Best: 41.36%\n",
            "Epoch 258 | T: 10s | Train: 1.4063 (57.1%) | Phys: 0.73 | Val: 2.3939 (40.93%) | Best: 41.36%\n",
            "Epoch 259 | T: 10s | Train: 1.3977 (57.2%) | Phys: 0.73 | Val: 2.4102 (40.64%) | Best: 41.36%\n",
            "Epoch 260 | T: 10s | Train: 1.3928 (57.5%) | Phys: 0.73 | Val: 2.3800 (41.10%) | Best: 41.36%\n",
            "Epoch 261 | T: 10s | Train: 1.3893 (57.8%) | Phys: 0.73 | Val: 2.4006 (40.56%) | Best: 41.36%\n",
            "Epoch 262 | T: 10s | Train: 1.3849 (57.7%) | Phys: 0.73 | Val: 2.4612 (39.68%) | Best: 41.36%\n",
            "Epoch 263 | T: 10s | Train: 1.3774 (58.0%) | Phys: 0.73 | Val: 2.3784 (41.74%) | Best: 41.74%\n",
            "Epoch 264 | T: 10s | Train: 1.3719 (58.1%) | Phys: 0.73 | Val: 2.3798 (41.64%) | Best: 41.74%\n",
            "Epoch 265 | T: 10s | Train: 1.3711 (58.2%) | Phys: 0.73 | Val: 2.4039 (40.30%) | Best: 41.74%\n",
            "Epoch 266 | T: 10s | Train: 1.3668 (58.2%) | Phys: 0.73 | Val: 2.4058 (41.09%) | Best: 41.74%\n",
            "Epoch 267 | T: 10s | Train: 1.3642 (58.4%) | Phys: 0.73 | Val: 2.4101 (40.29%) | Best: 41.74%\n",
            "Epoch 268 | T: 10s | Train: 1.3588 (58.4%) | Phys: 0.73 | Val: 2.3911 (41.16%) | Best: 41.74%\n",
            "Epoch 269 | T: 10s | Train: 1.3552 (58.8%) | Phys: 0.73 | Val: 2.4006 (41.30%) | Best: 41.74%\n",
            "Epoch 270 | T: 10s | Train: 1.3474 (58.7%) | Phys: 0.73 | Val: 2.4093 (40.86%) | Best: 41.74%\n",
            "Epoch 271 | T: 10s | Train: 1.3357 (59.1%) | Phys: 0.73 | Val: 2.4261 (41.27%) | Best: 41.74%\n",
            "Epoch 272 | T: 10s | Train: 1.3387 (59.0%) | Phys: 0.73 | Val: 2.4015 (41.31%) | Best: 41.74%\n",
            "Epoch 273 | T: 10s | Train: 1.3254 (59.5%) | Phys: 0.73 | Val: 2.4151 (41.14%) | Best: 41.74%\n",
            "Epoch 274 | T: 10s | Train: 1.3247 (59.2%) | Phys: 0.73 | Val: 2.4213 (41.16%) | Best: 41.74%\n",
            "Epoch 275 | T: 10s | Train: 1.3301 (59.3%) | Phys: 0.73 | Val: 2.4076 (41.37%) | Best: 41.74%\n",
            "Epoch 276 | T: 10s | Train: 1.3210 (59.4%) | Phys: 0.73 | Val: 2.4325 (41.38%) | Best: 41.74%\n",
            "Epoch 277 | T: 10s | Train: 1.3096 (59.8%) | Phys: 0.73 | Val: 2.4356 (40.76%) | Best: 41.74%\n",
            "Epoch 278 | T: 10s | Train: 1.3091 (59.7%) | Phys: 0.73 | Val: 2.4224 (41.30%) | Best: 41.74%\n",
            "Epoch 279 | T: 10s | Train: 1.3013 (59.9%) | Phys: 0.73 | Val: 2.4081 (41.60%) | Best: 41.74%\n",
            "Epoch 280 | T: 10s | Train: 1.2983 (60.0%) | Phys: 0.73 | Val: 2.4254 (41.03%) | Best: 41.74%\n",
            "Epoch 281 | T: 10s | Train: 1.2906 (60.2%) | Phys: 0.73 | Val: 2.4206 (41.12%) | Best: 41.74%\n",
            "Epoch 282 | T: 10s | Train: 1.2870 (60.4%) | Phys: 0.73 | Val: 2.4326 (41.23%) | Best: 41.74%\n",
            "Epoch 283 | T: 10s | Train: 1.2836 (60.8%) | Phys: 0.73 | Val: 2.4364 (40.93%) | Best: 41.74%\n",
            "Epoch 284 | T: 10s | Train: 1.2777 (60.8%) | Phys: 0.73 | Val: 2.4317 (41.18%) | Best: 41.74%\n",
            "Epoch 285 | T: 10s | Train: 1.2741 (61.0%) | Phys: 0.73 | Val: 2.4211 (40.95%) | Best: 41.74%\n",
            "Epoch 286 | T: 10s | Train: 1.2703 (61.0%) | Phys: 0.73 | Val: 2.4269 (41.29%) | Best: 41.74%\n",
            "Epoch 287 | T: 10s | Train: 1.2682 (60.9%) | Phys: 0.73 | Val: 2.4524 (41.03%) | Best: 41.74%\n",
            "Epoch 288 | T: 10s | Train: 1.2589 (61.3%) | Phys: 0.73 | Val: 2.4422 (41.22%) | Best: 41.74%\n",
            "Epoch 289 | T: 10s | Train: 1.2533 (61.6%) | Phys: 0.73 | Val: 2.4202 (41.47%) | Best: 41.74%\n",
            "Epoch 290 | T: 10s | Train: 1.2545 (61.4%) | Phys: 0.73 | Val: 2.4350 (40.97%) | Best: 41.74%\n",
            "Epoch 291 | T: 10s | Train: 1.2445 (61.6%) | Phys: 0.73 | Val: 2.4116 (41.40%) | Best: 41.74%\n",
            "Epoch 292 | T: 10s | Train: 1.2426 (61.6%) | Phys: 0.73 | Val: 2.4293 (41.30%) | Best: 41.74%\n",
            "Epoch 293 | T: 10s | Train: 1.2405 (61.9%) | Phys: 0.73 | Val: 2.4613 (40.70%) | Best: 41.74%\n",
            "Epoch 294 | T: 10s | Train: 1.2381 (61.8%) | Phys: 0.73 | Val: 2.4569 (40.92%) | Best: 41.74%\n",
            "Epoch 295 | T: 10s | Train: 1.2302 (62.3%) | Phys: 0.73 | Val: 2.4336 (41.33%) | Best: 41.74%\n",
            "Epoch 296 | T: 10s | Train: 1.2177 (62.3%) | Phys: 0.73 | Val: 2.4382 (41.29%) | Best: 41.74%\n",
            "Epoch 297 | T: 10s | Train: 1.2163 (62.5%) | Phys: 0.73 | Val: 2.4661 (40.76%) | Best: 41.74%\n",
            "Epoch 298 | T: 10s | Train: 1.2117 (62.6%) | Phys: 0.73 | Val: 2.4632 (41.18%) | Best: 41.74%\n",
            "Epoch 299 | T: 10s | Train: 1.2071 (62.7%) | Phys: 0.73 | Val: 2.4321 (41.51%) | Best: 41.74%\n",
            "Epoch 300 | T: 10s | Train: 1.2060 (63.0%) | Phys: 0.73 | Val: 2.4691 (41.29%) | Best: 41.74%\n",
            "Epoch 301 | T: 10s | Train: 1.1984 (62.8%) | Phys: 0.73 | Val: 2.4606 (41.09%) | Best: 41.74%\n",
            "Epoch 302 | T: 10s | Train: 1.1938 (63.3%) | Phys: 0.73 | Val: 2.4403 (41.17%) | Best: 41.74%\n",
            "Epoch 303 | T: 10s | Train: 1.1913 (63.2%) | Phys: 0.73 | Val: 2.4783 (41.07%) | Best: 41.74%\n",
            "Epoch 304 | T: 10s | Train: 1.1809 (63.7%) | Phys: 0.73 | Val: 2.4784 (40.48%) | Best: 41.74%\n",
            "Epoch 305 | T: 10s | Train: 1.1764 (63.5%) | Phys: 0.73 | Val: 2.4694 (40.82%) | Best: 41.74%\n",
            "Epoch 306 | T: 10s | Train: 1.1791 (63.5%) | Phys: 0.73 | Val: 2.4438 (41.38%) | Best: 41.74%\n",
            "Epoch 307 | T: 10s | Train: 1.1699 (63.6%) | Phys: 0.73 | Val: 2.4607 (41.56%) | Best: 41.74%\n",
            "Epoch 308 | T: 10s | Train: 1.1652 (64.0%) | Phys: 0.73 | Val: 2.4675 (40.91%) | Best: 41.74%\n",
            "Epoch 309 | T: 10s | Train: 1.1608 (64.0%) | Phys: 0.73 | Val: 2.4557 (41.61%) | Best: 41.74%\n",
            "Epoch 310 | T: 10s | Train: 1.1524 (64.2%) | Phys: 0.73 | Val: 2.4686 (41.12%) | Best: 41.74%\n",
            "Epoch 311 | T: 10s | Train: 1.1480 (64.5%) | Phys: 0.73 | Val: 2.4563 (41.73%) | Best: 41.74%\n",
            "Epoch 312 | T: 10s | Train: 1.1458 (64.4%) | Phys: 0.73 | Val: 2.4623 (41.25%) | Best: 41.74%\n",
            "Epoch 313 | T: 10s | Train: 1.1408 (64.7%) | Phys: 0.73 | Val: 2.4998 (40.70%) | Best: 41.74%\n",
            "Epoch 314 | T: 10s | Train: 1.1429 (64.6%) | Phys: 0.73 | Val: 2.4620 (41.41%) | Best: 41.74%\n",
            "Epoch 315 | T: 10s | Train: 1.1290 (64.9%) | Phys: 0.73 | Val: 2.4627 (41.66%) | Best: 41.74%\n",
            "Epoch 316 | T: 10s | Train: 1.1304 (64.9%) | Phys: 0.73 | Val: 2.4627 (42.17%) | Best: 42.17%\n",
            "Epoch 317 | T: 10s | Train: 1.1249 (65.2%) | Phys: 0.73 | Val: 2.4834 (40.62%) | Best: 42.17%\n",
            "Epoch 318 | T: 10s | Train: 1.1246 (65.1%) | Phys: 0.73 | Val: 2.4801 (41.41%) | Best: 42.17%\n",
            "Epoch 319 | T: 10s | Train: 1.1164 (65.4%) | Phys: 0.73 | Val: 2.4641 (41.27%) | Best: 42.17%\n",
            "Epoch 320 | T: 10s | Train: 1.1112 (65.5%) | Phys: 0.73 | Val: 2.4758 (41.21%) | Best: 42.17%\n",
            "Epoch 321 | T: 10s | Train: 1.1049 (65.6%) | Phys: 0.73 | Val: 2.4706 (41.99%) | Best: 42.17%\n",
            "Epoch 322 | T: 10s | Train: 1.1000 (66.1%) | Phys: 0.73 | Val: 2.5161 (40.76%) | Best: 42.17%\n",
            "Epoch 323 | T: 10s | Train: 1.0999 (65.9%) | Phys: 0.73 | Val: 2.4669 (41.92%) | Best: 42.17%\n",
            "Epoch 324 | T: 10s | Train: 1.0935 (66.1%) | Phys: 0.73 | Val: 2.4964 (40.96%) | Best: 42.17%\n",
            "Epoch 325 | T: 10s | Train: 1.0850 (66.3%) | Phys: 0.73 | Val: 2.4748 (41.94%) | Best: 42.17%\n",
            "Epoch 326 | T: 10s | Train: 1.0821 (66.4%) | Phys: 0.73 | Val: 2.4898 (40.73%) | Best: 42.17%\n",
            "Epoch 327 | T: 10s | Train: 1.0727 (66.5%) | Phys: 0.73 | Val: 2.4995 (41.14%) | Best: 42.17%\n",
            "Epoch 328 | T: 10s | Train: 1.0783 (66.7%) | Phys: 0.73 | Val: 2.4834 (41.48%) | Best: 42.17%\n",
            "Epoch 329 | T: 10s | Train: 1.0648 (66.9%) | Phys: 0.73 | Val: 2.4911 (41.25%) | Best: 42.17%\n",
            "Epoch 330 | T: 10s | Train: 1.0628 (66.7%) | Phys: 0.73 | Val: 2.4904 (41.51%) | Best: 42.17%\n",
            "Epoch 331 | T: 10s | Train: 1.0599 (67.1%) | Phys: 0.73 | Val: 2.5001 (41.46%) | Best: 42.17%\n",
            "Epoch 332 | T: 10s | Train: 1.0591 (67.0%) | Phys: 0.73 | Val: 2.4706 (42.11%) | Best: 42.17%\n",
            "Epoch 333 | T: 10s | Train: 1.0540 (67.2%) | Phys: 0.73 | Val: 2.5307 (40.57%) | Best: 42.17%\n",
            "Epoch 334 | T: 10s | Train: 1.0446 (67.5%) | Phys: 0.73 | Val: 2.5113 (41.43%) | Best: 42.17%\n",
            "Epoch 335 | T: 10s | Train: 1.0408 (67.6%) | Phys: 0.73 | Val: 2.5054 (41.70%) | Best: 42.17%\n",
            "Epoch 336 | T: 10s | Train: 1.0375 (67.7%) | Phys: 0.73 | Val: 2.5063 (41.58%) | Best: 42.17%\n",
            "Epoch 337 | T: 10s | Train: 1.0303 (67.8%) | Phys: 0.73 | Val: 2.5074 (41.57%) | Best: 42.17%\n",
            "Epoch 338 | T: 10s | Train: 1.0302 (67.8%) | Phys: 0.73 | Val: 2.5177 (40.85%) | Best: 42.17%\n",
            "Epoch 339 | T: 10s | Train: 1.0205 (68.4%) | Phys: 0.73 | Val: 2.5215 (41.11%) | Best: 42.17%\n",
            "Epoch 340 | T: 10s | Train: 1.0185 (68.2%) | Phys: 0.73 | Val: 2.5080 (41.49%) | Best: 42.17%\n",
            "Epoch 341 | T: 10s | Train: 1.0142 (68.4%) | Phys: 0.73 | Val: 2.5165 (41.48%) | Best: 42.17%\n",
            "Epoch 342 | T: 10s | Train: 1.0123 (68.4%) | Phys: 0.73 | Val: 2.5222 (41.65%) | Best: 42.17%\n",
            "Epoch 343 | T: 10s | Train: 1.0064 (68.5%) | Phys: 0.73 | Val: 2.5063 (41.50%) | Best: 42.17%\n",
            "Epoch 344 | T: 10s | Train: 1.0003 (68.7%) | Phys: 0.73 | Val: 2.5245 (41.02%) | Best: 42.17%\n",
            "Epoch 345 | T: 10s | Train: 0.9953 (69.0%) | Phys: 0.73 | Val: 2.5536 (40.74%) | Best: 42.17%\n",
            "Epoch 346 | T: 10s | Train: 0.9885 (69.1%) | Phys: 0.73 | Val: 2.5268 (41.65%) | Best: 42.17%\n",
            "Epoch 347 | T: 10s | Train: 0.9853 (69.2%) | Phys: 0.73 | Val: 2.5525 (41.52%) | Best: 42.17%\n",
            "Epoch 348 | T: 10s | Train: 0.9810 (69.7%) | Phys: 0.73 | Val: 2.5344 (41.16%) | Best: 42.17%\n",
            "Epoch 349 | T: 10s | Train: 0.9802 (69.4%) | Phys: 0.73 | Val: 2.5088 (41.90%) | Best: 42.17%\n",
            "Epoch 350 | T: 10s | Train: 0.9749 (69.7%) | Phys: 0.73 | Val: 2.5540 (41.31%) | Best: 42.17%\n",
            "Epoch 351 | T: 10s | Train: 0.9721 (69.6%) | Phys: 0.73 | Val: 2.5229 (41.75%) | Best: 42.17%\n",
            "Epoch 352 | T: 10s | Train: 0.9675 (69.7%) | Phys: 0.73 | Val: 2.5410 (41.21%) | Best: 42.17%\n",
            "Epoch 353 | T: 10s | Train: 0.9569 (70.3%) | Phys: 0.73 | Val: 2.5584 (40.57%) | Best: 42.17%\n",
            "Epoch 354 | T: 10s | Train: 0.9592 (70.1%) | Phys: 0.73 | Val: 2.5528 (41.12%) | Best: 42.17%\n",
            "Epoch 355 | T: 10s | Train: 0.9547 (70.1%) | Phys: 0.73 | Val: 2.5527 (41.46%) | Best: 42.17%\n",
            "Epoch 356 | T: 10s | Train: 0.9453 (70.1%) | Phys: 0.73 | Val: 2.5524 (41.73%) | Best: 42.17%\n",
            "Epoch 357 | T: 10s | Train: 0.9398 (70.8%) | Phys: 0.73 | Val: 2.5530 (41.62%) | Best: 42.17%\n",
            "Epoch 358 | T: 10s | Train: 0.9401 (70.6%) | Phys: 0.73 | Val: 2.5717 (41.25%) | Best: 42.17%\n",
            "Epoch 359 | T: 10s | Train: 0.9349 (70.9%) | Phys: 0.73 | Val: 2.5711 (41.25%) | Best: 42.17%\n",
            "Epoch 360 | T: 10s | Train: 0.9317 (70.9%) | Phys: 0.73 | Val: 2.5676 (41.69%) | Best: 42.17%\n",
            "Epoch 361 | T: 10s | Train: 0.9255 (71.1%) | Phys: 0.73 | Val: 2.5849 (41.02%) | Best: 42.17%\n",
            "Epoch 362 | T: 10s | Train: 0.9254 (70.9%) | Phys: 0.73 | Val: 2.5536 (41.83%) | Best: 42.17%\n",
            "Epoch 363 | T: 10s | Train: 0.9167 (71.3%) | Phys: 0.73 | Val: 2.5619 (41.53%) | Best: 42.17%\n",
            "Epoch 364 | T: 10s | Train: 0.9058 (71.7%) | Phys: 0.73 | Val: 2.5576 (42.02%) | Best: 42.17%\n",
            "Epoch 365 | T: 10s | Train: 0.9128 (71.3%) | Phys: 0.73 | Val: 2.5511 (41.08%) | Best: 42.17%\n",
            "Epoch 366 | T: 10s | Train: 0.9005 (71.7%) | Phys: 0.73 | Val: 2.5703 (41.14%) | Best: 42.17%\n",
            "Epoch 367 | T: 10s | Train: 0.9044 (71.7%) | Phys: 0.73 | Val: 2.6080 (40.92%) | Best: 42.17%\n",
            "Epoch 368 | T: 10s | Train: 0.8944 (72.0%) | Phys: 0.73 | Val: 2.5887 (41.26%) | Best: 42.17%\n",
            "Epoch 369 | T: 10s | Train: 0.8906 (72.2%) | Phys: 0.73 | Val: 2.5804 (41.44%) | Best: 42.17%\n",
            "Epoch 370 | T: 10s | Train: 0.8831 (72.0%) | Phys: 0.73 | Val: 2.5811 (41.49%) | Best: 42.17%\n",
            "Epoch 371 | T: 10s | Train: 0.8863 (72.4%) | Phys: 0.73 | Val: 2.5823 (41.59%) | Best: 42.17%\n",
            "Epoch 372 | T: 10s | Train: 0.8817 (72.2%) | Phys: 0.73 | Val: 2.6129 (41.26%) | Best: 42.17%\n",
            "Epoch 373 | T: 10s | Train: 0.8715 (72.6%) | Phys: 0.73 | Val: 2.6000 (41.86%) | Best: 42.17%\n",
            "Epoch 374 | T: 10s | Train: 0.8665 (72.7%) | Phys: 0.73 | Val: 2.6378 (40.89%) | Best: 42.17%\n",
            "Epoch 375 | T: 10s | Train: 0.8672 (72.7%) | Phys: 0.73 | Val: 2.6082 (41.59%) | Best: 42.17%\n",
            "Epoch 376 | T: 10s | Train: 0.8568 (73.4%) | Phys: 0.73 | Val: 2.5957 (41.90%) | Best: 42.17%\n",
            "Epoch 377 | T: 10s | Train: 0.8585 (72.9%) | Phys: 0.73 | Val: 2.6169 (41.29%) | Best: 42.17%\n",
            "Epoch 378 | T: 10s | Train: 0.8525 (73.4%) | Phys: 0.73 | Val: 2.6176 (41.27%) | Best: 42.17%\n",
            "Epoch 379 | T: 10s | Train: 0.8529 (73.3%) | Phys: 0.73 | Val: 2.6176 (41.31%) | Best: 42.17%\n",
            "Epoch 380 | T: 10s | Train: 0.8471 (73.4%) | Phys: 0.73 | Val: 2.6021 (41.50%) | Best: 42.17%\n",
            "Epoch 381 | T: 10s | Train: 0.8389 (73.5%) | Phys: 0.73 | Val: 2.6290 (40.86%) | Best: 42.17%\n",
            "Epoch 382 | T: 10s | Train: 0.8382 (73.8%) | Phys: 0.73 | Val: 2.6232 (41.54%) | Best: 42.17%\n",
            "Epoch 383 | T: 10s | Train: 0.8345 (73.9%) | Phys: 0.73 | Val: 2.6126 (41.31%) | Best: 42.17%\n",
            "Epoch 384 | T: 10s | Train: 0.8279 (74.2%) | Phys: 0.73 | Val: 2.6155 (41.55%) | Best: 42.17%\n",
            "Epoch 385 | T: 10s | Train: 0.8290 (74.1%) | Phys: 0.73 | Val: 2.6210 (41.63%) | Best: 42.17%\n",
            "Epoch 386 | T: 10s | Train: 0.8199 (74.3%) | Phys: 0.73 | Val: 2.6411 (41.69%) | Best: 42.17%\n",
            "Epoch 387 | T: 10s | Train: 0.8172 (74.2%) | Phys: 0.73 | Val: 2.6314 (41.70%) | Best: 42.17%\n",
            "Epoch 388 | T: 10s | Train: 0.8168 (74.3%) | Phys: 0.73 | Val: 2.6288 (41.65%) | Best: 42.17%\n",
            "Epoch 389 | T: 10s | Train: 0.8062 (74.7%) | Phys: 0.73 | Val: 2.6224 (41.52%) | Best: 42.17%\n",
            "Epoch 390 | T: 10s | Train: 0.8021 (75.1%) | Phys: 0.73 | Val: 2.6164 (41.53%) | Best: 42.17%\n",
            "Epoch 391 | T: 10s | Train: 0.8034 (74.9%) | Phys: 0.73 | Val: 2.6524 (41.44%) | Best: 42.17%\n",
            "Epoch 392 | T: 10s | Train: 0.7902 (75.1%) | Phys: 0.73 | Val: 2.6602 (41.49%) | Best: 42.17%\n",
            "Epoch 393 | T: 10s | Train: 0.7905 (75.1%) | Phys: 0.73 | Val: 2.6489 (41.54%) | Best: 42.17%\n",
            "Epoch 394 | T: 10s | Train: 0.7934 (75.0%) | Phys: 0.73 | Val: 2.6649 (41.47%) | Best: 42.17%\n",
            "Epoch 395 | T: 10s | Train: 0.7852 (75.5%) | Phys: 0.74 | Val: 2.6488 (41.38%) | Best: 42.17%\n",
            "Epoch 396 | T: 10s | Train: 0.7780 (75.7%) | Phys: 0.73 | Val: 2.6707 (41.27%) | Best: 42.17%\n",
            "Epoch 397 | T: 10s | Train: 0.7770 (75.6%) | Phys: 0.74 | Val: 2.6600 (41.79%) | Best: 42.17%\n",
            "Epoch 398 | T: 10s | Train: 0.7734 (75.8%) | Phys: 0.73 | Val: 2.6542 (41.52%) | Best: 42.17%\n",
            "Epoch 399 | T: 10s | Train: 0.7672 (76.0%) | Phys: 0.73 | Val: 2.6700 (41.73%) | Best: 42.17%\n",
            "Epoch 400 | T: 10s | Train: 0.7646 (76.0%) | Phys: 0.73 | Val: 2.6649 (41.58%) | Best: 42.17%\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": [],
      "name": "CIFAR-100 MLP Weak",
      "gpuType": "L4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}