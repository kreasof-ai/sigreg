{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/KellerJordan/Muon"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VksAFdlc0Qe2",
        "outputId": "6095b0c3-81cd-4803-971a-f3b01dc44ca0"
      },
      "id": "VksAFdlc0Qe2",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/KellerJordan/Muon\n",
            "  Cloning https://github.com/KellerJordan/Muon to /tmp/pip-req-build-rbepg3z4\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/KellerJordan/Muon /tmp/pip-req-build-rbepg3z4\n",
            "  Resolved https://github.com/KellerJordan/Muon to commit 6399c658d3c4a3356ba823fa6664b10e23871068\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: muon-optimizer\n",
            "  Building wheel for muon-optimizer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for muon-optimizer: filename=muon_optimizer-0.1.0-py3-none-any.whl size=7141 sha256=b83a842c137a81536e58526f9bd50fd8558d9592d8c21406371a2c645d11d92d\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-fr_1gmfg/wheels/6e/33/94/64d18603ba0f39064aab523d6edf493c388cfb7419bb5c9043\n",
            "Successfully built muon-optimizer\n",
            "Installing collected packages: muon-optimizer\n",
            "Successfully installed muon-optimizer-0.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "id": "pBxX3I1kYwojaamntSxiNZTK",
      "metadata": {
        "tags": [],
        "id": "pBxX3I1kYwojaamntSxiNZTK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d198d0a0-7e5b-42a3-b250-59fb8e0c6381"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "import math\n",
        "import time\n",
        "\n",
        "from muon import SingleDeviceMuonWithAuxAdam\n",
        "\n",
        "# ==========================================\n",
        "# 1. Configuration (Tuned for ViT on CIFAR)\n",
        "# ==========================================\n",
        "REG_MODE = 'weak'\n",
        "SIGR_ALPHA = 0.1   # Strength of the physics constraint\n",
        "SKETCH_DIM = 64    # Dimension of the random observer\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "LEARNING_RATE = 1e-2  # Slightly higher initial LR for AdamW with cosine schedule\n",
        "EPOCHS = 400\n",
        "WEIGHT_DECAY = 0.05\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "if torch.backends.mps.is_available(): DEVICE = 'mps'\n",
        "\n",
        "# Regularization Config\n",
        "MIXUP_ALPHA = 0.8\n",
        "CUTMIX_ALPHA = 1.0\n",
        "DROP_PATH_RATE = 0.1\n",
        "\n",
        "print(f\"Training on device: {DEVICE}\")\n",
        "\n",
        "# ==========================================\n",
        "# 2. Data Preparation (THE FIX: Strong Augmentation)\n",
        "# ==========================================\n",
        "def get_data_loaders():\n",
        "    print('==> Preparing data with Strong Augmentation...')\n",
        "\n",
        "    mean = (0.5071, 0.4867, 0.4408)\n",
        "    std = (0.2675, 0.2565, 0.2761)\n",
        "\n",
        "    # FIX 1: Add RandAugment\n",
        "    transform_train = transforms.Compose([\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandAugment(num_ops=2, magnitude=9), # <--- CRITICAL FOR ViT\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean, std),\n",
        "    ])\n",
        "\n",
        "    transform_test = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean, std),\n",
        "    ])\n",
        "\n",
        "    trainset = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=transform_train)\n",
        "    # Increase workers to handle augmentation load\n",
        "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
        "\n",
        "    testset = torchvision.datasets.CIFAR100(root='./data', train=False, download=True, transform=transform_test)\n",
        "    testloader = torch.utils.data.DataLoader(testset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
        "\n",
        "    return trainloader, testloader\n",
        "\n",
        "\n",
        "# ------------------------------------------\n",
        "# Physics Engine: The Regularizers\n",
        "# ------------------------------------------\n",
        "\n",
        "def sigreg_weak_loss(x, sketch_dim=64):\n",
        "    \"\"\"\n",
        "    Forces Covariance(x) ~ Identity.\n",
        "    Matches the 2nd Moment (Spherical Cloud).\n",
        "    \"\"\"\n",
        "    N, C = x.size()\n",
        "    # 1. Sketching (Optional for C=512, but good for consistency)\n",
        "    if C > sketch_dim:\n",
        "        S = torch.randn(sketch_dim, C, device=x.device) / (C ** 0.5)\n",
        "        x = x @ S.T  # [N, sketch_dim]\n",
        "    else:\n",
        "        sketch_dim = C\n",
        "\n",
        "    # 2. Centering & Covariance\n",
        "    x = x - x.mean(dim=0, keepdim=True)\n",
        "    cov = (x.T @ x) / (N - 1 + 1e-6)\n",
        "\n",
        "    # 3. Target Identity\n",
        "    target = torch.eye(sketch_dim, device=x.device)\n",
        "\n",
        "    # 4. Off-diagonal suppression + Diagonal maintenance\n",
        "    return torch.norm(cov - target, p='fro')\n",
        "\n",
        "def sigreg_strong_loss(x, sketch_dim=64):\n",
        "    \"\"\"\n",
        "    Forces ECF(x) ~ ECF(Gaussian).\n",
        "    Matches ALL Moments (Maximum Entropy Cloud).\n",
        "    Exact implementation of LeJEPA Algorithm 1.\n",
        "    \"\"\"\n",
        "    N, C = x.size()\n",
        "\n",
        "    # 1. Projection (The Observer)\n",
        "    # Project channels down to sketch_dim\n",
        "    A = torch.randn(C, sketch_dim, device=x.device)\n",
        "    A = A / (A.norm(p=2, dim=0, keepdim=True) + 1e-6)\n",
        "\n",
        "    # 2. Integration Points\n",
        "    t = torch.linspace(-5, 5, 17, device=x.device)\n",
        "\n",
        "    # 3. Theoretical Gaussian CF\n",
        "    exp_f = torch.exp(-0.5 * t**2)\n",
        "\n",
        "    # 4. Empirical CF\n",
        "    # proj: [N, sketch_dim]\n",
        "    proj = x @ A\n",
        "\n",
        "    # args: [N, sketch_dim, T]\n",
        "    args = proj.unsqueeze(2) * t.view(1, 1, -1)\n",
        "\n",
        "    # ecf: [sketch_dim, T] (Mean over batch)\n",
        "    ecf = torch.exp(1j * args).mean(dim=0)\n",
        "\n",
        "    # 5. Weighted L2 Distance\n",
        "    # |ecf - gauss|^2 * gauss_weight\n",
        "    diff_sq = (ecf - exp_f.unsqueeze(0)).abs().square()\n",
        "    err = diff_sq * exp_f.unsqueeze(0)\n",
        "\n",
        "    # 6. Integrate\n",
        "    loss = torch.trapz(err, t, dim=1) * N\n",
        "\n",
        "    return loss.mean()\n",
        "\n",
        "# ==========================================\n",
        "# 3. Mixup / CutMix Utilities\n",
        "# ==========================================\n",
        "def rand_bbox(size, lam):\n",
        "    W = size[2]\n",
        "    H = size[3]\n",
        "    cut_rat = np.sqrt(1. - lam)\n",
        "    cut_w = int(W * cut_rat)\n",
        "    cut_h = int(H * cut_rat)\n",
        "\n",
        "    cx = np.random.randint(W)\n",
        "    cy = np.random.randint(H)\n",
        "\n",
        "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
        "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
        "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
        "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
        "\n",
        "    return bbx1, bby1, bbx2, bby2\n",
        "\n",
        "def drop_path(x, drop_prob: float = 0., training: bool = False):\n",
        "    if drop_prob == 0. or not training: return x\n",
        "    keep_prob = 1 - drop_prob\n",
        "    shape = (x.shape[0],) + (1,) * (x.ndim - 1)\n",
        "    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
        "    random_tensor.floor_()\n",
        "    return x.div(keep_prob) * random_tensor\n",
        "\n",
        "class DropPath(nn.Module):\n",
        "    def __init__(self, drop_prob=None):\n",
        "        super(DropPath, self).__init__()\n",
        "        self.drop_prob = drop_prob\n",
        "    def forward(self, x): return drop_path(x, self.drop_prob, self.training)\n",
        "\n",
        "class PatchEmbedding(nn.Module):\n",
        "    def __init__(self, img_size=32, patch_size=4, in_chans=3, embed_dim=192):\n",
        "        super().__init__()\n",
        "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "    def forward(self, x):\n",
        "        x = self.proj(x).flatten(2).transpose(1, 2)\n",
        "        return x\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, dim, num_heads=8, qkv_bias=False, attn_drop=0., proj_drop=0.):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        head_dim = dim // num_heads\n",
        "        self.scale = head_dim ** -0.5\n",
        "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
        "        self.attn_drop = nn.Dropout(attn_drop)\n",
        "        self.proj = nn.Linear(dim, dim)\n",
        "        self.proj_drop = nn.Dropout(proj_drop)\n",
        "    def forward(self, x):\n",
        "        B, N, C = x.shape\n",
        "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        attn = self.attn_drop(attn)\n",
        "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
        "        x = self.proj(x)\n",
        "        x = self.proj_drop(x)\n",
        "        return x\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, dim, hidden_dim, drop=0.):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(dim, hidden_dim)\n",
        "        self.act = nn.GELU()\n",
        "        self.fc2 = nn.Linear(hidden_dim, dim)\n",
        "        self.drop = nn.Dropout(drop)\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.drop(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.drop(x)\n",
        "        return x\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, dim, num_heads, mlp_ratio=4., drop=0., attn_drop=0., drop_path=0., reg_mode='strong', sketch_dim=64):\n",
        "        super().__init__()\n",
        "        self.norm1 = nn.LayerNorm(dim)\n",
        "        self.attn = MultiHeadAttention(dim, num_heads=num_heads, qkv_bias=True, attn_drop=attn_drop, proj_drop=drop)\n",
        "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
        "        self.norm2 = nn.LayerNorm(dim)\n",
        "        self.mlp = MLP(dim, int(dim * mlp_ratio), drop=drop)\n",
        "\n",
        "        self.reg_mode = reg_mode\n",
        "        self.sketch_dim = sketch_dim\n",
        "    def forward(self, x):\n",
        "        x = x + self.drop_path(self.attn(self.norm1(x)))\n",
        "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
        "\n",
        "\n",
        "        # --- PHYSICS INJECTION ---\n",
        "        reg_loss = torch.tensor(0.0, device=x.device)\n",
        "        if self.reg_mode != 'baseline':\n",
        "            # Global Average Pool of the tokens [B, N, C] -> [B, C]\n",
        "            # This represents the \"Image Vector\" at this depth\n",
        "            flat_rep = x.mean(dim=1)\n",
        "\n",
        "            # Crucial: Pre-Norm vs Post-Norm context.\n",
        "            # LayerNorm forces variance=1. SIGReg forces Distribution=Gaussian.\n",
        "            # They are compatible.\n",
        "            if self.reg_mode == 'weak':\n",
        "                reg_loss = sigreg_weak_loss(flat_rep, self.sketch_dim)\n",
        "            elif self.reg_mode == 'strong':\n",
        "                reg_loss = sigreg_strong_loss(flat_rep, self.sketch_dim)\n",
        "\n",
        "        return x, reg_loss\n",
        "\n",
        "class VisionTransformer(nn.Module):\n",
        "    def __init__(self, img_size=32, patch_size=4, in_chans=3, num_classes=100, embed_dim=192, depth=9, num_heads=3, mlp_ratio=4., drop_rate=0., attn_drop_rate=0., drop_path_rate=0.1, reg_mode='strong', sketch_dim=64):\n",
        "        super().__init__()\n",
        "        self.patch_embed = PatchEmbedding(img_size, patch_size, in_chans, embed_dim)\n",
        "        num_patches = self.patch_embed.proj.weight.shape[2] # Just a hack to get patch count logic\n",
        "        num_patches = (img_size // patch_size) ** 2\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n",
        "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]\n",
        "        self.blocks = nn.ModuleList([\n",
        "            TransformerBlock(embed_dim, num_heads, mlp_ratio, drop_rate, attn_drop_rate, dpr[i], reg_mode, sketch_dim)\n",
        "            for i in range(depth)])\n",
        "        self.norm = nn.LayerNorm(embed_dim)\n",
        "        self.head = nn.Linear(embed_dim, num_classes)\n",
        "        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n",
        "        nn.init.trunc_normal_(self.cls_token, std=0.02)\n",
        "        nn.init.xavier_uniform_(self.head.weight)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B = x.shape[0]\n",
        "        x = self.patch_embed(x)\n",
        "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
        "        x = torch.cat((cls_tokens, x), dim=1)\n",
        "        x = x + self.pos_embed\n",
        "\n",
        "        total_phys_loss = 0.0\n",
        "\n",
        "        # Pass through blocks\n",
        "        for blk in self.blocks:\n",
        "            x, l_loss = blk(x, )\n",
        "            total_phys_loss += l_loss\n",
        "\n",
        "        x = self.norm(x)\n",
        "        out = self.head(x[:, 0])\n",
        "        return out, (total_phys_loss / len(self.blocks))\n",
        "\n",
        "# ==========================================\n",
        "# 5. Training Engine (Updated for Mixup/CutMix)\n",
        "# ==========================================\n",
        "def train(epoch, net, trainloader, optimizer, criterion):\n",
        "    net.train()\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    phys_loss_meter = 0\n",
        "\n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
        "\n",
        "        # Apply Mixup/CutMix\n",
        "        r = np.random.rand(1)\n",
        "        if r < 0.5: # Mixup\n",
        "            lam = np.random.beta(MIXUP_ALPHA, MIXUP_ALPHA)\n",
        "            index = torch.randperm(inputs.size(0)).to(DEVICE)\n",
        "            inputs = lam * inputs + (1 - lam) * inputs[index, :]\n",
        "            targets_a, targets_b = targets, targets[index]\n",
        "        else: # CutMix\n",
        "            lam = np.random.beta(CUTMIX_ALPHA, CUTMIX_ALPHA)\n",
        "            rand_index = torch.randperm(inputs.size(0)).to(DEVICE)\n",
        "            target_a = targets\n",
        "            target_b = targets[rand_index]\n",
        "            bbx1, bby1, bbx2, bby2 = rand_bbox(inputs.size(), lam)\n",
        "            inputs[:, :, bbx1:bbx2, bby1:bby2] = inputs[rand_index, :, bbx1:bbx2, bby1:bby2]\n",
        "            lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (inputs.size()[-1] * inputs.size()[-2]))\n",
        "            targets_a, targets_b = target_a, target_b\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward\n",
        "        outputs, p_loss = net(inputs)\n",
        "\n",
        "        # Task Loss\n",
        "        c_loss = criterion(outputs, targets_a) * lam + criterion(outputs, targets_b) * (1. - lam)\n",
        "\n",
        "        # Total Loss\n",
        "        loss = (1 - SIGR_ALPHA) * c_loss + (SIGR_ALPHA * p_loss)\n",
        "\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(net.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += ((1 - SIGR_ALPHA) * c_loss).item() # Log only task loss for comparison\n",
        "        phys_loss_meter += (SIGR_ALPHA * p_loss).item()\n",
        "\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += (lam * predicted.eq(targets_a).float() + (1 - lam) * predicted.eq(targets_b).float()).sum().item()\n",
        "\n",
        "    acc = 100. * correct / total\n",
        "    return train_loss / (batch_idx + 1), acc, phys_loss_meter / (batch_idx + 1)\n",
        "\n",
        "def test(epoch, net, testloader, criterion):\n",
        "    net.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "            inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
        "            outputs, _ = net(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "    acc = 100. * correct / total\n",
        "    return test_loss / (batch_idx + 1), acc\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    trainloader, testloader = get_data_loaders()\n",
        "\n",
        "    net = VisionTransformer(img_size=32, patch_size=4, embed_dim=192, depth=9, num_heads=3, drop_path_rate=0.1, reg_mode=REG_MODE, sketch_dim=SKETCH_DIM)\n",
        "    net = net.to(DEVICE)\n",
        "\n",
        "    hidden_weights = [p for p in net.parameters() if p.ndim == 2]\n",
        "    non_hidden_params = [p for p in net.parameters() if p.ndim != 2]\n",
        "\n",
        "    param_groups = [\n",
        "      dict(params=hidden_weights, use_muon=True, lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY),\n",
        "      dict(params=non_hidden_params, use_muon=False, lr=1e-3, betas=(0.9, 0.95), weight_decay=WEIGHT_DECAY)\n",
        "    ]\n",
        "\n",
        "    # Standard CrossEntropy for final eval, SoftLabel for training is handled by Mixup logic\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    optimizer = SingleDeviceMuonWithAuxAdam(param_groups)\n",
        "\n",
        "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS, eta_min=1e-5)\n",
        "\n",
        "    print(f\"Starting training for {EPOCHS} epochs with RandAugment + Mixup/CutMix...\")\n",
        "\n",
        "    best_acc = 0\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        start_time = time.time()\n",
        "\n",
        "        train_loss, train_acc, physics_loss = train(epoch, net, trainloader, optimizer, criterion)\n",
        "        test_loss, test_acc = test(epoch, net, testloader, criterion)\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        if test_acc > best_acc:\n",
        "            best_acc = test_acc\n",
        "            # torch.save(net.state_dict(), f'thermo_resnet_{REG_MODE}.pth')\n",
        "\n",
        "        epoch_time = time.time() - start_time\n",
        "\n",
        "        print(f\"Epoch {epoch+1} | T: {epoch_time:.0f}s | \"\n",
        "              f\"Train: {train_loss:.4f} ({train_acc:.1f}%) | \"\n",
        "              f\"Phys: {physics_loss:.2f} | \"\n",
        "              f\"Val: {test_loss:.4f} ({test_acc:.2f}%) | \"\n",
        "              f\"Best: {best_acc:.2f}%\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on device: cuda\n",
            "==> Preparing data with Strong Augmentation...\n",
            "Starting training for 400 epochs with RandAugment + Mixup/CutMix...\n",
            "Epoch 1 | T: 39s | Train: 3.8211 (6.6%) | Phys: 0.70 | Val: 3.4228 (19.26%) | Best: 19.26%\n",
            "Epoch 2 | T: 38s | Train: 3.5648 (11.4%) | Phys: 0.64 | Val: 3.0543 (25.77%) | Best: 25.77%\n",
            "Epoch 3 | T: 37s | Train: 3.4455 (13.8%) | Phys: 0.63 | Val: 2.8928 (28.85%) | Best: 28.85%\n",
            "Epoch 4 | T: 38s | Train: 3.3835 (15.2%) | Phys: 0.63 | Val: 2.7620 (29.64%) | Best: 29.64%\n",
            "Epoch 5 | T: 38s | Train: 3.3288 (16.4%) | Phys: 0.63 | Val: 2.6686 (33.62%) | Best: 33.62%\n",
            "Epoch 6 | T: 38s | Train: 3.2292 (18.6%) | Phys: 0.63 | Val: 2.5124 (36.54%) | Best: 36.54%\n",
            "Epoch 7 | T: 38s | Train: 3.1890 (19.4%) | Phys: 0.63 | Val: 2.4933 (37.32%) | Best: 37.32%\n",
            "Epoch 8 | T: 38s | Train: 3.1362 (20.9%) | Phys: 0.62 | Val: 2.3959 (39.35%) | Best: 39.35%\n",
            "Epoch 9 | T: 38s | Train: 3.1083 (21.6%) | Phys: 0.62 | Val: 2.3102 (41.20%) | Best: 41.20%\n",
            "Epoch 10 | T: 37s | Train: 3.0957 (22.2%) | Phys: 0.62 | Val: 2.2818 (41.23%) | Best: 41.23%\n",
            "Epoch 11 | T: 38s | Train: 3.0486 (23.0%) | Phys: 0.62 | Val: 2.1760 (43.59%) | Best: 43.59%\n",
            "Epoch 12 | T: 39s | Train: 3.0090 (23.9%) | Phys: 0.62 | Val: 2.2369 (43.04%) | Best: 43.59%\n",
            "Epoch 13 | T: 38s | Train: 3.0221 (23.7%) | Phys: 0.62 | Val: 2.3149 (41.87%) | Best: 43.59%\n",
            "Epoch 14 | T: 37s | Train: 2.9957 (24.2%) | Phys: 0.62 | Val: 2.2505 (42.42%) | Best: 43.59%\n",
            "Epoch 15 | T: 37s | Train: 2.9702 (25.0%) | Phys: 0.62 | Val: 2.2250 (43.92%) | Best: 43.92%\n",
            "Epoch 16 | T: 38s | Train: 2.9546 (25.4%) | Phys: 0.62 | Val: 2.1153 (45.13%) | Best: 45.13%\n",
            "Epoch 17 | T: 37s | Train: 2.9598 (25.3%) | Phys: 0.62 | Val: 2.0610 (46.38%) | Best: 46.38%\n",
            "Epoch 18 | T: 38s | Train: 2.9203 (26.4%) | Phys: 0.62 | Val: 2.0132 (48.01%) | Best: 48.01%\n",
            "Epoch 19 | T: 38s | Train: 2.9999 (24.5%) | Phys: 0.62 | Val: 2.0933 (46.03%) | Best: 48.01%\n",
            "Epoch 20 | T: 38s | Train: 2.8673 (27.5%) | Phys: 0.62 | Val: 2.0007 (48.86%) | Best: 48.86%\n",
            "Epoch 21 | T: 37s | Train: 2.9370 (26.0%) | Phys: 0.62 | Val: 2.0307 (48.52%) | Best: 48.86%\n",
            "Epoch 22 | T: 37s | Train: 2.8774 (27.2%) | Phys: 0.62 | Val: 1.9882 (48.05%) | Best: 48.86%\n",
            "Epoch 23 | T: 37s | Train: 2.9308 (26.3%) | Phys: 0.62 | Val: 2.0687 (47.67%) | Best: 48.86%\n",
            "Epoch 24 | T: 38s | Train: 2.9308 (26.3%) | Phys: 0.62 | Val: 1.9382 (49.76%) | Best: 49.76%\n",
            "Epoch 25 | T: 38s | Train: 2.8602 (27.9%) | Phys: 0.62 | Val: 1.9273 (50.00%) | Best: 50.00%\n",
            "Epoch 26 | T: 38s | Train: 2.8921 (27.2%) | Phys: 0.62 | Val: 2.0304 (48.54%) | Best: 50.00%\n",
            "Epoch 27 | T: 38s | Train: 2.8931 (27.1%) | Phys: 0.62 | Val: 1.9887 (49.10%) | Best: 50.00%\n",
            "Epoch 28 | T: 38s | Train: 2.8957 (27.0%) | Phys: 0.62 | Val: 1.9542 (49.27%) | Best: 50.00%\n",
            "Epoch 29 | T: 38s | Train: 2.8425 (28.1%) | Phys: 0.62 | Val: 1.9325 (49.63%) | Best: 50.00%\n",
            "Epoch 30 | T: 37s | Train: 2.8415 (28.3%) | Phys: 0.62 | Val: 1.8667 (51.77%) | Best: 51.77%\n",
            "Epoch 31 | T: 38s | Train: 2.8272 (28.6%) | Phys: 0.62 | Val: 1.9179 (51.14%) | Best: 51.77%\n",
            "Epoch 32 | T: 38s | Train: 2.8532 (28.2%) | Phys: 0.62 | Val: 1.8674 (51.32%) | Best: 51.77%\n",
            "Epoch 33 | T: 38s | Train: 2.8694 (27.8%) | Phys: 0.62 | Val: 1.9069 (50.20%) | Best: 51.77%\n",
            "Epoch 34 | T: 38s | Train: 2.8004 (29.3%) | Phys: 0.62 | Val: 1.8800 (51.02%) | Best: 51.77%\n",
            "Epoch 35 | T: 38s | Train: 2.8113 (29.3%) | Phys: 0.62 | Val: 1.9334 (50.62%) | Best: 51.77%\n",
            "Epoch 36 | T: 38s | Train: 2.8497 (28.5%) | Phys: 0.62 | Val: 1.9563 (50.35%) | Best: 51.77%\n",
            "Epoch 37 | T: 38s | Train: 2.8111 (29.2%) | Phys: 0.62 | Val: 1.9641 (49.39%) | Best: 51.77%\n",
            "Epoch 38 | T: 38s | Train: 2.7862 (29.8%) | Phys: 0.62 | Val: 1.8073 (52.50%) | Best: 52.50%\n",
            "Epoch 39 | T: 38s | Train: 2.8293 (28.8%) | Phys: 0.62 | Val: 1.8632 (52.07%) | Best: 52.50%\n",
            "Epoch 40 | T: 38s | Train: 2.8058 (29.4%) | Phys: 0.62 | Val: 1.8868 (51.83%) | Best: 52.50%\n",
            "Epoch 41 | T: 38s | Train: 2.7547 (30.4%) | Phys: 0.62 | Val: 1.8688 (51.12%) | Best: 52.50%\n",
            "Epoch 42 | T: 38s | Train: 2.8267 (28.8%) | Phys: 0.62 | Val: 1.8734 (52.95%) | Best: 52.95%\n",
            "Epoch 43 | T: 37s | Train: 2.7835 (29.7%) | Phys: 0.62 | Val: 1.7804 (53.75%) | Best: 53.75%\n",
            "Epoch 44 | T: 37s | Train: 2.7950 (29.4%) | Phys: 0.62 | Val: 1.8299 (52.71%) | Best: 53.75%\n",
            "Epoch 45 | T: 38s | Train: 2.7555 (30.4%) | Phys: 0.62 | Val: 1.8650 (52.27%) | Best: 53.75%\n",
            "Epoch 46 | T: 38s | Train: 2.7906 (30.0%) | Phys: 0.62 | Val: 1.8559 (52.25%) | Best: 53.75%\n",
            "Epoch 47 | T: 38s | Train: 2.7977 (29.5%) | Phys: 0.62 | Val: 1.8440 (52.76%) | Best: 53.75%\n",
            "Epoch 48 | T: 38s | Train: 2.7869 (29.9%) | Phys: 0.62 | Val: 1.8685 (52.44%) | Best: 53.75%\n",
            "Epoch 49 | T: 37s | Train: 2.7850 (30.1%) | Phys: 0.62 | Val: 1.7538 (53.73%) | Best: 53.75%\n",
            "Epoch 50 | T: 36s | Train: 2.7525 (30.7%) | Phys: 0.62 | Val: 1.8348 (52.89%) | Best: 53.75%\n",
            "Epoch 51 | T: 38s | Train: 2.8068 (29.3%) | Phys: 0.62 | Val: 1.8353 (52.49%) | Best: 53.75%\n",
            "Epoch 52 | T: 37s | Train: 2.7650 (30.6%) | Phys: 0.62 | Val: 1.8017 (53.08%) | Best: 53.75%\n",
            "Epoch 53 | T: 38s | Train: 2.7555 (30.7%) | Phys: 0.62 | Val: 1.8108 (54.01%) | Best: 54.01%\n",
            "Epoch 54 | T: 38s | Train: 2.8423 (28.6%) | Phys: 0.62 | Val: 1.8303 (52.69%) | Best: 54.01%\n",
            "Epoch 55 | T: 38s | Train: 2.7619 (30.4%) | Phys: 0.62 | Val: 1.7635 (54.02%) | Best: 54.02%\n",
            "Epoch 56 | T: 38s | Train: 2.7513 (30.6%) | Phys: 0.62 | Val: 1.7891 (53.08%) | Best: 54.02%\n",
            "Epoch 57 | T: 38s | Train: 2.7529 (30.5%) | Phys: 0.62 | Val: 1.8034 (53.40%) | Best: 54.02%\n",
            "Epoch 58 | T: 38s | Train: 2.8076 (29.4%) | Phys: 0.62 | Val: 1.8009 (53.61%) | Best: 54.02%\n",
            "Epoch 59 | T: 38s | Train: 2.8065 (29.5%) | Phys: 0.62 | Val: 1.8184 (53.01%) | Best: 54.02%\n",
            "Epoch 60 | T: 38s | Train: 2.7548 (30.7%) | Phys: 0.62 | Val: 1.7643 (53.98%) | Best: 54.02%\n",
            "Epoch 61 | T: 37s | Train: 2.7487 (30.8%) | Phys: 0.62 | Val: 1.7790 (54.33%) | Best: 54.33%\n",
            "Epoch 62 | T: 37s | Train: 2.7520 (30.6%) | Phys: 0.62 | Val: 1.7840 (53.38%) | Best: 54.33%\n",
            "Epoch 63 | T: 38s | Train: 2.7864 (30.0%) | Phys: 0.62 | Val: 1.8805 (52.56%) | Best: 54.33%\n",
            "Epoch 64 | T: 38s | Train: 2.7453 (30.8%) | Phys: 0.62 | Val: 1.8187 (52.80%) | Best: 54.33%\n",
            "Epoch 65 | T: 38s | Train: 2.7826 (29.9%) | Phys: 0.62 | Val: 1.8215 (53.03%) | Best: 54.33%\n",
            "Epoch 66 | T: 38s | Train: 2.7761 (30.0%) | Phys: 0.62 | Val: 1.7511 (54.85%) | Best: 54.85%\n",
            "Epoch 67 | T: 38s | Train: 2.7623 (30.5%) | Phys: 0.62 | Val: 1.8101 (53.62%) | Best: 54.85%\n",
            "Epoch 68 | T: 38s | Train: 2.7691 (30.4%) | Phys: 0.62 | Val: 1.8397 (53.63%) | Best: 54.85%\n",
            "Epoch 69 | T: 38s | Train: 2.7466 (30.8%) | Phys: 0.62 | Val: 1.7729 (53.76%) | Best: 54.85%\n",
            "Epoch 70 | T: 37s | Train: 2.7650 (30.3%) | Phys: 0.62 | Val: 1.8417 (52.26%) | Best: 54.85%\n",
            "Epoch 71 | T: 38s | Train: 2.7646 (30.5%) | Phys: 0.62 | Val: 1.7915 (54.26%) | Best: 54.85%\n",
            "Epoch 72 | T: 38s | Train: 2.7776 (29.9%) | Phys: 0.62 | Val: 1.7699 (54.52%) | Best: 54.85%\n",
            "Epoch 73 | T: 37s | Train: 2.7479 (30.9%) | Phys: 0.62 | Val: 1.7627 (54.41%) | Best: 54.85%\n",
            "Epoch 74 | T: 38s | Train: 2.7769 (30.2%) | Phys: 0.62 | Val: 1.7332 (55.59%) | Best: 55.59%\n",
            "Epoch 75 | T: 37s | Train: 2.7405 (30.9%) | Phys: 0.62 | Val: 1.7078 (55.43%) | Best: 55.59%\n",
            "Epoch 76 | T: 38s | Train: 2.7815 (30.3%) | Phys: 0.62 | Val: 1.8160 (54.34%) | Best: 55.59%\n",
            "Epoch 77 | T: 38s | Train: 2.7261 (31.4%) | Phys: 0.62 | Val: 1.7726 (53.94%) | Best: 55.59%\n",
            "Epoch 78 | T: 38s | Train: 2.7447 (31.3%) | Phys: 0.62 | Val: 1.7559 (54.42%) | Best: 55.59%\n",
            "Epoch 79 | T: 38s | Train: 2.6654 (32.9%) | Phys: 0.62 | Val: 1.6884 (55.55%) | Best: 55.59%\n",
            "Epoch 80 | T: 38s | Train: 2.7200 (31.6%) | Phys: 0.62 | Val: 1.7542 (54.77%) | Best: 55.59%\n",
            "Epoch 81 | T: 38s | Train: 2.7101 (31.9%) | Phys: 0.62 | Val: 1.7349 (54.90%) | Best: 55.59%\n",
            "Epoch 82 | T: 38s | Train: 2.7434 (31.2%) | Phys: 0.62 | Val: 1.7460 (54.99%) | Best: 55.59%\n",
            "Epoch 83 | T: 38s | Train: 2.7734 (30.5%) | Phys: 0.62 | Val: 1.7702 (53.80%) | Best: 55.59%\n",
            "Epoch 84 | T: 37s | Train: 2.7520 (30.8%) | Phys: 0.62 | Val: 1.7348 (54.70%) | Best: 55.59%\n",
            "Epoch 85 | T: 38s | Train: 2.6952 (32.0%) | Phys: 0.62 | Val: 1.6832 (56.30%) | Best: 56.30%\n",
            "Epoch 86 | T: 38s | Train: 2.6861 (32.4%) | Phys: 0.62 | Val: 1.7256 (54.90%) | Best: 56.30%\n",
            "Epoch 87 | T: 38s | Train: 2.7500 (30.9%) | Phys: 0.62 | Val: 1.7428 (54.41%) | Best: 56.30%\n",
            "Epoch 88 | T: 38s | Train: 2.7059 (31.9%) | Phys: 0.62 | Val: 1.7235 (55.05%) | Best: 56.30%\n",
            "Epoch 89 | T: 38s | Train: 2.7383 (31.5%) | Phys: 0.62 | Val: 1.6860 (55.91%) | Best: 56.30%\n",
            "Epoch 90 | T: 38s | Train: 2.6947 (32.2%) | Phys: 0.62 | Val: 1.7455 (54.61%) | Best: 56.30%\n",
            "Epoch 91 | T: 37s | Train: 2.7341 (31.4%) | Phys: 0.62 | Val: 1.6876 (56.15%) | Best: 56.30%\n",
            "Epoch 92 | T: 38s | Train: 2.6818 (32.6%) | Phys: 0.62 | Val: 1.7520 (55.55%) | Best: 56.30%\n",
            "Epoch 93 | T: 36s | Train: 2.6982 (32.2%) | Phys: 0.62 | Val: 1.7693 (54.95%) | Best: 56.30%\n",
            "Epoch 94 | T: 37s | Train: 2.7253 (31.5%) | Phys: 0.62 | Val: 1.7089 (55.91%) | Best: 56.30%\n",
            "Epoch 95 | T: 37s | Train: 2.7244 (31.5%) | Phys: 0.62 | Val: 1.7433 (54.73%) | Best: 56.30%\n",
            "Epoch 96 | T: 38s | Train: 2.7021 (32.2%) | Phys: 0.62 | Val: 1.7221 (55.68%) | Best: 56.30%\n",
            "Epoch 97 | T: 37s | Train: 2.6992 (32.0%) | Phys: 0.62 | Val: 1.6830 (56.06%) | Best: 56.30%\n",
            "Epoch 98 | T: 38s | Train: 2.7391 (31.4%) | Phys: 0.62 | Val: 1.7159 (55.84%) | Best: 56.30%\n",
            "Epoch 99 | T: 38s | Train: 2.7294 (31.9%) | Phys: 0.62 | Val: 1.7146 (55.82%) | Best: 56.30%\n",
            "Epoch 100 | T: 38s | Train: 2.7534 (30.6%) | Phys: 0.62 | Val: 1.7301 (54.97%) | Best: 56.30%\n",
            "Epoch 101 | T: 38s | Train: 2.6956 (32.3%) | Phys: 0.62 | Val: 1.7290 (55.62%) | Best: 56.30%\n",
            "Epoch 102 | T: 37s | Train: 2.6747 (32.8%) | Phys: 0.62 | Val: 1.7146 (56.01%) | Best: 56.30%\n",
            "Epoch 103 | T: 38s | Train: 2.7210 (31.7%) | Phys: 0.62 | Val: 1.6719 (56.24%) | Best: 56.30%\n",
            "Epoch 104 | T: 38s | Train: 2.7172 (31.7%) | Phys: 0.62 | Val: 1.7184 (56.68%) | Best: 56.68%\n",
            "Epoch 105 | T: 38s | Train: 2.6667 (33.0%) | Phys: 0.62 | Val: 1.6577 (56.65%) | Best: 56.68%\n",
            "Epoch 106 | T: 38s | Train: 2.6819 (32.6%) | Phys: 0.62 | Val: 1.7459 (54.74%) | Best: 56.68%\n",
            "Epoch 107 | T: 38s | Train: 2.7232 (31.6%) | Phys: 0.62 | Val: 1.7758 (55.17%) | Best: 56.68%\n",
            "Epoch 108 | T: 38s | Train: 2.7023 (32.2%) | Phys: 0.62 | Val: 1.6921 (55.71%) | Best: 56.68%\n",
            "Epoch 109 | T: 38s | Train: 2.7172 (31.7%) | Phys: 0.62 | Val: 1.6794 (56.07%) | Best: 56.68%\n",
            "Epoch 110 | T: 37s | Train: 2.7591 (31.0%) | Phys: 0.62 | Val: 1.7164 (55.42%) | Best: 56.68%\n",
            "Epoch 111 | T: 38s | Train: 2.6903 (32.6%) | Phys: 0.62 | Val: 1.6724 (56.53%) | Best: 56.68%\n",
            "Epoch 112 | T: 38s | Train: 2.6804 (32.8%) | Phys: 0.62 | Val: 1.6466 (56.90%) | Best: 56.90%\n",
            "Epoch 113 | T: 38s | Train: 2.7082 (32.0%) | Phys: 0.62 | Val: 1.7239 (55.42%) | Best: 56.90%\n",
            "Epoch 114 | T: 37s | Train: 2.6490 (33.3%) | Phys: 0.62 | Val: 1.6790 (57.26%) | Best: 57.26%\n",
            "Epoch 115 | T: 38s | Train: 2.6761 (33.0%) | Phys: 0.62 | Val: 1.6647 (56.81%) | Best: 57.26%\n",
            "Epoch 116 | T: 38s | Train: 2.6959 (32.3%) | Phys: 0.62 | Val: 1.7568 (54.75%) | Best: 57.26%\n",
            "Epoch 117 | T: 38s | Train: 2.6894 (32.4%) | Phys: 0.62 | Val: 1.7148 (56.51%) | Best: 57.26%\n",
            "Epoch 118 | T: 38s | Train: 2.6902 (32.7%) | Phys: 0.62 | Val: 1.7182 (56.22%) | Best: 57.26%\n",
            "Epoch 119 | T: 38s | Train: 2.6640 (33.1%) | Phys: 0.62 | Val: 1.6601 (56.88%) | Best: 57.26%\n",
            "Epoch 120 | T: 37s | Train: 2.6756 (32.8%) | Phys: 0.62 | Val: 1.7061 (56.03%) | Best: 57.26%\n",
            "Epoch 121 | T: 37s | Train: 2.7256 (31.7%) | Phys: 0.62 | Val: 1.6519 (56.85%) | Best: 57.26%\n",
            "Epoch 122 | T: 35s | Train: 2.6703 (32.8%) | Phys: 0.62 | Val: 1.7194 (55.91%) | Best: 57.26%\n",
            "Epoch 123 | T: 38s | Train: 2.6372 (33.5%) | Phys: 0.62 | Val: 1.6634 (56.75%) | Best: 57.26%\n",
            "Epoch 124 | T: 37s | Train: 2.6661 (33.2%) | Phys: 0.62 | Val: 1.6641 (56.50%) | Best: 57.26%\n",
            "Epoch 125 | T: 38s | Train: 2.6804 (32.8%) | Phys: 0.62 | Val: 1.6637 (57.90%) | Best: 57.90%\n",
            "Epoch 126 | T: 38s | Train: 2.6862 (32.8%) | Phys: 0.62 | Val: 1.6796 (56.78%) | Best: 57.90%\n",
            "Epoch 127 | T: 38s | Train: 2.7119 (32.2%) | Phys: 0.62 | Val: 1.6490 (57.06%) | Best: 57.90%\n",
            "Epoch 128 | T: 37s | Train: 2.6902 (32.3%) | Phys: 0.62 | Val: 1.6760 (57.19%) | Best: 57.90%\n",
            "Epoch 129 | T: 38s | Train: 2.6383 (34.0%) | Phys: 0.62 | Val: 1.6109 (57.62%) | Best: 57.90%\n",
            "Epoch 130 | T: 38s | Train: 2.6393 (33.9%) | Phys: 0.62 | Val: 1.6366 (57.52%) | Best: 57.90%\n",
            "Epoch 131 | T: 38s | Train: 2.6867 (32.8%) | Phys: 0.62 | Val: 1.6306 (57.73%) | Best: 57.90%\n",
            "Epoch 132 | T: 37s | Train: 2.6642 (33.3%) | Phys: 0.62 | Val: 1.6093 (58.00%) | Best: 58.00%\n",
            "Epoch 133 | T: 38s | Train: 2.6297 (34.0%) | Phys: 0.62 | Val: 1.6076 (57.76%) | Best: 58.00%\n",
            "Epoch 134 | T: 37s | Train: 2.6791 (33.1%) | Phys: 0.62 | Val: 1.6186 (57.55%) | Best: 58.00%\n",
            "Epoch 135 | T: 37s | Train: 2.6494 (33.4%) | Phys: 0.62 | Val: 1.6378 (58.16%) | Best: 58.16%\n",
            "Epoch 136 | T: 37s | Train: 2.6505 (33.5%) | Phys: 0.62 | Val: 1.6168 (57.79%) | Best: 58.16%\n",
            "Epoch 137 | T: 37s | Train: 2.6355 (33.7%) | Phys: 0.62 | Val: 1.6169 (58.08%) | Best: 58.16%\n",
            "Epoch 138 | T: 38s | Train: 2.5976 (34.8%) | Phys: 0.62 | Val: 1.5620 (58.79%) | Best: 58.79%\n",
            "Epoch 139 | T: 37s | Train: 2.6104 (34.6%) | Phys: 0.62 | Val: 1.5775 (59.00%) | Best: 59.00%\n",
            "Epoch 140 | T: 37s | Train: 2.6512 (33.2%) | Phys: 0.62 | Val: 1.6071 (58.61%) | Best: 59.00%\n",
            "Epoch 141 | T: 37s | Train: 2.6648 (33.1%) | Phys: 0.62 | Val: 1.6334 (57.18%) | Best: 59.00%\n",
            "Epoch 142 | T: 38s | Train: 2.6489 (33.6%) | Phys: 0.62 | Val: 1.6468 (57.39%) | Best: 59.00%\n",
            "Epoch 143 | T: 37s | Train: 2.6466 (33.7%) | Phys: 0.62 | Val: 1.6205 (57.67%) | Best: 59.00%\n",
            "Epoch 144 | T: 37s | Train: 2.5850 (34.9%) | Phys: 0.62 | Val: 1.5843 (58.50%) | Best: 59.00%\n",
            "Epoch 145 | T: 38s | Train: 2.6238 (34.3%) | Phys: 0.62 | Val: 1.6004 (57.89%) | Best: 59.00%\n",
            "Epoch 146 | T: 38s | Train: 2.6235 (34.3%) | Phys: 0.62 | Val: 1.5920 (58.37%) | Best: 59.00%\n",
            "Epoch 147 | T: 36s | Train: 2.6432 (33.6%) | Phys: 0.62 | Val: 1.5666 (59.08%) | Best: 59.08%\n",
            "Epoch 148 | T: 37s | Train: 2.6739 (32.9%) | Phys: 0.62 | Val: 1.5513 (59.46%) | Best: 59.46%\n",
            "Epoch 149 | T: 38s | Train: 2.6813 (32.7%) | Phys: 0.62 | Val: 1.5905 (59.57%) | Best: 59.57%\n",
            "Epoch 150 | T: 37s | Train: 2.5960 (34.8%) | Phys: 0.62 | Val: 1.6014 (58.67%) | Best: 59.57%\n",
            "Epoch 151 | T: 38s | Train: 2.5944 (35.0%) | Phys: 0.62 | Val: 1.5649 (59.37%) | Best: 59.57%\n",
            "Epoch 152 | T: 38s | Train: 2.5708 (35.5%) | Phys: 0.62 | Val: 1.5746 (58.95%) | Best: 59.57%\n",
            "Epoch 153 | T: 38s | Train: 2.5628 (36.2%) | Phys: 0.62 | Val: 1.5779 (58.30%) | Best: 59.57%\n",
            "Epoch 154 | T: 38s | Train: 2.5408 (36.1%) | Phys: 0.62 | Val: 1.5487 (59.03%) | Best: 59.57%\n",
            "Epoch 155 | T: 38s | Train: 2.6562 (33.7%) | Phys: 0.62 | Val: 1.6367 (58.47%) | Best: 59.57%\n",
            "Epoch 156 | T: 38s | Train: 2.6321 (34.3%) | Phys: 0.62 | Val: 1.5749 (59.52%) | Best: 59.57%\n",
            "Epoch 157 | T: 38s | Train: 2.6316 (34.1%) | Phys: 0.62 | Val: 1.5853 (58.26%) | Best: 59.57%\n",
            "Epoch 158 | T: 38s | Train: 2.6042 (34.7%) | Phys: 0.62 | Val: 1.5117 (60.36%) | Best: 60.36%\n",
            "Epoch 159 | T: 38s | Train: 2.5736 (35.6%) | Phys: 0.62 | Val: 1.5455 (59.76%) | Best: 60.36%\n",
            "Epoch 160 | T: 37s | Train: 2.6148 (34.6%) | Phys: 0.62 | Val: 1.5344 (60.05%) | Best: 60.36%\n",
            "Epoch 161 | T: 38s | Train: 2.5765 (35.6%) | Phys: 0.62 | Val: 1.5332 (60.34%) | Best: 60.36%\n",
            "Epoch 162 | T: 37s | Train: 2.5653 (35.7%) | Phys: 0.62 | Val: 1.5487 (59.20%) | Best: 60.36%\n",
            "Epoch 163 | T: 37s | Train: 2.5863 (35.1%) | Phys: 0.62 | Val: 1.5322 (59.91%) | Best: 60.36%\n",
            "Epoch 164 | T: 38s | Train: 2.5764 (35.4%) | Phys: 0.62 | Val: 1.5619 (59.18%) | Best: 60.36%\n",
            "Epoch 165 | T: 38s | Train: 2.5712 (35.6%) | Phys: 0.62 | Val: 1.5268 (60.34%) | Best: 60.36%\n",
            "Epoch 166 | T: 38s | Train: 2.5286 (36.5%) | Phys: 0.62 | Val: 1.5303 (60.01%) | Best: 60.36%\n",
            "Epoch 167 | T: 38s | Train: 2.5870 (35.3%) | Phys: 0.62 | Val: 1.5157 (60.04%) | Best: 60.36%\n",
            "Epoch 168 | T: 37s | Train: 2.5854 (35.3%) | Phys: 0.63 | Val: 1.5959 (58.00%) | Best: 60.36%\n",
            "Epoch 169 | T: 37s | Train: 2.6000 (34.9%) | Phys: 0.62 | Val: 1.5534 (60.11%) | Best: 60.36%\n",
            "Epoch 170 | T: 38s | Train: 2.5683 (36.1%) | Phys: 0.62 | Val: 1.5336 (60.32%) | Best: 60.36%\n",
            "Epoch 171 | T: 38s | Train: 2.5886 (35.6%) | Phys: 0.62 | Val: 1.4954 (61.59%) | Best: 61.59%\n",
            "Epoch 172 | T: 38s | Train: 2.5948 (35.3%) | Phys: 0.62 | Val: 1.4862 (60.98%) | Best: 61.59%\n",
            "Epoch 173 | T: 37s | Train: 2.5583 (36.3%) | Phys: 0.62 | Val: 1.4604 (61.62%) | Best: 61.62%\n",
            "Epoch 174 | T: 38s | Train: 2.5766 (35.5%) | Phys: 0.62 | Val: 1.4968 (61.03%) | Best: 61.62%\n",
            "Epoch 175 | T: 38s | Train: 2.5492 (36.5%) | Phys: 0.63 | Val: 1.5026 (61.16%) | Best: 61.62%\n",
            "Epoch 176 | T: 38s | Train: 2.5436 (36.5%) | Phys: 0.62 | Val: 1.4842 (61.15%) | Best: 61.62%\n",
            "Epoch 177 | T: 38s | Train: 2.5703 (35.8%) | Phys: 0.62 | Val: 1.5507 (59.75%) | Best: 61.62%\n",
            "Epoch 178 | T: 39s | Train: 2.5516 (36.3%) | Phys: 0.62 | Val: 1.4662 (61.39%) | Best: 61.62%\n",
            "Epoch 179 | T: 39s | Train: 2.5263 (36.8%) | Phys: 0.62 | Val: 1.5522 (59.80%) | Best: 61.62%\n",
            "Epoch 180 | T: 38s | Train: 2.5885 (35.3%) | Phys: 0.63 | Val: 1.5173 (61.21%) | Best: 61.62%\n",
            "Epoch 181 | T: 38s | Train: 2.5621 (36.0%) | Phys: 0.63 | Val: 1.4699 (61.59%) | Best: 61.62%\n",
            "Epoch 182 | T: 38s | Train: 2.5695 (35.8%) | Phys: 0.62 | Val: 1.4966 (60.71%) | Best: 61.62%\n",
            "Epoch 183 | T: 38s | Train: 2.5147 (37.3%) | Phys: 0.62 | Val: 1.4801 (60.67%) | Best: 61.62%\n",
            "Epoch 184 | T: 38s | Train: 2.5750 (35.5%) | Phys: 0.63 | Val: 1.4988 (60.88%) | Best: 61.62%\n",
            "Epoch 185 | T: 38s | Train: 2.5552 (36.1%) | Phys: 0.63 | Val: 1.4730 (61.79%) | Best: 61.79%\n",
            "Epoch 186 | T: 38s | Train: 2.5259 (37.1%) | Phys: 0.62 | Val: 1.4793 (61.29%) | Best: 61.79%\n",
            "Epoch 187 | T: 38s | Train: 2.5321 (36.6%) | Phys: 0.62 | Val: 1.4715 (61.08%) | Best: 61.79%\n",
            "Epoch 188 | T: 37s | Train: 2.5417 (36.9%) | Phys: 0.63 | Val: 1.4616 (61.99%) | Best: 61.99%\n",
            "Epoch 189 | T: 36s | Train: 2.5171 (37.0%) | Phys: 0.63 | Val: 1.4622 (61.60%) | Best: 61.99%\n",
            "Epoch 190 | T: 37s | Train: 2.5170 (37.4%) | Phys: 0.63 | Val: 1.5452 (60.54%) | Best: 61.99%\n",
            "Epoch 191 | T: 38s | Train: 2.4979 (37.5%) | Phys: 0.63 | Val: 1.4664 (61.61%) | Best: 61.99%\n",
            "Epoch 192 | T: 38s | Train: 2.4811 (38.3%) | Phys: 0.63 | Val: 1.4661 (61.53%) | Best: 61.99%\n",
            "Epoch 193 | T: 38s | Train: 2.5186 (37.3%) | Phys: 0.63 | Val: 1.4764 (61.39%) | Best: 61.99%\n",
            "Epoch 194 | T: 38s | Train: 2.4920 (37.6%) | Phys: 0.63 | Val: 1.4498 (62.15%) | Best: 62.15%\n",
            "Epoch 195 | T: 38s | Train: 2.4687 (38.2%) | Phys: 0.63 | Val: 1.4921 (61.24%) | Best: 62.15%\n",
            "Epoch 196 | T: 37s | Train: 2.5131 (37.3%) | Phys: 0.63 | Val: 1.4236 (62.78%) | Best: 62.78%\n",
            "Epoch 197 | T: 38s | Train: 2.4875 (37.9%) | Phys: 0.63 | Val: 1.4616 (61.76%) | Best: 62.78%\n",
            "Epoch 198 | T: 37s | Train: 2.4367 (39.2%) | Phys: 0.63 | Val: 1.4358 (62.13%) | Best: 62.78%\n",
            "Epoch 199 | T: 38s | Train: 2.4391 (39.5%) | Phys: 0.63 | Val: 1.4548 (62.55%) | Best: 62.78%\n",
            "Epoch 200 | T: 37s | Train: 2.4697 (38.5%) | Phys: 0.63 | Val: 1.4057 (63.50%) | Best: 63.50%\n",
            "Epoch 201 | T: 37s | Train: 2.5116 (37.3%) | Phys: 0.63 | Val: 1.4044 (62.62%) | Best: 63.50%\n",
            "Epoch 202 | T: 37s | Train: 2.4626 (38.3%) | Phys: 0.63 | Val: 1.4216 (62.95%) | Best: 63.50%\n",
            "Epoch 203 | T: 38s | Train: 2.5028 (37.4%) | Phys: 0.63 | Val: 1.4383 (62.55%) | Best: 63.50%\n",
            "Epoch 204 | T: 37s | Train: 2.4733 (38.4%) | Phys: 0.63 | Val: 1.3985 (63.11%) | Best: 63.50%\n",
            "Epoch 205 | T: 37s | Train: 2.4768 (38.5%) | Phys: 0.63 | Val: 1.4723 (61.35%) | Best: 63.50%\n",
            "Epoch 206 | T: 38s | Train: 2.4360 (39.5%) | Phys: 0.63 | Val: 1.3854 (63.86%) | Best: 63.86%\n",
            "Epoch 207 | T: 38s | Train: 2.4469 (39.2%) | Phys: 0.63 | Val: 1.4131 (63.59%) | Best: 63.86%\n",
            "Epoch 208 | T: 38s | Train: 2.4415 (39.2%) | Phys: 0.63 | Val: 1.3479 (64.12%) | Best: 64.12%\n",
            "Epoch 209 | T: 38s | Train: 2.4326 (39.4%) | Phys: 0.63 | Val: 1.3921 (63.61%) | Best: 64.12%\n",
            "Epoch 210 | T: 38s | Train: 2.4288 (39.7%) | Phys: 0.63 | Val: 1.3770 (64.48%) | Best: 64.48%\n",
            "Epoch 211 | T: 38s | Train: 2.4332 (39.3%) | Phys: 0.63 | Val: 1.3585 (64.66%) | Best: 64.66%\n",
            "Epoch 212 | T: 38s | Train: 2.3750 (41.0%) | Phys: 0.63 | Val: 1.3795 (63.95%) | Best: 64.66%\n",
            "Epoch 213 | T: 38s | Train: 2.3901 (40.7%) | Phys: 0.63 | Val: 1.3193 (64.82%) | Best: 64.82%\n",
            "Epoch 214 | T: 38s | Train: 2.4100 (39.9%) | Phys: 0.63 | Val: 1.3291 (64.46%) | Best: 64.82%\n",
            "Epoch 215 | T: 37s | Train: 2.4486 (39.2%) | Phys: 0.63 | Val: 1.3639 (64.39%) | Best: 64.82%\n",
            "Epoch 216 | T: 38s | Train: 2.4301 (39.4%) | Phys: 0.63 | Val: 1.4098 (63.70%) | Best: 64.82%\n",
            "Epoch 217 | T: 38s | Train: 2.4509 (39.1%) | Phys: 0.63 | Val: 1.3566 (64.87%) | Best: 64.87%\n",
            "Epoch 218 | T: 38s | Train: 2.4593 (38.9%) | Phys: 0.63 | Val: 1.3662 (64.43%) | Best: 64.87%\n",
            "Epoch 219 | T: 38s | Train: 2.4287 (39.5%) | Phys: 0.63 | Val: 1.3909 (63.64%) | Best: 64.87%\n",
            "Epoch 220 | T: 38s | Train: 2.3865 (40.7%) | Phys: 0.63 | Val: 1.3170 (65.09%) | Best: 65.09%\n",
            "Epoch 221 | T: 38s | Train: 2.4300 (39.8%) | Phys: 0.63 | Val: 1.3686 (64.06%) | Best: 65.09%\n",
            "Epoch 222 | T: 38s | Train: 2.3823 (40.8%) | Phys: 0.63 | Val: 1.3389 (64.90%) | Best: 65.09%\n",
            "Epoch 223 | T: 38s | Train: 2.4007 (40.2%) | Phys: 0.63 | Val: 1.3156 (64.75%) | Best: 65.09%\n",
            "Epoch 224 | T: 38s | Train: 2.3901 (40.8%) | Phys: 0.63 | Val: 1.3338 (64.83%) | Best: 65.09%\n",
            "Epoch 225 | T: 39s | Train: 2.3531 (41.7%) | Phys: 0.63 | Val: 1.3583 (64.63%) | Best: 65.09%\n",
            "Epoch 226 | T: 38s | Train: 2.3684 (41.4%) | Phys: 0.63 | Val: 1.2921 (65.84%) | Best: 65.84%\n",
            "Epoch 227 | T: 38s | Train: 2.3860 (40.4%) | Phys: 0.63 | Val: 1.3321 (65.51%) | Best: 65.84%\n",
            "Epoch 228 | T: 37s | Train: 2.3732 (41.2%) | Phys: 0.63 | Val: 1.2939 (66.10%) | Best: 66.10%\n",
            "Epoch 229 | T: 37s | Train: 2.3725 (41.0%) | Phys: 0.63 | Val: 1.3018 (65.89%) | Best: 66.10%\n",
            "Epoch 230 | T: 38s | Train: 2.3586 (41.8%) | Phys: 0.63 | Val: 1.3118 (65.23%) | Best: 66.10%\n",
            "Epoch 231 | T: 38s | Train: 2.3513 (41.7%) | Phys: 0.63 | Val: 1.2997 (66.00%) | Best: 66.10%\n",
            "Epoch 232 | T: 38s | Train: 2.3179 (42.5%) | Phys: 0.63 | Val: 1.2674 (66.39%) | Best: 66.39%\n",
            "Epoch 233 | T: 38s | Train: 2.2986 (43.0%) | Phys: 0.63 | Val: 1.2749 (66.13%) | Best: 66.39%\n",
            "Epoch 234 | T: 37s | Train: 2.3426 (42.0%) | Phys: 0.63 | Val: 1.2810 (66.40%) | Best: 66.40%\n",
            "Epoch 235 | T: 38s | Train: 2.3428 (42.1%) | Phys: 0.63 | Val: 1.3319 (65.32%) | Best: 66.40%\n",
            "Epoch 236 | T: 38s | Train: 2.3871 (40.8%) | Phys: 0.63 | Val: 1.3281 (65.05%) | Best: 66.40%\n",
            "Epoch 237 | T: 38s | Train: 2.3556 (41.6%) | Phys: 0.63 | Val: 1.2848 (66.23%) | Best: 66.40%\n",
            "Epoch 238 | T: 38s | Train: 2.3211 (42.4%) | Phys: 0.63 | Val: 1.2924 (66.39%) | Best: 66.40%\n",
            "Epoch 239 | T: 38s | Train: 2.3234 (42.6%) | Phys: 0.63 | Val: 1.2770 (66.72%) | Best: 66.72%\n",
            "Epoch 240 | T: 37s | Train: 2.3457 (42.2%) | Phys: 0.63 | Val: 1.3184 (65.92%) | Best: 66.72%\n",
            "Epoch 241 | T: 38s | Train: 2.2842 (43.7%) | Phys: 0.63 | Val: 1.2581 (66.63%) | Best: 66.72%\n",
            "Epoch 242 | T: 38s | Train: 2.3417 (42.3%) | Phys: 0.63 | Val: 1.2642 (66.71%) | Best: 66.72%\n",
            "Epoch 243 | T: 38s | Train: 2.3333 (42.5%) | Phys: 0.63 | Val: 1.2331 (66.74%) | Best: 66.74%\n",
            "Epoch 244 | T: 38s | Train: 2.2867 (43.3%) | Phys: 0.63 | Val: 1.2652 (66.63%) | Best: 66.74%\n",
            "Epoch 245 | T: 38s | Train: 2.3036 (43.3%) | Phys: 0.63 | Val: 1.2567 (67.04%) | Best: 67.04%\n",
            "Epoch 246 | T: 38s | Train: 2.3306 (42.6%) | Phys: 0.63 | Val: 1.2509 (66.79%) | Best: 67.04%\n",
            "Epoch 247 | T: 38s | Train: 2.3497 (41.9%) | Phys: 0.63 | Val: 1.2526 (67.09%) | Best: 67.09%\n",
            "Epoch 248 | T: 37s | Train: 2.3192 (42.8%) | Phys: 0.63 | Val: 1.3010 (66.00%) | Best: 67.09%\n",
            "Epoch 249 | T: 38s | Train: 2.2886 (43.5%) | Phys: 0.63 | Val: 1.2310 (67.03%) | Best: 67.09%\n",
            "Epoch 250 | T: 38s | Train: 2.2584 (44.5%) | Phys: 0.63 | Val: 1.2330 (67.38%) | Best: 67.38%\n",
            "Epoch 251 | T: 37s | Train: 2.2556 (44.4%) | Phys: 0.63 | Val: 1.2455 (66.91%) | Best: 67.38%\n",
            "Epoch 252 | T: 37s | Train: 2.2515 (44.6%) | Phys: 0.63 | Val: 1.2554 (66.89%) | Best: 67.38%\n",
            "Epoch 253 | T: 38s | Train: 2.2711 (44.1%) | Phys: 0.63 | Val: 1.2678 (67.28%) | Best: 67.38%\n",
            "Epoch 254 | T: 36s | Train: 2.2115 (45.4%) | Phys: 0.63 | Val: 1.2189 (67.33%) | Best: 67.38%\n",
            "Epoch 255 | T: 38s | Train: 2.1811 (46.2%) | Phys: 0.63 | Val: 1.1874 (68.39%) | Best: 68.39%\n",
            "Epoch 256 | T: 38s | Train: 2.1637 (46.6%) | Phys: 0.63 | Val: 1.1728 (68.85%) | Best: 68.85%\n",
            "Epoch 257 | T: 38s | Train: 2.3156 (43.0%) | Phys: 0.63 | Val: 1.2376 (68.45%) | Best: 68.85%\n",
            "Epoch 258 | T: 38s | Train: 2.2645 (44.3%) | Phys: 0.63 | Val: 1.2360 (67.94%) | Best: 68.85%\n",
            "Epoch 259 | T: 38s | Train: 2.2235 (45.4%) | Phys: 0.63 | Val: 1.2122 (67.71%) | Best: 68.85%\n",
            "Epoch 260 | T: 36s | Train: 2.2365 (45.2%) | Phys: 0.63 | Val: 1.2061 (68.14%) | Best: 68.85%\n",
            "Epoch 261 | T: 36s | Train: 2.1931 (46.1%) | Phys: 0.63 | Val: 1.1834 (68.85%) | Best: 68.85%\n",
            "Epoch 262 | T: 38s | Train: 2.2152 (45.7%) | Phys: 0.63 | Val: 1.2206 (68.23%) | Best: 68.85%\n",
            "Epoch 263 | T: 37s | Train: 2.2120 (45.8%) | Phys: 0.63 | Val: 1.2125 (67.78%) | Best: 68.85%\n",
            "Epoch 264 | T: 38s | Train: 2.1798 (46.9%) | Phys: 0.63 | Val: 1.1811 (68.66%) | Best: 68.85%\n",
            "Epoch 265 | T: 38s | Train: 2.1831 (46.7%) | Phys: 0.64 | Val: 1.1747 (68.41%) | Best: 68.85%\n",
            "Epoch 266 | T: 38s | Train: 2.1945 (46.2%) | Phys: 0.63 | Val: 1.2155 (67.87%) | Best: 68.85%\n",
            "Epoch 267 | T: 37s | Train: 2.1989 (46.3%) | Phys: 0.64 | Val: 1.1920 (69.01%) | Best: 69.01%\n",
            "Epoch 268 | T: 37s | Train: 2.2245 (45.5%) | Phys: 0.64 | Val: 1.1704 (69.16%) | Best: 69.16%\n",
            "Epoch 269 | T: 38s | Train: 2.1419 (47.9%) | Phys: 0.64 | Val: 1.1758 (68.95%) | Best: 69.16%\n",
            "Epoch 270 | T: 37s | Train: 2.2475 (44.7%) | Phys: 0.64 | Val: 1.1866 (69.21%) | Best: 69.21%\n",
            "Epoch 271 | T: 38s | Train: 2.2089 (46.0%) | Phys: 0.64 | Val: 1.1782 (69.39%) | Best: 69.39%\n",
            "Epoch 272 | T: 38s | Train: 2.1705 (46.9%) | Phys: 0.64 | Val: 1.1861 (68.76%) | Best: 69.39%\n",
            "Epoch 273 | T: 38s | Train: 2.1739 (47.1%) | Phys: 0.64 | Val: 1.1538 (69.61%) | Best: 69.61%\n",
            "Epoch 274 | T: 38s | Train: 2.2052 (46.2%) | Phys: 0.64 | Val: 1.1945 (69.18%) | Best: 69.61%\n",
            "Epoch 275 | T: 37s | Train: 2.1447 (47.7%) | Phys: 0.64 | Val: 1.1374 (69.67%) | Best: 69.67%\n",
            "Epoch 276 | T: 37s | Train: 2.1220 (48.6%) | Phys: 0.64 | Val: 1.1496 (69.79%) | Best: 69.79%\n",
            "Epoch 277 | T: 38s | Train: 2.1652 (47.4%) | Phys: 0.64 | Val: 1.1390 (70.41%) | Best: 70.41%\n",
            "Epoch 278 | T: 38s | Train: 2.1355 (48.0%) | Phys: 0.64 | Val: 1.1463 (69.80%) | Best: 70.41%\n",
            "Epoch 279 | T: 37s | Train: 2.1771 (46.7%) | Phys: 0.64 | Val: 1.1329 (70.10%) | Best: 70.41%\n",
            "Epoch 280 | T: 38s | Train: 2.1826 (46.8%) | Phys: 0.64 | Val: 1.1594 (69.75%) | Best: 70.41%\n",
            "Epoch 281 | T: 38s | Train: 2.1281 (48.2%) | Phys: 0.64 | Val: 1.1523 (69.78%) | Best: 70.41%\n",
            "Epoch 282 | T: 37s | Train: 2.1315 (48.1%) | Phys: 0.64 | Val: 1.1362 (70.08%) | Best: 70.41%\n",
            "Epoch 283 | T: 38s | Train: 2.0683 (49.6%) | Phys: 0.64 | Val: 1.1352 (70.17%) | Best: 70.41%\n",
            "Epoch 284 | T: 38s | Train: 2.1150 (48.6%) | Phys: 0.64 | Val: 1.1397 (69.83%) | Best: 70.41%\n",
            "Epoch 285 | T: 37s | Train: 2.0488 (50.3%) | Phys: 0.64 | Val: 1.1472 (70.07%) | Best: 70.41%\n",
            "Epoch 286 | T: 38s | Train: 2.1421 (47.9%) | Phys: 0.64 | Val: 1.1067 (70.62%) | Best: 70.62%\n",
            "Epoch 287 | T: 37s | Train: 2.0852 (49.7%) | Phys: 0.64 | Val: 1.1232 (70.58%) | Best: 70.62%\n",
            "Epoch 288 | T: 38s | Train: 2.1218 (48.5%) | Phys: 0.64 | Val: 1.1184 (70.55%) | Best: 70.62%\n",
            "Epoch 289 | T: 38s | Train: 2.1452 (48.1%) | Phys: 0.64 | Val: 1.1487 (70.50%) | Best: 70.62%\n",
            "Epoch 290 | T: 38s | Train: 2.0924 (49.1%) | Phys: 0.64 | Val: 1.1181 (70.94%) | Best: 70.94%\n",
            "Epoch 291 | T: 37s | Train: 2.0774 (49.7%) | Phys: 0.64 | Val: 1.1531 (70.09%) | Best: 70.94%\n",
            "Epoch 292 | T: 38s | Train: 2.0357 (51.0%) | Phys: 0.64 | Val: 1.1261 (70.20%) | Best: 70.94%\n",
            "Epoch 293 | T: 38s | Train: 2.0473 (50.7%) | Phys: 0.64 | Val: 1.1135 (70.84%) | Best: 70.94%\n",
            "Epoch 294 | T: 37s | Train: 2.0303 (51.3%) | Phys: 0.64 | Val: 1.0962 (70.99%) | Best: 70.99%\n",
            "Epoch 295 | T: 38s | Train: 2.0692 (50.1%) | Phys: 0.64 | Val: 1.0929 (71.12%) | Best: 71.12%\n",
            "Epoch 296 | T: 38s | Train: 2.0042 (51.6%) | Phys: 0.64 | Val: 1.0901 (70.98%) | Best: 71.12%\n",
            "Epoch 297 | T: 37s | Train: 2.0302 (50.8%) | Phys: 0.64 | Val: 1.1127 (70.88%) | Best: 71.12%\n",
            "Epoch 298 | T: 38s | Train: 2.0207 (51.6%) | Phys: 0.64 | Val: 1.0965 (70.69%) | Best: 71.12%\n",
            "Epoch 299 | T: 37s | Train: 2.0493 (50.4%) | Phys: 0.64 | Val: 1.1394 (70.98%) | Best: 71.12%\n",
            "Epoch 300 | T: 38s | Train: 2.0330 (51.1%) | Phys: 0.64 | Val: 1.1185 (70.73%) | Best: 71.12%\n",
            "Epoch 301 | T: 38s | Train: 1.9843 (52.5%) | Phys: 0.64 | Val: 1.0936 (71.12%) | Best: 71.12%\n",
            "Epoch 302 | T: 38s | Train: 2.0724 (50.3%) | Phys: 0.64 | Val: 1.0783 (71.89%) | Best: 71.89%\n",
            "Epoch 303 | T: 38s | Train: 1.9702 (52.9%) | Phys: 0.64 | Val: 1.1010 (71.24%) | Best: 71.89%\n",
            "Epoch 304 | T: 38s | Train: 2.0102 (51.7%) | Phys: 0.64 | Val: 1.0862 (71.31%) | Best: 71.89%\n",
            "Epoch 305 | T: 37s | Train: 2.0139 (51.4%) | Phys: 0.64 | Val: 1.0838 (71.44%) | Best: 71.89%\n",
            "Epoch 306 | T: 38s | Train: 1.9548 (53.3%) | Phys: 0.64 | Val: 1.0897 (71.53%) | Best: 71.89%\n",
            "Epoch 307 | T: 38s | Train: 2.0448 (50.6%) | Phys: 0.64 | Val: 1.0786 (71.85%) | Best: 71.89%\n",
            "Epoch 308 | T: 38s | Train: 2.0215 (51.5%) | Phys: 0.64 | Val: 1.0867 (71.95%) | Best: 71.95%\n",
            "Epoch 309 | T: 37s | Train: 2.0476 (50.9%) | Phys: 0.64 | Val: 1.0839 (71.75%) | Best: 71.95%\n",
            "Epoch 310 | T: 38s | Train: 2.0013 (52.1%) | Phys: 0.64 | Val: 1.1186 (71.33%) | Best: 71.95%\n",
            "Epoch 311 | T: 37s | Train: 1.9959 (52.2%) | Phys: 0.64 | Val: 1.0879 (71.36%) | Best: 71.95%\n",
            "Epoch 312 | T: 38s | Train: 1.9549 (53.4%) | Phys: 0.64 | Val: 1.1084 (71.28%) | Best: 71.95%\n",
            "Epoch 313 | T: 38s | Train: 1.9609 (52.9%) | Phys: 0.64 | Val: 1.0843 (71.79%) | Best: 71.95%\n",
            "Epoch 314 | T: 38s | Train: 1.9531 (53.5%) | Phys: 0.64 | Val: 1.0377 (73.25%) | Best: 73.25%\n",
            "Epoch 315 | T: 38s | Train: 1.9888 (52.3%) | Phys: 0.64 | Val: 1.0766 (71.58%) | Best: 73.25%\n",
            "Epoch 316 | T: 38s | Train: 1.9718 (52.8%) | Phys: 0.64 | Val: 1.0875 (72.04%) | Best: 73.25%\n",
            "Epoch 317 | T: 38s | Train: 1.9627 (53.4%) | Phys: 0.64 | Val: 1.0548 (72.56%) | Best: 73.25%\n",
            "Epoch 318 | T: 38s | Train: 1.9471 (53.6%) | Phys: 0.64 | Val: 1.0702 (72.38%) | Best: 73.25%\n",
            "Epoch 319 | T: 37s | Train: 1.9088 (54.8%) | Phys: 0.64 | Val: 1.0582 (72.66%) | Best: 73.25%\n",
            "Epoch 320 | T: 38s | Train: 1.9144 (54.8%) | Phys: 0.64 | Val: 1.0694 (72.59%) | Best: 73.25%\n",
            "Epoch 321 | T: 38s | Train: 1.9179 (54.6%) | Phys: 0.64 | Val: 1.0588 (72.70%) | Best: 73.25%\n",
            "Epoch 322 | T: 37s | Train: 1.8671 (55.8%) | Phys: 0.64 | Val: 1.0480 (72.45%) | Best: 73.25%\n",
            "Epoch 323 | T: 37s | Train: 1.9584 (53.4%) | Phys: 0.64 | Val: 1.0779 (72.64%) | Best: 73.25%\n",
            "Epoch 324 | T: 37s | Train: 1.8505 (56.6%) | Phys: 0.64 | Val: 1.0617 (72.60%) | Best: 73.25%\n",
            "Epoch 325 | T: 38s | Train: 1.9884 (52.6%) | Phys: 0.65 | Val: 1.0590 (72.54%) | Best: 73.25%\n",
            "Epoch 326 | T: 37s | Train: 1.8249 (57.2%) | Phys: 0.64 | Val: 1.0668 (72.65%) | Best: 73.25%\n",
            "Epoch 327 | T: 38s | Train: 1.8261 (57.0%) | Phys: 0.64 | Val: 1.0459 (72.90%) | Best: 73.25%\n",
            "Epoch 328 | T: 38s | Train: 1.8693 (55.8%) | Phys: 0.64 | Val: 1.0372 (72.83%) | Best: 73.25%\n",
            "Epoch 329 | T: 38s | Train: 1.8257 (57.4%) | Phys: 0.65 | Val: 1.0606 (72.95%) | Best: 73.25%\n",
            "Epoch 330 | T: 38s | Train: 1.9170 (54.1%) | Phys: 0.65 | Val: 1.0419 (73.29%) | Best: 73.29%\n",
            "Epoch 331 | T: 38s | Train: 1.8409 (56.6%) | Phys: 0.64 | Val: 1.0351 (73.09%) | Best: 73.29%\n",
            "Epoch 332 | T: 38s | Train: 1.8811 (55.7%) | Phys: 0.65 | Val: 1.0529 (73.23%) | Best: 73.29%\n",
            "Epoch 333 | T: 37s | Train: 1.8505 (56.5%) | Phys: 0.65 | Val: 1.0300 (73.04%) | Best: 73.29%\n",
            "Epoch 334 | T: 38s | Train: 1.7941 (58.3%) | Phys: 0.65 | Val: 1.0448 (73.12%) | Best: 73.29%\n",
            "Epoch 335 | T: 38s | Train: 1.8517 (56.5%) | Phys: 0.65 | Val: 1.0304 (73.74%) | Best: 73.74%\n",
            "Epoch 336 | T: 38s | Train: 1.7895 (58.1%) | Phys: 0.65 | Val: 1.0233 (73.29%) | Best: 73.74%\n",
            "Epoch 337 | T: 38s | Train: 1.7892 (58.4%) | Phys: 0.65 | Val: 1.0544 (72.97%) | Best: 73.74%\n",
            "Epoch 338 | T: 38s | Train: 1.8401 (57.0%) | Phys: 0.65 | Val: 1.0264 (73.85%) | Best: 73.85%\n",
            "Epoch 339 | T: 37s | Train: 1.8079 (57.8%) | Phys: 0.65 | Val: 1.0202 (73.83%) | Best: 73.85%\n",
            "Epoch 340 | T: 38s | Train: 1.8346 (56.8%) | Phys: 0.65 | Val: 1.0141 (73.98%) | Best: 73.98%\n",
            "Epoch 341 | T: 38s | Train: 1.7649 (58.9%) | Phys: 0.65 | Val: 1.0447 (73.50%) | Best: 73.98%\n",
            "Epoch 342 | T: 38s | Train: 1.7268 (59.7%) | Phys: 0.65 | Val: 1.0177 (74.24%) | Best: 74.24%\n",
            "Epoch 343 | T: 38s | Train: 1.8218 (57.5%) | Phys: 0.65 | Val: 1.0185 (74.63%) | Best: 74.63%\n",
            "Epoch 344 | T: 38s | Train: 1.7524 (59.1%) | Phys: 0.65 | Val: 1.0236 (74.11%) | Best: 74.63%\n",
            "Epoch 345 | T: 38s | Train: 1.8218 (57.4%) | Phys: 0.65 | Val: 1.0343 (73.73%) | Best: 74.63%\n",
            "Epoch 346 | T: 37s | Train: 1.7252 (60.1%) | Phys: 0.65 | Val: 1.0152 (74.12%) | Best: 74.63%\n",
            "Epoch 347 | T: 38s | Train: 1.7866 (58.3%) | Phys: 0.65 | Val: 1.0252 (73.67%) | Best: 74.63%\n",
            "Epoch 348 | T: 38s | Train: 1.7963 (57.9%) | Phys: 0.65 | Val: 1.0287 (73.97%) | Best: 74.63%\n",
            "Epoch 349 | T: 38s | Train: 1.7653 (58.6%) | Phys: 0.65 | Val: 1.0180 (73.84%) | Best: 74.63%\n",
            "Epoch 350 | T: 38s | Train: 1.7196 (60.2%) | Phys: 0.65 | Val: 1.0085 (74.08%) | Best: 74.63%\n",
            "Epoch 351 | T: 37s | Train: 1.8322 (57.0%) | Phys: 0.65 | Val: 1.0182 (74.34%) | Best: 74.63%\n",
            "Epoch 352 | T: 38s | Train: 1.8267 (56.8%) | Phys: 0.65 | Val: 0.9960 (74.78%) | Best: 74.78%\n",
            "Epoch 353 | T: 38s | Train: 1.7443 (59.6%) | Phys: 0.65 | Val: 1.0125 (74.35%) | Best: 74.78%\n",
            "Epoch 354 | T: 38s | Train: 1.6541 (62.1%) | Phys: 0.65 | Val: 1.0156 (74.38%) | Best: 74.78%\n",
            "Epoch 355 | T: 38s | Train: 1.7259 (59.9%) | Phys: 0.65 | Val: 1.0109 (74.71%) | Best: 74.78%\n",
            "Epoch 356 | T: 38s | Train: 1.7476 (59.3%) | Phys: 0.65 | Val: 1.0113 (74.26%) | Best: 74.78%\n",
            "Epoch 357 | T: 38s | Train: 1.7266 (60.1%) | Phys: 0.65 | Val: 1.0122 (74.68%) | Best: 74.78%\n",
            "Epoch 358 | T: 37s | Train: 1.7283 (60.4%) | Phys: 0.65 | Val: 1.0198 (74.73%) | Best: 74.78%\n",
            "Epoch 359 | T: 37s | Train: 1.7146 (60.5%) | Phys: 0.65 | Val: 0.9988 (74.54%) | Best: 74.78%\n",
            "Epoch 360 | T: 38s | Train: 1.7993 (57.8%) | Phys: 0.65 | Val: 0.9875 (74.93%) | Best: 74.93%\n",
            "Epoch 361 | T: 38s | Train: 1.7908 (58.4%) | Phys: 0.65 | Val: 1.0040 (74.83%) | Best: 74.93%\n",
            "Epoch 362 | T: 38s | Train: 1.7371 (59.7%) | Phys: 0.65 | Val: 0.9877 (75.09%) | Best: 75.09%\n",
            "Epoch 363 | T: 38s | Train: 1.7149 (59.9%) | Phys: 0.65 | Val: 0.9873 (75.09%) | Best: 75.09%\n",
            "Epoch 364 | T: 38s | Train: 1.7395 (59.8%) | Phys: 0.65 | Val: 0.9847 (75.01%) | Best: 75.09%\n",
            "Epoch 365 | T: 37s | Train: 1.6822 (61.0%) | Phys: 0.65 | Val: 0.9861 (74.93%) | Best: 75.09%\n",
            "Epoch 366 | T: 38s | Train: 1.6378 (62.3%) | Phys: 0.65 | Val: 0.9852 (74.78%) | Best: 75.09%\n",
            "Epoch 367 | T: 38s | Train: 1.6880 (60.9%) | Phys: 0.65 | Val: 0.9828 (75.63%) | Best: 75.63%\n",
            "Epoch 368 | T: 38s | Train: 1.6888 (61.1%) | Phys: 0.65 | Val: 0.9779 (75.55%) | Best: 75.63%\n",
            "Epoch 369 | T: 38s | Train: 1.6467 (62.1%) | Phys: 0.65 | Val: 0.9891 (75.40%) | Best: 75.63%\n",
            "Epoch 370 | T: 38s | Train: 1.6415 (62.0%) | Phys: 0.65 | Val: 0.9816 (75.47%) | Best: 75.63%\n",
            "Epoch 371 | T: 38s | Train: 1.6748 (61.2%) | Phys: 0.65 | Val: 0.9916 (75.20%) | Best: 75.63%\n",
            "Epoch 372 | T: 38s | Train: 1.6702 (61.2%) | Phys: 0.65 | Val: 0.9788 (75.40%) | Best: 75.63%\n",
            "Epoch 373 | T: 38s | Train: 1.6524 (62.2%) | Phys: 0.65 | Val: 0.9810 (75.46%) | Best: 75.63%\n",
            "Epoch 374 | T: 38s | Train: 1.5954 (63.4%) | Phys: 0.65 | Val: 0.9670 (75.99%) | Best: 75.99%\n",
            "Epoch 375 | T: 38s | Train: 1.7023 (60.5%) | Phys: 0.65 | Val: 0.9761 (75.53%) | Best: 75.99%\n",
            "Epoch 376 | T: 38s | Train: 1.6958 (60.9%) | Phys: 0.65 | Val: 0.9746 (75.73%) | Best: 75.99%\n",
            "Epoch 377 | T: 38s | Train: 1.6903 (60.7%) | Phys: 0.65 | Val: 0.9792 (75.76%) | Best: 75.99%\n",
            "Epoch 378 | T: 38s | Train: 1.6515 (61.9%) | Phys: 0.65 | Val: 0.9681 (76.09%) | Best: 76.09%\n",
            "Epoch 379 | T: 38s | Train: 1.6152 (63.3%) | Phys: 0.65 | Val: 0.9722 (75.63%) | Best: 76.09%\n",
            "Epoch 380 | T: 38s | Train: 1.6178 (63.0%) | Phys: 0.65 | Val: 0.9707 (76.03%) | Best: 76.09%\n",
            "Epoch 381 | T: 38s | Train: 1.6213 (62.5%) | Phys: 0.65 | Val: 0.9788 (75.89%) | Best: 76.09%\n",
            "Epoch 382 | T: 38s | Train: 1.6818 (60.8%) | Phys: 0.65 | Val: 0.9719 (75.84%) | Best: 76.09%\n",
            "Epoch 383 | T: 38s | Train: 1.6273 (63.3%) | Phys: 0.65 | Val: 0.9728 (75.88%) | Best: 76.09%\n",
            "Epoch 384 | T: 38s | Train: 1.6180 (62.8%) | Phys: 0.65 | Val: 0.9756 (75.82%) | Best: 76.09%\n",
            "Epoch 385 | T: 38s | Train: 1.6258 (62.8%) | Phys: 0.65 | Val: 0.9751 (75.95%) | Best: 76.09%\n",
            "Epoch 386 | T: 38s | Train: 1.6467 (62.0%) | Phys: 0.65 | Val: 0.9699 (75.85%) | Best: 76.09%\n",
            "Epoch 387 | T: 38s | Train: 1.6800 (61.2%) | Phys: 0.65 | Val: 0.9693 (75.97%) | Best: 76.09%\n",
            "Epoch 388 | T: 36s | Train: 1.6708 (61.6%) | Phys: 0.65 | Val: 0.9709 (75.77%) | Best: 76.09%\n",
            "Epoch 389 | T: 38s | Train: 1.6402 (62.2%) | Phys: 0.65 | Val: 0.9639 (76.07%) | Best: 76.09%\n",
            "Epoch 390 | T: 38s | Train: 1.6158 (63.2%) | Phys: 0.65 | Val: 0.9698 (75.96%) | Best: 76.09%\n",
            "Epoch 391 | T: 37s | Train: 1.6200 (62.9%) | Phys: 0.65 | Val: 0.9646 (76.11%) | Best: 76.11%\n",
            "Epoch 392 | T: 38s | Train: 1.6872 (61.2%) | Phys: 0.65 | Val: 0.9686 (76.08%) | Best: 76.11%\n",
            "Epoch 393 | T: 38s | Train: 1.5949 (63.6%) | Phys: 0.65 | Val: 0.9654 (76.01%) | Best: 76.11%\n",
            "Epoch 394 | T: 37s | Train: 1.5291 (65.5%) | Phys: 0.65 | Val: 0.9694 (76.11%) | Best: 76.11%\n",
            "Epoch 395 | T: 38s | Train: 1.6022 (63.8%) | Phys: 0.65 | Val: 0.9674 (76.19%) | Best: 76.19%\n",
            "Epoch 396 | T: 37s | Train: 1.5534 (64.3%) | Phys: 0.65 | Val: 0.9608 (76.12%) | Best: 76.19%\n",
            "Epoch 397 | T: 38s | Train: 1.7143 (60.1%) | Phys: 0.65 | Val: 0.9648 (76.19%) | Best: 76.19%\n",
            "Epoch 398 | T: 38s | Train: 1.6213 (63.2%) | Phys: 0.65 | Val: 0.9643 (76.12%) | Best: 76.19%\n",
            "Epoch 399 | T: 38s | Train: 1.6381 (62.9%) | Phys: 0.65 | Val: 0.9647 (76.24%) | Best: 76.24%\n",
            "Epoch 400 | T: 38s | Train: 1.5775 (64.1%) | Phys: 0.65 | Val: 0.9651 (76.14%) | Best: 76.24%\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": [],
      "name": "CIFAR-100 ViT SIGReg Weak",
      "gpuType": "L4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}