{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/KellerJordan/Muon"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5mzrN-dHw_BQ",
        "outputId": "36c38abd-d66e-449c-dda8-a3666e8cdcef"
      },
      "id": "5mzrN-dHw_BQ",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/KellerJordan/Muon\n",
            "  Cloning https://github.com/KellerJordan/Muon to /tmp/pip-req-build-oifnbiaf\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/KellerJordan/Muon /tmp/pip-req-build-oifnbiaf\n",
            "  Resolved https://github.com/KellerJordan/Muon to commit 6399c658d3c4a3356ba823fa6664b10e23871068\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: muon-optimizer\n",
            "  Building wheel for muon-optimizer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for muon-optimizer: filename=muon_optimizer-0.1.0-py3-none-any.whl size=7141 sha256=12f3dd99d0372fffcf39f1c9733980145c0fec5f908bc5a3207f7ae316267028\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-464g7e7q/wheels/6e/33/94/64d18603ba0f39064aab523d6edf493c388cfb7419bb5c9043\n",
            "Successfully built muon-optimizer\n",
            "Installing collected packages: muon-optimizer\n",
            "Successfully installed muon-optimizer-0.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "id": "pBxX3I1kYwojaamntSxiNZTK",
      "metadata": {
        "tags": [],
        "id": "pBxX3I1kYwojaamntSxiNZTK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb8ba929-f19b-4885-9fa7-e79ac9196d6b"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "import math\n",
        "import time\n",
        "\n",
        "from muon import SingleDeviceMuonWithAuxAdam\n",
        "\n",
        "# ==========================================\n",
        "# 1. Configuration (Tuned for ViT on CIFAR)\n",
        "# ==========================================\n",
        "REG_MODE = 'baseline'\n",
        "SIGR_ALPHA = 0.1   # Strength of the physics constraint\n",
        "SKETCH_DIM = 64    # Dimension of the random observer\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "LEARNING_RATE = 1e-2  # Slightly higher initial LR for AdamW with cosine schedule\n",
        "EPOCHS = 400\n",
        "WEIGHT_DECAY = 0.05\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "if torch.backends.mps.is_available(): DEVICE = 'mps'\n",
        "\n",
        "# Regularization Config\n",
        "MIXUP_ALPHA = 0.8\n",
        "CUTMIX_ALPHA = 1.0\n",
        "DROP_PATH_RATE = 0.1\n",
        "\n",
        "print(f\"Training on device: {DEVICE}\")\n",
        "\n",
        "# ==========================================\n",
        "# 2. Data Preparation (THE FIX: Strong Augmentation)\n",
        "# ==========================================\n",
        "def get_data_loaders():\n",
        "    print('==> Preparing data with Strong Augmentation...')\n",
        "\n",
        "    mean = (0.5071, 0.4867, 0.4408)\n",
        "    std = (0.2675, 0.2565, 0.2761)\n",
        "\n",
        "    # FIX 1: Add RandAugment\n",
        "    transform_train = transforms.Compose([\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandAugment(num_ops=2, magnitude=9), # <--- CRITICAL FOR ViT\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean, std),\n",
        "    ])\n",
        "\n",
        "    transform_test = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean, std),\n",
        "    ])\n",
        "\n",
        "    trainset = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=transform_train)\n",
        "    # Increase workers to handle augmentation load\n",
        "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
        "\n",
        "    testset = torchvision.datasets.CIFAR100(root='./data', train=False, download=True, transform=transform_test)\n",
        "    testloader = torch.utils.data.DataLoader(testset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
        "\n",
        "    return trainloader, testloader\n",
        "\n",
        "\n",
        "# ------------------------------------------\n",
        "# Physics Engine: The Regularizers\n",
        "# ------------------------------------------\n",
        "\n",
        "def sigreg_weak_loss(x, sketch_dim=64):\n",
        "    \"\"\"\n",
        "    Forces Covariance(x) ~ Identity.\n",
        "    Matches the 2nd Moment (Spherical Cloud).\n",
        "    \"\"\"\n",
        "    N, C = x.size()\n",
        "    # 1. Sketching (Optional for C=512, but good for consistency)\n",
        "    if C > sketch_dim:\n",
        "        S = torch.randn(sketch_dim, C, device=x.device) / (C ** 0.5)\n",
        "        x = x @ S.T  # [N, sketch_dim]\n",
        "    else:\n",
        "        sketch_dim = C\n",
        "\n",
        "    # 2. Centering & Covariance\n",
        "    x = x - x.mean(dim=0, keepdim=True)\n",
        "    cov = (x.T @ x) / (N - 1 + 1e-6)\n",
        "\n",
        "    # 3. Target Identity\n",
        "    target = torch.eye(sketch_dim, device=x.device)\n",
        "\n",
        "    # 4. Off-diagonal suppression + Diagonal maintenance\n",
        "    return torch.norm(cov - target, p='fro')\n",
        "\n",
        "def sigreg_strong_loss(x, sketch_dim=64):\n",
        "    \"\"\"\n",
        "    Forces ECF(x) ~ ECF(Gaussian).\n",
        "    Matches ALL Moments (Maximum Entropy Cloud).\n",
        "    Exact implementation of LeJEPA Algorithm 1.\n",
        "    \"\"\"\n",
        "    N, C = x.size()\n",
        "\n",
        "    # 1. Projection (The Observer)\n",
        "    # Project channels down to sketch_dim\n",
        "    A = torch.randn(C, sketch_dim, device=x.device)\n",
        "    A = A / (A.norm(p=2, dim=0, keepdim=True) + 1e-6)\n",
        "\n",
        "    # 2. Integration Points\n",
        "    t = torch.linspace(-5, 5, 17, device=x.device)\n",
        "\n",
        "    # 3. Theoretical Gaussian CF\n",
        "    exp_f = torch.exp(-0.5 * t**2)\n",
        "\n",
        "    # 4. Empirical CF\n",
        "    # proj: [N, sketch_dim]\n",
        "    proj = x @ A\n",
        "\n",
        "    # args: [N, sketch_dim, T]\n",
        "    args = proj.unsqueeze(2) * t.view(1, 1, -1)\n",
        "\n",
        "    # ecf: [sketch_dim, T] (Mean over batch)\n",
        "    ecf = torch.exp(1j * args).mean(dim=0)\n",
        "\n",
        "    # 5. Weighted L2 Distance\n",
        "    # |ecf - gauss|^2 * gauss_weight\n",
        "    diff_sq = (ecf - exp_f.unsqueeze(0)).abs().square()\n",
        "    err = diff_sq * exp_f.unsqueeze(0)\n",
        "\n",
        "    # 6. Integrate\n",
        "    loss = torch.trapz(err, t, dim=1) * N\n",
        "\n",
        "    return loss.mean()\n",
        "\n",
        "# ==========================================\n",
        "# 3. Mixup / CutMix Utilities\n",
        "# ==========================================\n",
        "def rand_bbox(size, lam):\n",
        "    W = size[2]\n",
        "    H = size[3]\n",
        "    cut_rat = np.sqrt(1. - lam)\n",
        "    cut_w = int(W * cut_rat)\n",
        "    cut_h = int(H * cut_rat)\n",
        "\n",
        "    cx = np.random.randint(W)\n",
        "    cy = np.random.randint(H)\n",
        "\n",
        "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
        "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
        "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
        "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
        "\n",
        "    return bbx1, bby1, bbx2, bby2\n",
        "\n",
        "def drop_path(x, drop_prob: float = 0., training: bool = False):\n",
        "    if drop_prob == 0. or not training: return x\n",
        "    keep_prob = 1 - drop_prob\n",
        "    shape = (x.shape[0],) + (1,) * (x.ndim - 1)\n",
        "    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
        "    random_tensor.floor_()\n",
        "    return x.div(keep_prob) * random_tensor\n",
        "\n",
        "class DropPath(nn.Module):\n",
        "    def __init__(self, drop_prob=None):\n",
        "        super(DropPath, self).__init__()\n",
        "        self.drop_prob = drop_prob\n",
        "    def forward(self, x): return drop_path(x, self.drop_prob, self.training)\n",
        "\n",
        "class PatchEmbedding(nn.Module):\n",
        "    def __init__(self, img_size=32, patch_size=4, in_chans=3, embed_dim=192):\n",
        "        super().__init__()\n",
        "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "    def forward(self, x):\n",
        "        x = self.proj(x).flatten(2).transpose(1, 2)\n",
        "        return x\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, dim, num_heads=8, qkv_bias=False, attn_drop=0., proj_drop=0.):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        head_dim = dim // num_heads\n",
        "        self.scale = head_dim ** -0.5\n",
        "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
        "        self.attn_drop = nn.Dropout(attn_drop)\n",
        "        self.proj = nn.Linear(dim, dim)\n",
        "        self.proj_drop = nn.Dropout(proj_drop)\n",
        "    def forward(self, x):\n",
        "        B, N, C = x.shape\n",
        "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        attn = self.attn_drop(attn)\n",
        "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
        "        x = self.proj(x)\n",
        "        x = self.proj_drop(x)\n",
        "        return x\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, dim, hidden_dim, drop=0.):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(dim, hidden_dim)\n",
        "        self.act = nn.GELU()\n",
        "        self.fc2 = nn.Linear(hidden_dim, dim)\n",
        "        self.drop = nn.Dropout(drop)\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.drop(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.drop(x)\n",
        "        return x\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, dim, num_heads, mlp_ratio=4., drop=0., attn_drop=0., drop_path=0., reg_mode='strong', sketch_dim=64):\n",
        "        super().__init__()\n",
        "        self.norm1 = nn.LayerNorm(dim)\n",
        "        self.attn = MultiHeadAttention(dim, num_heads=num_heads, qkv_bias=True, attn_drop=attn_drop, proj_drop=drop)\n",
        "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
        "        self.norm2 = nn.LayerNorm(dim)\n",
        "        self.mlp = MLP(dim, int(dim * mlp_ratio), drop=drop)\n",
        "\n",
        "        self.reg_mode = reg_mode\n",
        "        self.sketch_dim = sketch_dim\n",
        "    def forward(self, x):\n",
        "        x = x + self.drop_path(self.attn(self.norm1(x)))\n",
        "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
        "\n",
        "\n",
        "        # --- PHYSICS INJECTION ---\n",
        "        reg_loss = torch.tensor(0.0, device=x.device)\n",
        "        if self.reg_mode != 'baseline':\n",
        "            # Global Average Pool of the tokens [B, N, C] -> [B, C]\n",
        "            # This represents the \"Image Vector\" at this depth\n",
        "            flat_rep = x.mean(dim=1)\n",
        "\n",
        "            # Crucial: Pre-Norm vs Post-Norm context.\n",
        "            # LayerNorm forces variance=1. SIGReg forces Distribution=Gaussian.\n",
        "            # They are compatible.\n",
        "            if self.reg_mode == 'weak':\n",
        "                reg_loss = sigreg_weak_loss(flat_rep, self.sketch_dim)\n",
        "            elif self.reg_mode == 'strong':\n",
        "                reg_loss = sigreg_strong_loss(flat_rep, self.sketch_dim)\n",
        "\n",
        "        return x, reg_loss\n",
        "\n",
        "class VisionTransformer(nn.Module):\n",
        "    def __init__(self, img_size=32, patch_size=4, in_chans=3, num_classes=100, embed_dim=192, depth=9, num_heads=3, mlp_ratio=4., drop_rate=0., attn_drop_rate=0., drop_path_rate=0.1, reg_mode='strong', sketch_dim=64):\n",
        "        super().__init__()\n",
        "        self.patch_embed = PatchEmbedding(img_size, patch_size, in_chans, embed_dim)\n",
        "        num_patches = self.patch_embed.proj.weight.shape[2] # Just a hack to get patch count logic\n",
        "        num_patches = (img_size // patch_size) ** 2\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n",
        "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]\n",
        "        self.blocks = nn.ModuleList([\n",
        "            TransformerBlock(embed_dim, num_heads, mlp_ratio, drop_rate, attn_drop_rate, dpr[i], reg_mode, sketch_dim)\n",
        "            for i in range(depth)])\n",
        "        self.norm = nn.LayerNorm(embed_dim)\n",
        "        self.head = nn.Linear(embed_dim, num_classes)\n",
        "        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n",
        "        nn.init.trunc_normal_(self.cls_token, std=0.02)\n",
        "        nn.init.xavier_uniform_(self.head.weight)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B = x.shape[0]\n",
        "        x = self.patch_embed(x)\n",
        "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
        "        x = torch.cat((cls_tokens, x), dim=1)\n",
        "        x = x + self.pos_embed\n",
        "\n",
        "        total_phys_loss = 0.0\n",
        "\n",
        "        # Pass through blocks\n",
        "        for blk in self.blocks:\n",
        "            x, l_loss = blk(x, )\n",
        "            total_phys_loss += l_loss\n",
        "\n",
        "        x = self.norm(x)\n",
        "        out = self.head(x[:, 0])\n",
        "        return out, (total_phys_loss / len(self.blocks))\n",
        "\n",
        "# ==========================================\n",
        "# 5. Training Engine (Updated for Mixup/CutMix)\n",
        "# ==========================================\n",
        "def train(epoch, net, trainloader, optimizer, criterion):\n",
        "    net.train()\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    phys_loss_meter = 0\n",
        "\n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
        "\n",
        "        # Apply Mixup/CutMix\n",
        "        r = np.random.rand(1)\n",
        "        if r < 0.5: # Mixup\n",
        "            lam = np.random.beta(MIXUP_ALPHA, MIXUP_ALPHA)\n",
        "            index = torch.randperm(inputs.size(0)).to(DEVICE)\n",
        "            inputs = lam * inputs + (1 - lam) * inputs[index, :]\n",
        "            targets_a, targets_b = targets, targets[index]\n",
        "        else: # CutMix\n",
        "            lam = np.random.beta(CUTMIX_ALPHA, CUTMIX_ALPHA)\n",
        "            rand_index = torch.randperm(inputs.size(0)).to(DEVICE)\n",
        "            target_a = targets\n",
        "            target_b = targets[rand_index]\n",
        "            bbx1, bby1, bbx2, bby2 = rand_bbox(inputs.size(), lam)\n",
        "            inputs[:, :, bbx1:bbx2, bby1:bby2] = inputs[rand_index, :, bbx1:bbx2, bby1:bby2]\n",
        "            lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (inputs.size()[-1] * inputs.size()[-2]))\n",
        "            targets_a, targets_b = target_a, target_b\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward\n",
        "        outputs, p_loss = net(inputs)\n",
        "\n",
        "        # Task Loss\n",
        "        c_loss = criterion(outputs, targets_a) * lam + criterion(outputs, targets_b) * (1. - lam)\n",
        "\n",
        "        # Total Loss\n",
        "        loss = (1 - SIGR_ALPHA) * c_loss + (SIGR_ALPHA * p_loss)\n",
        "\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(net.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += ((1 - SIGR_ALPHA) * c_loss).item() # Log only task loss for comparison\n",
        "        phys_loss_meter += (SIGR_ALPHA * p_loss).item()\n",
        "\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += (lam * predicted.eq(targets_a).float() + (1 - lam) * predicted.eq(targets_b).float()).sum().item()\n",
        "\n",
        "    acc = 100. * correct / total\n",
        "    return train_loss / (batch_idx + 1), acc, phys_loss_meter / (batch_idx + 1)\n",
        "\n",
        "def test(epoch, net, testloader, criterion):\n",
        "    net.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "            inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
        "            outputs, _ = net(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "    acc = 100. * correct / total\n",
        "    return test_loss / (batch_idx + 1), acc\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    trainloader, testloader = get_data_loaders()\n",
        "\n",
        "    net = VisionTransformer(img_size=32, patch_size=4, embed_dim=192, depth=9, num_heads=3, drop_path_rate=0.1, reg_mode=REG_MODE, sketch_dim=SKETCH_DIM)\n",
        "    net = net.to(DEVICE)\n",
        "\n",
        "    hidden_weights = [p for p in net.parameters() if p.ndim == 2]\n",
        "    non_hidden_params = [p for p in net.parameters() if p.ndim != 2]\n",
        "\n",
        "    param_groups = [\n",
        "      dict(params=hidden_weights, use_muon=True, lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY),\n",
        "      dict(params=non_hidden_params, use_muon=False, lr=1e-3, betas=(0.9, 0.95), weight_decay=WEIGHT_DECAY)\n",
        "    ]\n",
        "\n",
        "    # Standard CrossEntropy for final eval, SoftLabel for training is handled by Mixup logic\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    optimizer = SingleDeviceMuonWithAuxAdam(param_groups)\n",
        "\n",
        "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS, eta_min=1e-5)\n",
        "\n",
        "    print(f\"Starting training for {EPOCHS} epochs with RandAugment + Mixup/CutMix...\")\n",
        "\n",
        "    best_acc = 0\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        start_time = time.time()\n",
        "\n",
        "        train_loss, train_acc, physics_loss = train(epoch, net, trainloader, optimizer, criterion)\n",
        "        test_loss, test_acc = test(epoch, net, testloader, criterion)\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        if test_acc > best_acc:\n",
        "            best_acc = test_acc\n",
        "            # torch.save(net.state_dict(), f'thermo_resnet_{REG_MODE}.pth')\n",
        "\n",
        "        epoch_time = time.time() - start_time\n",
        "\n",
        "        print(f\"Epoch {epoch+1} | T: {epoch_time:.0f}s | \"\n",
        "              f\"Train: {train_loss:.4f} ({train_acc:.1f}%) | \"\n",
        "              f\"Phys: {physics_loss:.2f} | \"\n",
        "              f\"Val: {test_loss:.4f} ({test_acc:.2f}%) | \"\n",
        "              f\"Best: {best_acc:.2f}%\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on device: cuda\n",
            "==> Preparing data with Strong Augmentation...\n",
            "Starting training for 400 epochs with RandAugment + Mixup/CutMix...\n",
            "Epoch 1 | T: 37s | Train: 3.8585 (6.1%) | Phys: 0.00 | Val: 3.6131 (13.88%) | Best: 13.88%\n",
            "Epoch 2 | T: 39s | Train: 3.6574 (9.2%) | Phys: 0.00 | Val: 3.2959 (20.19%) | Best: 20.19%\n",
            "Epoch 3 | T: 37s | Train: 3.5778 (11.0%) | Phys: 0.00 | Val: 3.1777 (22.81%) | Best: 22.81%\n",
            "Epoch 4 | T: 37s | Train: 3.5181 (12.4%) | Phys: 0.00 | Val: 3.0726 (24.68%) | Best: 24.68%\n",
            "Epoch 5 | T: 37s | Train: 3.4263 (14.0%) | Phys: 0.00 | Val: 2.9215 (29.43%) | Best: 29.43%\n",
            "Epoch 6 | T: 37s | Train: 3.3413 (16.1%) | Phys: 0.00 | Val: 2.7626 (31.64%) | Best: 31.64%\n",
            "Epoch 7 | T: 37s | Train: 3.2502 (17.9%) | Phys: 0.00 | Val: 2.5451 (36.59%) | Best: 36.59%\n",
            "Epoch 8 | T: 37s | Train: 3.1865 (19.5%) | Phys: 0.00 | Val: 2.3575 (40.44%) | Best: 40.44%\n",
            "Epoch 9 | T: 37s | Train: 3.1738 (20.0%) | Phys: 0.00 | Val: 2.5585 (37.12%) | Best: 40.44%\n",
            "Epoch 10 | T: 37s | Train: 3.1352 (21.0%) | Phys: 0.00 | Val: 2.3973 (39.91%) | Best: 40.44%\n",
            "Epoch 11 | T: 36s | Train: 3.1041 (21.8%) | Phys: 0.00 | Val: 2.3727 (39.52%) | Best: 40.44%\n",
            "Epoch 12 | T: 37s | Train: 3.0767 (22.4%) | Phys: 0.00 | Val: 2.2905 (41.91%) | Best: 41.91%\n",
            "Epoch 13 | T: 37s | Train: 3.0365 (23.3%) | Phys: 0.00 | Val: 2.1943 (43.88%) | Best: 43.88%\n",
            "Epoch 14 | T: 37s | Train: 3.0431 (23.3%) | Phys: 0.00 | Val: 2.1250 (44.72%) | Best: 44.72%\n",
            "Epoch 15 | T: 37s | Train: 3.0141 (23.8%) | Phys: 0.00 | Val: 2.2697 (42.53%) | Best: 44.72%\n",
            "Epoch 16 | T: 37s | Train: 2.9962 (24.5%) | Phys: 0.00 | Val: 2.1926 (44.75%) | Best: 44.75%\n",
            "Epoch 17 | T: 37s | Train: 3.0084 (24.4%) | Phys: 0.00 | Val: 2.0937 (45.74%) | Best: 45.74%\n",
            "Epoch 18 | T: 36s | Train: 2.9584 (25.4%) | Phys: 0.00 | Val: 2.0960 (45.33%) | Best: 45.74%\n",
            "Epoch 19 | T: 37s | Train: 2.9279 (25.9%) | Phys: 0.00 | Val: 2.1247 (45.57%) | Best: 45.74%\n",
            "Epoch 20 | T: 38s | Train: 2.9806 (24.8%) | Phys: 0.00 | Val: 2.1247 (45.27%) | Best: 45.74%\n",
            "Epoch 21 | T: 38s | Train: 2.9570 (25.4%) | Phys: 0.00 | Val: 2.1575 (45.81%) | Best: 45.81%\n",
            "Epoch 22 | T: 37s | Train: 2.9285 (26.1%) | Phys: 0.00 | Val: 2.0747 (46.35%) | Best: 46.35%\n",
            "Epoch 23 | T: 37s | Train: 2.9789 (25.0%) | Phys: 0.00 | Val: 2.0977 (47.22%) | Best: 47.22%\n",
            "Epoch 24 | T: 36s | Train: 2.9588 (25.3%) | Phys: 0.00 | Val: 2.0386 (47.14%) | Best: 47.22%\n",
            "Epoch 25 | T: 37s | Train: 2.9565 (25.5%) | Phys: 0.00 | Val: 2.0445 (47.47%) | Best: 47.47%\n",
            "Epoch 26 | T: 37s | Train: 2.9474 (25.6%) | Phys: 0.00 | Val: 2.0832 (47.55%) | Best: 47.55%\n",
            "Epoch 27 | T: 37s | Train: 2.9270 (26.2%) | Phys: 0.00 | Val: 2.1230 (46.36%) | Best: 47.55%\n",
            "Epoch 28 | T: 37s | Train: 2.9063 (26.5%) | Phys: 0.00 | Val: 2.0228 (47.76%) | Best: 47.76%\n",
            "Epoch 29 | T: 37s | Train: 2.8926 (26.9%) | Phys: 0.00 | Val: 2.0289 (47.92%) | Best: 47.92%\n",
            "Epoch 30 | T: 37s | Train: 2.9193 (26.4%) | Phys: 0.00 | Val: 1.9402 (50.12%) | Best: 50.12%\n",
            "Epoch 31 | T: 36s | Train: 2.8978 (26.8%) | Phys: 0.00 | Val: 1.9415 (49.74%) | Best: 50.12%\n",
            "Epoch 32 | T: 37s | Train: 2.8782 (27.3%) | Phys: 0.00 | Val: 1.9941 (48.70%) | Best: 50.12%\n",
            "Epoch 33 | T: 37s | Train: 2.9164 (26.7%) | Phys: 0.00 | Val: 1.9807 (49.44%) | Best: 50.12%\n",
            "Epoch 34 | T: 37s | Train: 2.8980 (26.9%) | Phys: 0.00 | Val: 2.0369 (47.74%) | Best: 50.12%\n",
            "Epoch 35 | T: 37s | Train: 2.8803 (27.3%) | Phys: 0.00 | Val: 2.0138 (49.42%) | Best: 50.12%\n",
            "Epoch 36 | T: 37s | Train: 2.8596 (27.9%) | Phys: 0.00 | Val: 1.9346 (51.28%) | Best: 51.28%\n",
            "Epoch 37 | T: 37s | Train: 2.8420 (28.4%) | Phys: 0.00 | Val: 1.8958 (50.76%) | Best: 51.28%\n",
            "Epoch 38 | T: 37s | Train: 2.8467 (28.1%) | Phys: 0.00 | Val: 1.9069 (51.31%) | Best: 51.31%\n",
            "Epoch 39 | T: 37s | Train: 2.8527 (28.1%) | Phys: 0.00 | Val: 1.9375 (50.68%) | Best: 51.31%\n",
            "Epoch 40 | T: 37s | Train: 2.8070 (29.1%) | Phys: 0.00 | Val: 1.8913 (51.74%) | Best: 51.74%\n",
            "Epoch 41 | T: 37s | Train: 2.8110 (29.1%) | Phys: 0.00 | Val: 1.9181 (50.54%) | Best: 51.74%\n",
            "Epoch 42 | T: 37s | Train: 2.8501 (28.2%) | Phys: 0.00 | Val: 1.9005 (50.79%) | Best: 51.74%\n",
            "Epoch 43 | T: 37s | Train: 2.8549 (27.7%) | Phys: 0.00 | Val: 1.9889 (49.26%) | Best: 51.74%\n",
            "Epoch 44 | T: 37s | Train: 2.8345 (28.4%) | Phys: 0.00 | Val: 1.9162 (51.17%) | Best: 51.74%\n",
            "Epoch 45 | T: 37s | Train: 2.8692 (27.6%) | Phys: 0.00 | Val: 1.8924 (51.54%) | Best: 51.74%\n",
            "Epoch 46 | T: 37s | Train: 2.8377 (28.3%) | Phys: 0.00 | Val: 1.9046 (50.59%) | Best: 51.74%\n",
            "Epoch 47 | T: 37s | Train: 2.7692 (30.0%) | Phys: 0.00 | Val: 1.9109 (51.52%) | Best: 51.74%\n",
            "Epoch 48 | T: 37s | Train: 2.8778 (27.9%) | Phys: 0.00 | Val: 1.9818 (49.98%) | Best: 51.74%\n",
            "Epoch 49 | T: 37s | Train: 2.8433 (28.5%) | Phys: 0.00 | Val: 1.9494 (49.92%) | Best: 51.74%\n",
            "Epoch 50 | T: 37s | Train: 2.7973 (29.3%) | Phys: 0.00 | Val: 1.8369 (52.45%) | Best: 52.45%\n",
            "Epoch 51 | T: 37s | Train: 2.8295 (28.6%) | Phys: 0.00 | Val: 1.8847 (51.38%) | Best: 52.45%\n",
            "Epoch 52 | T: 37s | Train: 2.8260 (28.9%) | Phys: 0.00 | Val: 2.0026 (49.61%) | Best: 52.45%\n",
            "Epoch 53 | T: 37s | Train: 2.8278 (28.6%) | Phys: 0.00 | Val: 1.8309 (52.42%) | Best: 52.45%\n",
            "Epoch 54 | T: 36s | Train: 2.8149 (28.9%) | Phys: 0.00 | Val: 1.8370 (51.66%) | Best: 52.45%\n",
            "Epoch 55 | T: 37s | Train: 2.7884 (29.5%) | Phys: 0.00 | Val: 2.0057 (49.17%) | Best: 52.45%\n",
            "Epoch 56 | T: 37s | Train: 2.8016 (29.2%) | Phys: 0.00 | Val: 1.8368 (52.06%) | Best: 52.45%\n",
            "Epoch 57 | T: 37s | Train: 2.8354 (28.6%) | Phys: 0.00 | Val: 1.8426 (52.43%) | Best: 52.45%\n",
            "Epoch 58 | T: 36s | Train: 2.8368 (28.5%) | Phys: 0.00 | Val: 1.8765 (52.40%) | Best: 52.45%\n",
            "Epoch 59 | T: 37s | Train: 2.8441 (28.7%) | Phys: 0.00 | Val: 1.9184 (50.80%) | Best: 52.45%\n",
            "Epoch 60 | T: 37s | Train: 2.8602 (28.3%) | Phys: 0.00 | Val: 1.9334 (50.89%) | Best: 52.45%\n",
            "Epoch 61 | T: 37s | Train: 2.8320 (28.9%) | Phys: 0.00 | Val: 1.8882 (52.36%) | Best: 52.45%\n",
            "Epoch 62 | T: 36s | Train: 2.7765 (30.1%) | Phys: 0.00 | Val: 1.8086 (53.16%) | Best: 53.16%\n",
            "Epoch 63 | T: 37s | Train: 2.7820 (30.0%) | Phys: 0.00 | Val: 1.8233 (53.20%) | Best: 53.20%\n",
            "Epoch 64 | T: 37s | Train: 2.7790 (29.6%) | Phys: 0.00 | Val: 1.8817 (52.06%) | Best: 53.20%\n",
            "Epoch 65 | T: 37s | Train: 2.7969 (29.4%) | Phys: 0.00 | Val: 1.8935 (51.32%) | Best: 53.20%\n",
            "Epoch 66 | T: 37s | Train: 2.7685 (29.9%) | Phys: 0.00 | Val: 1.8927 (51.45%) | Best: 53.20%\n",
            "Epoch 67 | T: 36s | Train: 2.8084 (29.5%) | Phys: 0.00 | Val: 1.8954 (51.69%) | Best: 53.20%\n",
            "Epoch 68 | T: 37s | Train: 2.8181 (29.1%) | Phys: 0.00 | Val: 1.9154 (50.52%) | Best: 53.20%\n",
            "Epoch 69 | T: 37s | Train: 2.7883 (29.8%) | Phys: 0.00 | Val: 1.8530 (52.25%) | Best: 53.20%\n",
            "Epoch 70 | T: 37s | Train: 2.7979 (29.8%) | Phys: 0.00 | Val: 1.8410 (52.60%) | Best: 53.20%\n",
            "Epoch 71 | T: 37s | Train: 2.7893 (29.6%) | Phys: 0.00 | Val: 1.8146 (53.35%) | Best: 53.35%\n",
            "Epoch 72 | T: 36s | Train: 2.7459 (30.7%) | Phys: 0.00 | Val: 1.8249 (52.39%) | Best: 53.35%\n",
            "Epoch 73 | T: 37s | Train: 2.7435 (30.9%) | Phys: 0.00 | Val: 1.8513 (52.13%) | Best: 53.35%\n",
            "Epoch 74 | T: 37s | Train: 2.7542 (30.5%) | Phys: 0.00 | Val: 1.7861 (53.77%) | Best: 53.77%\n",
            "Epoch 75 | T: 36s | Train: 2.7537 (30.7%) | Phys: 0.00 | Val: 1.8601 (52.13%) | Best: 53.77%\n",
            "Epoch 76 | T: 37s | Train: 2.7633 (30.4%) | Phys: 0.00 | Val: 1.8472 (53.06%) | Best: 53.77%\n",
            "Epoch 77 | T: 37s | Train: 2.8193 (29.2%) | Phys: 0.00 | Val: 1.7716 (53.68%) | Best: 53.77%\n",
            "Epoch 78 | T: 37s | Train: 2.7737 (30.3%) | Phys: 0.00 | Val: 1.8951 (51.48%) | Best: 53.77%\n",
            "Epoch 79 | T: 37s | Train: 2.7437 (30.9%) | Phys: 0.00 | Val: 1.8073 (53.27%) | Best: 53.77%\n",
            "Epoch 80 | T: 37s | Train: 2.7019 (31.8%) | Phys: 0.00 | Val: 1.7781 (53.51%) | Best: 53.77%\n",
            "Epoch 81 | T: 37s | Train: 2.7878 (30.1%) | Phys: 0.00 | Val: 1.8537 (52.87%) | Best: 53.77%\n",
            "Epoch 82 | T: 37s | Train: 2.8146 (29.3%) | Phys: 0.00 | Val: 1.8404 (51.97%) | Best: 53.77%\n",
            "Epoch 83 | T: 36s | Train: 2.7893 (29.7%) | Phys: 0.00 | Val: 1.8156 (53.08%) | Best: 53.77%\n",
            "Epoch 84 | T: 37s | Train: 2.7880 (30.1%) | Phys: 0.00 | Val: 1.7837 (54.23%) | Best: 54.23%\n",
            "Epoch 85 | T: 37s | Train: 2.7537 (30.6%) | Phys: 0.00 | Val: 1.8161 (53.61%) | Best: 54.23%\n",
            "Epoch 86 | T: 37s | Train: 2.7722 (30.3%) | Phys: 0.00 | Val: 1.7776 (54.60%) | Best: 54.60%\n",
            "Epoch 87 | T: 37s | Train: 2.7724 (30.1%) | Phys: 0.00 | Val: 1.7891 (54.93%) | Best: 54.93%\n",
            "Epoch 88 | T: 37s | Train: 2.7244 (31.4%) | Phys: 0.00 | Val: 1.7843 (54.50%) | Best: 54.93%\n",
            "Epoch 89 | T: 37s | Train: 2.7737 (30.2%) | Phys: 0.00 | Val: 1.7606 (54.25%) | Best: 54.93%\n",
            "Epoch 90 | T: 37s | Train: 2.7727 (30.4%) | Phys: 0.00 | Val: 1.7712 (53.45%) | Best: 54.93%\n",
            "Epoch 91 | T: 37s | Train: 2.7813 (30.1%) | Phys: 0.00 | Val: 1.8338 (52.52%) | Best: 54.93%\n",
            "Epoch 92 | T: 37s | Train: 2.7300 (31.0%) | Phys: 0.00 | Val: 1.7126 (55.27%) | Best: 55.27%\n",
            "Epoch 93 | T: 37s | Train: 2.7760 (30.1%) | Phys: 0.00 | Val: 1.8231 (52.85%) | Best: 55.27%\n",
            "Epoch 94 | T: 37s | Train: 2.8073 (29.7%) | Phys: 0.00 | Val: 1.7806 (54.35%) | Best: 55.27%\n",
            "Epoch 95 | T: 37s | Train: 2.7087 (31.7%) | Phys: 0.00 | Val: 1.7697 (53.79%) | Best: 55.27%\n",
            "Epoch 96 | T: 37s | Train: 2.7623 (30.3%) | Phys: 0.00 | Val: 1.7639 (54.60%) | Best: 55.27%\n",
            "Epoch 97 | T: 36s | Train: 2.7534 (30.6%) | Phys: 0.00 | Val: 1.7884 (54.00%) | Best: 55.27%\n",
            "Epoch 98 | T: 37s | Train: 2.7348 (31.3%) | Phys: 0.00 | Val: 1.8078 (53.39%) | Best: 55.27%\n",
            "Epoch 99 | T: 37s | Train: 2.7614 (30.6%) | Phys: 0.00 | Val: 1.7847 (53.93%) | Best: 55.27%\n",
            "Epoch 100 | T: 37s | Train: 2.7433 (31.0%) | Phys: 0.00 | Val: 1.7407 (54.83%) | Best: 55.27%\n",
            "Epoch 101 | T: 37s | Train: 2.7343 (31.1%) | Phys: 0.00 | Val: 1.8016 (53.69%) | Best: 55.27%\n",
            "Epoch 102 | T: 37s | Train: 2.7368 (31.0%) | Phys: 0.00 | Val: 1.7673 (53.81%) | Best: 55.27%\n",
            "Epoch 103 | T: 37s | Train: 2.7262 (31.2%) | Phys: 0.00 | Val: 1.7135 (55.57%) | Best: 55.57%\n",
            "Epoch 104 | T: 37s | Train: 2.7275 (31.4%) | Phys: 0.00 | Val: 1.7186 (55.04%) | Best: 55.57%\n",
            "Epoch 105 | T: 36s | Train: 2.7367 (31.3%) | Phys: 0.00 | Val: 1.7789 (54.63%) | Best: 55.57%\n",
            "Epoch 106 | T: 37s | Train: 2.7051 (31.8%) | Phys: 0.00 | Val: 1.7619 (54.34%) | Best: 55.57%\n",
            "Epoch 107 | T: 37s | Train: 2.7432 (31.3%) | Phys: 0.00 | Val: 1.7701 (54.76%) | Best: 55.57%\n",
            "Epoch 108 | T: 36s | Train: 2.7253 (31.4%) | Phys: 0.00 | Val: 1.7154 (55.42%) | Best: 55.57%\n",
            "Epoch 109 | T: 37s | Train: 2.7251 (31.4%) | Phys: 0.00 | Val: 1.8021 (53.94%) | Best: 55.57%\n",
            "Epoch 110 | T: 37s | Train: 2.7161 (31.7%) | Phys: 0.00 | Val: 1.7242 (55.91%) | Best: 55.91%\n",
            "Epoch 111 | T: 36s | Train: 2.7358 (31.1%) | Phys: 0.00 | Val: 1.6909 (55.97%) | Best: 55.97%\n",
            "Epoch 112 | T: 37s | Train: 2.7119 (31.9%) | Phys: 0.00 | Val: 1.7835 (54.39%) | Best: 55.97%\n",
            "Epoch 113 | T: 37s | Train: 2.7468 (31.1%) | Phys: 0.00 | Val: 1.7981 (53.77%) | Best: 55.97%\n",
            "Epoch 114 | T: 36s | Train: 2.7185 (31.8%) | Phys: 0.00 | Val: 1.7293 (56.18%) | Best: 56.18%\n",
            "Epoch 115 | T: 37s | Train: 2.7084 (31.6%) | Phys: 0.00 | Val: 1.7733 (54.24%) | Best: 56.18%\n",
            "Epoch 116 | T: 36s | Train: 2.7181 (31.7%) | Phys: 0.00 | Val: 1.7040 (55.38%) | Best: 56.18%\n",
            "Epoch 117 | T: 37s | Train: 2.7147 (31.8%) | Phys: 0.00 | Val: 1.7421 (54.59%) | Best: 56.18%\n",
            "Epoch 118 | T: 37s | Train: 2.7113 (31.7%) | Phys: 0.00 | Val: 1.7880 (54.23%) | Best: 56.18%\n",
            "Epoch 119 | T: 37s | Train: 2.7012 (31.9%) | Phys: 0.00 | Val: 1.7841 (53.80%) | Best: 56.18%\n",
            "Epoch 120 | T: 37s | Train: 2.6907 (32.1%) | Phys: 0.00 | Val: 1.7463 (54.64%) | Best: 56.18%\n",
            "Epoch 121 | T: 37s | Train: 2.6676 (33.0%) | Phys: 0.00 | Val: 1.7446 (54.50%) | Best: 56.18%\n",
            "Epoch 122 | T: 37s | Train: 2.6709 (32.8%) | Phys: 0.00 | Val: 1.7163 (55.58%) | Best: 56.18%\n",
            "Epoch 123 | T: 37s | Train: 2.7085 (32.0%) | Phys: 0.00 | Val: 1.7085 (55.98%) | Best: 56.18%\n",
            "Epoch 124 | T: 37s | Train: 2.6950 (32.0%) | Phys: 0.00 | Val: 1.7671 (54.56%) | Best: 56.18%\n",
            "Epoch 125 | T: 37s | Train: 2.6536 (33.3%) | Phys: 0.00 | Val: 1.7573 (54.22%) | Best: 56.18%\n",
            "Epoch 126 | T: 37s | Train: 2.7171 (31.6%) | Phys: 0.00 | Val: 1.6856 (56.52%) | Best: 56.52%\n",
            "Epoch 127 | T: 36s | Train: 2.6785 (32.4%) | Phys: 0.00 | Val: 1.6869 (55.53%) | Best: 56.52%\n",
            "Epoch 128 | T: 37s | Train: 2.6671 (32.9%) | Phys: 0.00 | Val: 1.6812 (56.25%) | Best: 56.52%\n",
            "Epoch 129 | T: 37s | Train: 2.6598 (33.0%) | Phys: 0.00 | Val: 1.6840 (55.45%) | Best: 56.52%\n",
            "Epoch 130 | T: 37s | Train: 2.6634 (32.8%) | Phys: 0.00 | Val: 1.6858 (55.89%) | Best: 56.52%\n",
            "Epoch 131 | T: 37s | Train: 2.6642 (33.4%) | Phys: 0.00 | Val: 1.6764 (55.86%) | Best: 56.52%\n",
            "Epoch 132 | T: 37s | Train: 2.6912 (32.3%) | Phys: 0.00 | Val: 1.6926 (56.19%) | Best: 56.52%\n",
            "Epoch 133 | T: 37s | Train: 2.7361 (31.2%) | Phys: 0.00 | Val: 1.6869 (56.04%) | Best: 56.52%\n",
            "Epoch 134 | T: 37s | Train: 2.6771 (32.6%) | Phys: 0.00 | Val: 1.6377 (57.45%) | Best: 57.45%\n",
            "Epoch 135 | T: 37s | Train: 2.6754 (33.0%) | Phys: 0.00 | Val: 1.7048 (56.41%) | Best: 57.45%\n",
            "Epoch 136 | T: 37s | Train: 2.6610 (33.0%) | Phys: 0.00 | Val: 1.6726 (56.33%) | Best: 57.45%\n",
            "Epoch 137 | T: 37s | Train: 2.7013 (31.9%) | Phys: 0.00 | Val: 1.6742 (56.77%) | Best: 57.45%\n",
            "Epoch 138 | T: 37s | Train: 2.6746 (32.8%) | Phys: 0.00 | Val: 1.6975 (56.57%) | Best: 57.45%\n",
            "Epoch 139 | T: 37s | Train: 2.7250 (31.3%) | Phys: 0.00 | Val: 1.6877 (56.21%) | Best: 57.45%\n",
            "Epoch 140 | T: 36s | Train: 2.6762 (32.7%) | Phys: 0.00 | Val: 1.6534 (56.44%) | Best: 57.45%\n",
            "Epoch 141 | T: 37s | Train: 2.6317 (33.9%) | Phys: 0.00 | Val: 1.6522 (57.34%) | Best: 57.45%\n",
            "Epoch 142 | T: 37s | Train: 2.6454 (33.3%) | Phys: 0.00 | Val: 1.6676 (56.64%) | Best: 57.45%\n",
            "Epoch 143 | T: 37s | Train: 2.6335 (34.2%) | Phys: 0.00 | Val: 1.6259 (57.59%) | Best: 57.59%\n",
            "Epoch 144 | T: 37s | Train: 2.6984 (32.1%) | Phys: 0.00 | Val: 1.6688 (56.78%) | Best: 57.59%\n",
            "Epoch 145 | T: 37s | Train: 2.6417 (33.8%) | Phys: 0.00 | Val: 1.6643 (57.57%) | Best: 57.59%\n",
            "Epoch 146 | T: 37s | Train: 2.6760 (32.8%) | Phys: 0.00 | Val: 1.6816 (57.14%) | Best: 57.59%\n",
            "Epoch 147 | T: 36s | Train: 2.6515 (33.5%) | Phys: 0.00 | Val: 1.5950 (58.33%) | Best: 58.33%\n",
            "Epoch 148 | T: 36s | Train: 2.6201 (34.3%) | Phys: 0.00 | Val: 1.6808 (57.26%) | Best: 58.33%\n",
            "Epoch 149 | T: 37s | Train: 2.6554 (33.4%) | Phys: 0.00 | Val: 1.7341 (56.50%) | Best: 58.33%\n",
            "Epoch 150 | T: 37s | Train: 2.6475 (33.5%) | Phys: 0.00 | Val: 1.5915 (58.24%) | Best: 58.33%\n",
            "Epoch 151 | T: 36s | Train: 2.6564 (33.3%) | Phys: 0.00 | Val: 1.6563 (56.75%) | Best: 58.33%\n",
            "Epoch 152 | T: 37s | Train: 2.6722 (32.8%) | Phys: 0.00 | Val: 1.6413 (57.44%) | Best: 58.33%\n",
            "Epoch 153 | T: 37s | Train: 2.6526 (33.7%) | Phys: 0.00 | Val: 1.6198 (57.75%) | Best: 58.33%\n",
            "Epoch 154 | T: 37s | Train: 2.6433 (33.6%) | Phys: 0.00 | Val: 1.6698 (56.83%) | Best: 58.33%\n",
            "Epoch 155 | T: 37s | Train: 2.6393 (33.7%) | Phys: 0.00 | Val: 1.6434 (58.23%) | Best: 58.33%\n",
            "Epoch 156 | T: 37s | Train: 2.6369 (33.8%) | Phys: 0.00 | Val: 1.6425 (57.77%) | Best: 58.33%\n",
            "Epoch 157 | T: 37s | Train: 2.6297 (33.9%) | Phys: 0.00 | Val: 1.6173 (58.24%) | Best: 58.33%\n",
            "Epoch 158 | T: 37s | Train: 2.6402 (33.8%) | Phys: 0.00 | Val: 1.6091 (58.09%) | Best: 58.33%\n",
            "Epoch 159 | T: 37s | Train: 2.6336 (33.9%) | Phys: 0.00 | Val: 1.6290 (57.92%) | Best: 58.33%\n",
            "Epoch 160 | T: 37s | Train: 2.6165 (34.4%) | Phys: 0.00 | Val: 1.6497 (57.77%) | Best: 58.33%\n",
            "Epoch 161 | T: 37s | Train: 2.6037 (34.7%) | Phys: 0.00 | Val: 1.5752 (58.96%) | Best: 58.96%\n",
            "Epoch 162 | T: 37s | Train: 2.6607 (33.4%) | Phys: 0.00 | Val: 1.5834 (58.49%) | Best: 58.96%\n",
            "Epoch 163 | T: 37s | Train: 2.6391 (33.4%) | Phys: 0.00 | Val: 1.6462 (58.19%) | Best: 58.96%\n",
            "Epoch 164 | T: 37s | Train: 2.6282 (34.0%) | Phys: 0.00 | Val: 1.5721 (59.46%) | Best: 59.46%\n",
            "Epoch 165 | T: 37s | Train: 2.5756 (35.3%) | Phys: 0.00 | Val: 1.6344 (58.22%) | Best: 59.46%\n",
            "Epoch 166 | T: 37s | Train: 2.5529 (36.0%) | Phys: 0.00 | Val: 1.5870 (58.16%) | Best: 59.46%\n",
            "Epoch 167 | T: 37s | Train: 2.5787 (35.4%) | Phys: 0.00 | Val: 1.5938 (58.61%) | Best: 59.46%\n",
            "Epoch 168 | T: 37s | Train: 2.5813 (35.3%) | Phys: 0.00 | Val: 1.6103 (58.44%) | Best: 59.46%\n",
            "Epoch 169 | T: 37s | Train: 2.6193 (34.6%) | Phys: 0.00 | Val: 1.5963 (58.57%) | Best: 59.46%\n",
            "Epoch 170 | T: 37s | Train: 2.5965 (34.8%) | Phys: 0.00 | Val: 1.5575 (59.36%) | Best: 59.46%\n",
            "Epoch 171 | T: 37s | Train: 2.5935 (35.0%) | Phys: 0.00 | Val: 1.6496 (57.84%) | Best: 59.46%\n",
            "Epoch 172 | T: 37s | Train: 2.5896 (35.4%) | Phys: 0.00 | Val: 1.5822 (58.49%) | Best: 59.46%\n",
            "Epoch 173 | T: 38s | Train: 2.6018 (34.9%) | Phys: 0.00 | Val: 1.5984 (59.39%) | Best: 59.46%\n",
            "Epoch 174 | T: 38s | Train: 2.5826 (35.1%) | Phys: 0.00 | Val: 1.5234 (59.47%) | Best: 59.47%\n",
            "Epoch 175 | T: 37s | Train: 2.6343 (34.0%) | Phys: 0.00 | Val: 1.5757 (59.24%) | Best: 59.47%\n",
            "Epoch 176 | T: 38s | Train: 2.5796 (35.4%) | Phys: 0.00 | Val: 1.5609 (60.18%) | Best: 60.18%\n",
            "Epoch 177 | T: 37s | Train: 2.5771 (35.3%) | Phys: 0.00 | Val: 1.5416 (60.13%) | Best: 60.18%\n",
            "Epoch 178 | T: 37s | Train: 2.6086 (34.8%) | Phys: 0.00 | Val: 1.5211 (60.75%) | Best: 60.75%\n",
            "Epoch 179 | T: 37s | Train: 2.5833 (35.2%) | Phys: 0.00 | Val: 1.5453 (59.80%) | Best: 60.75%\n",
            "Epoch 180 | T: 36s | Train: 2.5849 (35.2%) | Phys: 0.00 | Val: 1.6041 (58.57%) | Best: 60.75%\n",
            "Epoch 181 | T: 37s | Train: 2.5885 (35.2%) | Phys: 0.00 | Val: 1.5054 (60.65%) | Best: 60.75%\n",
            "Epoch 182 | T: 37s | Train: 2.5700 (36.1%) | Phys: 0.00 | Val: 1.5378 (59.61%) | Best: 60.75%\n",
            "Epoch 183 | T: 37s | Train: 2.5479 (36.0%) | Phys: 0.00 | Val: 1.5022 (60.29%) | Best: 60.75%\n",
            "Epoch 184 | T: 36s | Train: 2.5870 (35.1%) | Phys: 0.00 | Val: 1.5132 (60.43%) | Best: 60.75%\n",
            "Epoch 185 | T: 36s | Train: 2.5953 (35.0%) | Phys: 0.00 | Val: 1.5227 (60.11%) | Best: 60.75%\n",
            "Epoch 186 | T: 37s | Train: 2.5791 (35.3%) | Phys: 0.00 | Val: 1.5460 (60.05%) | Best: 60.75%\n",
            "Epoch 187 | T: 36s | Train: 2.5533 (36.0%) | Phys: 0.00 | Val: 1.5414 (60.07%) | Best: 60.75%\n",
            "Epoch 188 | T: 37s | Train: 2.5394 (36.4%) | Phys: 0.00 | Val: 1.4756 (61.69%) | Best: 61.69%\n",
            "Epoch 189 | T: 37s | Train: 2.5552 (36.1%) | Phys: 0.00 | Val: 1.5200 (60.35%) | Best: 61.69%\n",
            "Epoch 190 | T: 37s | Train: 2.5536 (36.3%) | Phys: 0.00 | Val: 1.4917 (61.01%) | Best: 61.69%\n",
            "Epoch 191 | T: 37s | Train: 2.5595 (35.8%) | Phys: 0.00 | Val: 1.5261 (60.17%) | Best: 61.69%\n",
            "Epoch 192 | T: 36s | Train: 2.5753 (35.4%) | Phys: 0.00 | Val: 1.5140 (61.08%) | Best: 61.69%\n",
            "Epoch 193 | T: 36s | Train: 2.5157 (37.1%) | Phys: 0.00 | Val: 1.5020 (60.64%) | Best: 61.69%\n",
            "Epoch 194 | T: 36s | Train: 2.5329 (36.4%) | Phys: 0.00 | Val: 1.5018 (61.07%) | Best: 61.69%\n",
            "Epoch 195 | T: 36s | Train: 2.5557 (36.0%) | Phys: 0.00 | Val: 1.5381 (59.64%) | Best: 61.69%\n",
            "Epoch 196 | T: 36s | Train: 2.4978 (37.4%) | Phys: 0.00 | Val: 1.4470 (62.13%) | Best: 62.13%\n",
            "Epoch 197 | T: 36s | Train: 2.5484 (36.5%) | Phys: 0.00 | Val: 1.5365 (59.96%) | Best: 62.13%\n",
            "Epoch 198 | T: 36s | Train: 2.5542 (36.2%) | Phys: 0.00 | Val: 1.4888 (61.27%) | Best: 62.13%\n",
            "Epoch 199 | T: 36s | Train: 2.5535 (35.9%) | Phys: 0.00 | Val: 1.5262 (60.26%) | Best: 62.13%\n",
            "Epoch 200 | T: 37s | Train: 2.4779 (37.9%) | Phys: 0.00 | Val: 1.4917 (60.99%) | Best: 62.13%\n",
            "Epoch 201 | T: 36s | Train: 2.5273 (36.9%) | Phys: 0.00 | Val: 1.4862 (61.65%) | Best: 62.13%\n",
            "Epoch 202 | T: 36s | Train: 2.4664 (38.4%) | Phys: 0.00 | Val: 1.4545 (61.85%) | Best: 62.13%\n",
            "Epoch 203 | T: 37s | Train: 2.4887 (37.6%) | Phys: 0.00 | Val: 1.4479 (62.07%) | Best: 62.13%\n",
            "Epoch 204 | T: 36s | Train: 2.4339 (39.1%) | Phys: 0.00 | Val: 1.4394 (62.54%) | Best: 62.54%\n",
            "Epoch 205 | T: 37s | Train: 2.5291 (36.7%) | Phys: 0.00 | Val: 1.5036 (61.17%) | Best: 62.54%\n",
            "Epoch 206 | T: 36s | Train: 2.4952 (37.4%) | Phys: 0.00 | Val: 1.4241 (62.70%) | Best: 62.70%\n",
            "Epoch 207 | T: 36s | Train: 2.4651 (38.3%) | Phys: 0.00 | Val: 1.4747 (61.26%) | Best: 62.70%\n",
            "Epoch 208 | T: 37s | Train: 2.4774 (38.2%) | Phys: 0.00 | Val: 1.5071 (61.21%) | Best: 62.70%\n",
            "Epoch 209 | T: 37s | Train: 2.4874 (37.8%) | Phys: 0.00 | Val: 1.4452 (62.45%) | Best: 62.70%\n",
            "Epoch 210 | T: 37s | Train: 2.4606 (38.7%) | Phys: 0.00 | Val: 1.4359 (62.57%) | Best: 62.70%\n",
            "Epoch 211 | T: 37s | Train: 2.5429 (36.6%) | Phys: 0.00 | Val: 1.4021 (63.38%) | Best: 63.38%\n",
            "Epoch 212 | T: 36s | Train: 2.4593 (38.7%) | Phys: 0.00 | Val: 1.4329 (62.46%) | Best: 63.38%\n",
            "Epoch 213 | T: 36s | Train: 2.4583 (38.4%) | Phys: 0.00 | Val: 1.4364 (62.52%) | Best: 63.38%\n",
            "Epoch 214 | T: 36s | Train: 2.4699 (38.3%) | Phys: 0.00 | Val: 1.4186 (62.35%) | Best: 63.38%\n",
            "Epoch 215 | T: 37s | Train: 2.4729 (38.1%) | Phys: 0.00 | Val: 1.4402 (62.35%) | Best: 63.38%\n",
            "Epoch 216 | T: 37s | Train: 2.3650 (40.5%) | Phys: 0.00 | Val: 1.3932 (63.52%) | Best: 63.52%\n",
            "Epoch 217 | T: 37s | Train: 2.5052 (37.6%) | Phys: 0.00 | Val: 1.4114 (62.94%) | Best: 63.52%\n",
            "Epoch 218 | T: 37s | Train: 2.4885 (38.0%) | Phys: 0.00 | Val: 1.4050 (63.47%) | Best: 63.52%\n",
            "Epoch 219 | T: 36s | Train: 2.4104 (40.1%) | Phys: 0.00 | Val: 1.3841 (62.84%) | Best: 63.52%\n",
            "Epoch 220 | T: 37s | Train: 2.3984 (40.2%) | Phys: 0.00 | Val: 1.3964 (63.42%) | Best: 63.52%\n",
            "Epoch 221 | T: 36s | Train: 2.4245 (39.5%) | Phys: 0.00 | Val: 1.3775 (64.12%) | Best: 64.12%\n",
            "Epoch 222 | T: 37s | Train: 2.4661 (38.7%) | Phys: 0.00 | Val: 1.3900 (63.43%) | Best: 64.12%\n",
            "Epoch 223 | T: 37s | Train: 2.4399 (39.2%) | Phys: 0.00 | Val: 1.4158 (62.61%) | Best: 64.12%\n",
            "Epoch 224 | T: 37s | Train: 2.4162 (39.6%) | Phys: 0.00 | Val: 1.3485 (63.93%) | Best: 64.12%\n",
            "Epoch 225 | T: 37s | Train: 2.4718 (38.5%) | Phys: 0.00 | Val: 1.4047 (63.51%) | Best: 64.12%\n",
            "Epoch 226 | T: 37s | Train: 2.3855 (40.7%) | Phys: 0.00 | Val: 1.3629 (64.00%) | Best: 64.12%\n",
            "Epoch 227 | T: 37s | Train: 2.4132 (39.7%) | Phys: 0.00 | Val: 1.3645 (63.90%) | Best: 64.12%\n",
            "Epoch 228 | T: 37s | Train: 2.4421 (39.0%) | Phys: 0.00 | Val: 1.3633 (64.28%) | Best: 64.28%\n",
            "Epoch 229 | T: 36s | Train: 2.4068 (40.2%) | Phys: 0.00 | Val: 1.3586 (64.01%) | Best: 64.28%\n",
            "Epoch 230 | T: 36s | Train: 2.4405 (39.5%) | Phys: 0.00 | Val: 1.3797 (63.75%) | Best: 64.28%\n",
            "Epoch 231 | T: 37s | Train: 2.3418 (41.3%) | Phys: 0.00 | Val: 1.3573 (64.19%) | Best: 64.28%\n",
            "Epoch 232 | T: 36s | Train: 2.3752 (40.9%) | Phys: 0.00 | Val: 1.3354 (64.86%) | Best: 64.86%\n",
            "Epoch 233 | T: 37s | Train: 2.3857 (40.6%) | Phys: 0.00 | Val: 1.3433 (64.76%) | Best: 64.86%\n",
            "Epoch 234 | T: 37s | Train: 2.3853 (40.4%) | Phys: 0.00 | Val: 1.3358 (64.63%) | Best: 64.86%\n",
            "Epoch 235 | T: 37s | Train: 2.3809 (40.5%) | Phys: 0.00 | Val: 1.3099 (64.39%) | Best: 64.86%\n",
            "Epoch 236 | T: 36s | Train: 2.3983 (40.3%) | Phys: 0.00 | Val: 1.3867 (63.39%) | Best: 64.86%\n",
            "Epoch 237 | T: 37s | Train: 2.3666 (41.0%) | Phys: 0.00 | Val: 1.3048 (65.14%) | Best: 65.14%\n",
            "Epoch 238 | T: 37s | Train: 2.3562 (41.4%) | Phys: 0.00 | Val: 1.3141 (64.52%) | Best: 65.14%\n",
            "Epoch 239 | T: 37s | Train: 2.3331 (42.0%) | Phys: 0.00 | Val: 1.3178 (65.05%) | Best: 65.14%\n",
            "Epoch 240 | T: 37s | Train: 2.3678 (41.2%) | Phys: 0.00 | Val: 1.3386 (64.53%) | Best: 65.14%\n",
            "Epoch 241 | T: 36s | Train: 2.3707 (41.1%) | Phys: 0.00 | Val: 1.2991 (65.31%) | Best: 65.31%\n",
            "Epoch 242 | T: 36s | Train: 2.3516 (41.6%) | Phys: 0.00 | Val: 1.3209 (65.46%) | Best: 65.46%\n",
            "Epoch 243 | T: 37s | Train: 2.3423 (41.8%) | Phys: 0.00 | Val: 1.3108 (65.33%) | Best: 65.46%\n",
            "Epoch 244 | T: 37s | Train: 2.3174 (42.3%) | Phys: 0.00 | Val: 1.2899 (66.16%) | Best: 66.16%\n",
            "Epoch 245 | T: 36s | Train: 2.3155 (42.5%) | Phys: 0.00 | Val: 1.2860 (66.41%) | Best: 66.41%\n",
            "Epoch 246 | T: 36s | Train: 2.3165 (42.7%) | Phys: 0.00 | Val: 1.2864 (65.78%) | Best: 66.41%\n",
            "Epoch 247 | T: 37s | Train: 2.3623 (41.6%) | Phys: 0.00 | Val: 1.3201 (65.19%) | Best: 66.41%\n",
            "Epoch 248 | T: 36s | Train: 2.3838 (40.9%) | Phys: 0.00 | Val: 1.3204 (65.45%) | Best: 66.41%\n",
            "Epoch 249 | T: 36s | Train: 2.3278 (42.3%) | Phys: 0.00 | Val: 1.2459 (66.69%) | Best: 66.69%\n",
            "Epoch 250 | T: 37s | Train: 2.3209 (42.3%) | Phys: 0.00 | Val: 1.2308 (67.00%) | Best: 67.00%\n",
            "Epoch 251 | T: 37s | Train: 2.3214 (42.6%) | Phys: 0.00 | Val: 1.3248 (65.99%) | Best: 67.00%\n",
            "Epoch 252 | T: 36s | Train: 2.3194 (42.4%) | Phys: 0.00 | Val: 1.2734 (66.17%) | Best: 67.00%\n",
            "Epoch 253 | T: 37s | Train: 2.2837 (43.6%) | Phys: 0.00 | Val: 1.2813 (65.93%) | Best: 67.00%\n",
            "Epoch 254 | T: 37s | Train: 2.3441 (42.2%) | Phys: 0.00 | Val: 1.2942 (66.31%) | Best: 67.00%\n",
            "Epoch 255 | T: 37s | Train: 2.3109 (42.6%) | Phys: 0.00 | Val: 1.2853 (66.60%) | Best: 67.00%\n",
            "Epoch 256 | T: 37s | Train: 2.2464 (44.1%) | Phys: 0.00 | Val: 1.2705 (66.38%) | Best: 67.00%\n",
            "Epoch 257 | T: 37s | Train: 2.2896 (43.3%) | Phys: 0.00 | Val: 1.2670 (66.34%) | Best: 67.00%\n",
            "Epoch 258 | T: 36s | Train: 2.2814 (43.4%) | Phys: 0.00 | Val: 1.2590 (66.71%) | Best: 67.00%\n",
            "Epoch 259 | T: 37s | Train: 2.2252 (45.1%) | Phys: 0.00 | Val: 1.2812 (66.10%) | Best: 67.00%\n",
            "Epoch 260 | T: 37s | Train: 2.2966 (43.5%) | Phys: 0.00 | Val: 1.2493 (66.84%) | Best: 67.00%\n",
            "Epoch 261 | T: 36s | Train: 2.2100 (45.6%) | Phys: 0.00 | Val: 1.2409 (67.00%) | Best: 67.00%\n",
            "Epoch 262 | T: 36s | Train: 2.2603 (44.4%) | Phys: 0.00 | Val: 1.2423 (67.07%) | Best: 67.07%\n",
            "Epoch 263 | T: 37s | Train: 2.2560 (43.9%) | Phys: 0.00 | Val: 1.2315 (67.34%) | Best: 67.34%\n",
            "Epoch 264 | T: 37s | Train: 2.2997 (43.5%) | Phys: 0.00 | Val: 1.2530 (67.12%) | Best: 67.34%\n",
            "Epoch 265 | T: 37s | Train: 2.2554 (44.5%) | Phys: 0.00 | Val: 1.2698 (66.65%) | Best: 67.34%\n",
            "Epoch 266 | T: 36s | Train: 2.2406 (44.8%) | Phys: 0.00 | Val: 1.2393 (67.12%) | Best: 67.34%\n",
            "Epoch 267 | T: 36s | Train: 2.1988 (45.8%) | Phys: 0.00 | Val: 1.2012 (68.45%) | Best: 68.45%\n",
            "Epoch 268 | T: 37s | Train: 2.2537 (44.6%) | Phys: 0.00 | Val: 1.2350 (67.93%) | Best: 68.45%\n",
            "Epoch 269 | T: 36s | Train: 2.2279 (45.5%) | Phys: 0.00 | Val: 1.1792 (68.55%) | Best: 68.55%\n",
            "Epoch 270 | T: 37s | Train: 2.2287 (45.2%) | Phys: 0.00 | Val: 1.2161 (67.68%) | Best: 68.55%\n",
            "Epoch 271 | T: 36s | Train: 2.2138 (45.5%) | Phys: 0.00 | Val: 1.2135 (68.65%) | Best: 68.65%\n",
            "Epoch 272 | T: 37s | Train: 2.2653 (44.5%) | Phys: 0.00 | Val: 1.2216 (67.79%) | Best: 68.65%\n",
            "Epoch 273 | T: 36s | Train: 2.2714 (43.9%) | Phys: 0.00 | Val: 1.1960 (68.13%) | Best: 68.65%\n",
            "Epoch 274 | T: 37s | Train: 2.1952 (46.3%) | Phys: 0.00 | Val: 1.2353 (67.70%) | Best: 68.65%\n",
            "Epoch 275 | T: 37s | Train: 2.2100 (45.6%) | Phys: 0.00 | Val: 1.2225 (67.96%) | Best: 68.65%\n",
            "Epoch 276 | T: 37s | Train: 2.1211 (47.7%) | Phys: 0.00 | Val: 1.1982 (68.60%) | Best: 68.65%\n",
            "Epoch 277 | T: 37s | Train: 2.1780 (46.3%) | Phys: 0.00 | Val: 1.1939 (68.59%) | Best: 68.65%\n",
            "Epoch 278 | T: 37s | Train: 2.1380 (47.5%) | Phys: 0.00 | Val: 1.1773 (68.63%) | Best: 68.65%\n",
            "Epoch 279 | T: 36s | Train: 2.1737 (46.5%) | Phys: 0.00 | Val: 1.1786 (69.07%) | Best: 69.07%\n",
            "Epoch 280 | T: 36s | Train: 2.1547 (47.4%) | Phys: 0.00 | Val: 1.2139 (68.56%) | Best: 69.07%\n",
            "Epoch 281 | T: 37s | Train: 2.2049 (46.0%) | Phys: 0.00 | Val: 1.2030 (68.26%) | Best: 69.07%\n",
            "Epoch 282 | T: 36s | Train: 2.1547 (47.1%) | Phys: 0.00 | Val: 1.1715 (68.70%) | Best: 69.07%\n",
            "Epoch 283 | T: 37s | Train: 2.1621 (46.8%) | Phys: 0.00 | Val: 1.1951 (68.72%) | Best: 69.07%\n",
            "Epoch 284 | T: 37s | Train: 2.1382 (47.9%) | Phys: 0.00 | Val: 1.1656 (69.05%) | Best: 69.07%\n",
            "Epoch 285 | T: 37s | Train: 2.1961 (46.1%) | Phys: 0.00 | Val: 1.1364 (69.69%) | Best: 69.69%\n",
            "Epoch 286 | T: 37s | Train: 2.0926 (48.7%) | Phys: 0.00 | Val: 1.1308 (69.91%) | Best: 69.91%\n",
            "Epoch 287 | T: 36s | Train: 2.0379 (50.4%) | Phys: 0.00 | Val: 1.1599 (69.33%) | Best: 69.91%\n",
            "Epoch 288 | T: 37s | Train: 2.1085 (48.4%) | Phys: 0.00 | Val: 1.1459 (69.39%) | Best: 69.91%\n",
            "Epoch 289 | T: 37s | Train: 2.1192 (48.4%) | Phys: 0.00 | Val: 1.1848 (68.50%) | Best: 69.91%\n",
            "Epoch 290 | T: 37s | Train: 2.1125 (48.5%) | Phys: 0.00 | Val: 1.1495 (69.67%) | Best: 69.91%\n",
            "Epoch 291 | T: 37s | Train: 2.1104 (48.5%) | Phys: 0.00 | Val: 1.1567 (69.69%) | Best: 69.91%\n",
            "Epoch 292 | T: 36s | Train: 2.1354 (47.8%) | Phys: 0.00 | Val: 1.1564 (69.74%) | Best: 69.91%\n",
            "Epoch 293 | T: 36s | Train: 2.1140 (48.4%) | Phys: 0.00 | Val: 1.1596 (69.88%) | Best: 69.91%\n",
            "Epoch 294 | T: 37s | Train: 2.0786 (49.5%) | Phys: 0.00 | Val: 1.1398 (70.08%) | Best: 70.08%\n",
            "Epoch 295 | T: 37s | Train: 2.0648 (49.8%) | Phys: 0.00 | Val: 1.1531 (69.47%) | Best: 70.08%\n",
            "Epoch 296 | T: 36s | Train: 2.0640 (50.0%) | Phys: 0.00 | Val: 1.1197 (69.88%) | Best: 70.08%\n",
            "Epoch 297 | T: 37s | Train: 2.0127 (51.4%) | Phys: 0.00 | Val: 1.1115 (70.21%) | Best: 70.21%\n",
            "Epoch 298 | T: 36s | Train: 2.0856 (49.5%) | Phys: 0.00 | Val: 1.1352 (70.39%) | Best: 70.39%\n",
            "Epoch 299 | T: 37s | Train: 2.1028 (48.7%) | Phys: 0.00 | Val: 1.1368 (70.24%) | Best: 70.39%\n",
            "Epoch 300 | T: 37s | Train: 1.9823 (51.5%) | Phys: 0.00 | Val: 1.1122 (70.52%) | Best: 70.52%\n",
            "Epoch 301 | T: 37s | Train: 2.0279 (51.0%) | Phys: 0.00 | Val: 1.1281 (70.84%) | Best: 70.84%\n",
            "Epoch 302 | T: 36s | Train: 2.0765 (49.6%) | Phys: 0.00 | Val: 1.1281 (70.49%) | Best: 70.84%\n",
            "Epoch 303 | T: 36s | Train: 2.0127 (51.4%) | Phys: 0.00 | Val: 1.0997 (71.02%) | Best: 71.02%\n",
            "Epoch 304 | T: 36s | Train: 2.0533 (50.4%) | Phys: 0.00 | Val: 1.1122 (70.99%) | Best: 71.02%\n",
            "Epoch 305 | T: 37s | Train: 1.9465 (53.5%) | Phys: 0.00 | Val: 1.1088 (71.17%) | Best: 71.17%\n",
            "Epoch 306 | T: 37s | Train: 1.9618 (52.9%) | Phys: 0.00 | Val: 1.1164 (70.87%) | Best: 71.17%\n",
            "Epoch 307 | T: 37s | Train: 2.0217 (51.1%) | Phys: 0.00 | Val: 1.1053 (70.79%) | Best: 71.17%\n",
            "Epoch 308 | T: 37s | Train: 2.0429 (50.8%) | Phys: 0.00 | Val: 1.1206 (70.96%) | Best: 71.17%\n",
            "Epoch 309 | T: 37s | Train: 1.9323 (53.3%) | Phys: 0.00 | Val: 1.1111 (70.70%) | Best: 71.17%\n",
            "Epoch 310 | T: 37s | Train: 2.0048 (51.6%) | Phys: 0.00 | Val: 1.1163 (70.26%) | Best: 71.17%\n",
            "Epoch 311 | T: 37s | Train: 2.0020 (51.5%) | Phys: 0.00 | Val: 1.0998 (70.26%) | Best: 71.17%\n",
            "Epoch 312 | T: 37s | Train: 1.9787 (52.3%) | Phys: 0.00 | Val: 1.0809 (71.71%) | Best: 71.71%\n",
            "Epoch 313 | T: 36s | Train: 1.9424 (53.3%) | Phys: 0.00 | Val: 1.1014 (71.19%) | Best: 71.71%\n",
            "Epoch 314 | T: 36s | Train: 1.9712 (52.3%) | Phys: 0.00 | Val: 1.1098 (70.83%) | Best: 71.71%\n",
            "Epoch 315 | T: 36s | Train: 1.8977 (54.4%) | Phys: 0.00 | Val: 1.1036 (71.75%) | Best: 71.75%\n",
            "Epoch 316 | T: 36s | Train: 1.9994 (51.8%) | Phys: 0.00 | Val: 1.0941 (71.81%) | Best: 71.81%\n",
            "Epoch 317 | T: 37s | Train: 1.9618 (52.8%) | Phys: 0.00 | Val: 1.0946 (71.42%) | Best: 71.81%\n",
            "Epoch 318 | T: 37s | Train: 1.9984 (51.7%) | Phys: 0.00 | Val: 1.0900 (71.36%) | Best: 71.81%\n",
            "Epoch 319 | T: 37s | Train: 1.9017 (54.3%) | Phys: 0.00 | Val: 1.0619 (71.94%) | Best: 71.94%\n",
            "Epoch 320 | T: 37s | Train: 1.9166 (54.4%) | Phys: 0.00 | Val: 1.0827 (72.03%) | Best: 72.03%\n",
            "Epoch 321 | T: 37s | Train: 1.8663 (55.4%) | Phys: 0.00 | Val: 1.0793 (72.03%) | Best: 72.03%\n",
            "Epoch 322 | T: 37s | Train: 1.9110 (54.2%) | Phys: 0.00 | Val: 1.0672 (72.22%) | Best: 72.22%\n",
            "Epoch 323 | T: 37s | Train: 1.9698 (52.9%) | Phys: 0.00 | Val: 1.0922 (71.70%) | Best: 72.22%\n",
            "Epoch 324 | T: 36s | Train: 1.8882 (54.7%) | Phys: 0.00 | Val: 1.0629 (72.26%) | Best: 72.26%\n",
            "Epoch 325 | T: 36s | Train: 1.9016 (54.2%) | Phys: 0.00 | Val: 1.0775 (72.12%) | Best: 72.26%\n",
            "Epoch 326 | T: 37s | Train: 1.8317 (56.2%) | Phys: 0.00 | Val: 1.0531 (72.59%) | Best: 72.59%\n",
            "Epoch 327 | T: 36s | Train: 1.8807 (55.2%) | Phys: 0.00 | Val: 1.0741 (71.73%) | Best: 72.59%\n",
            "Epoch 328 | T: 37s | Train: 1.9690 (52.8%) | Phys: 0.00 | Val: 1.0765 (72.04%) | Best: 72.59%\n",
            "Epoch 329 | T: 37s | Train: 1.8065 (57.4%) | Phys: 0.00 | Val: 1.0600 (72.47%) | Best: 72.59%\n",
            "Epoch 330 | T: 36s | Train: 1.8807 (55.1%) | Phys: 0.00 | Val: 1.0670 (72.46%) | Best: 72.59%\n",
            "Epoch 331 | T: 37s | Train: 1.8279 (56.6%) | Phys: 0.00 | Val: 1.0569 (72.73%) | Best: 72.73%\n",
            "Epoch 332 | T: 37s | Train: 1.8372 (56.7%) | Phys: 0.00 | Val: 1.0552 (72.70%) | Best: 72.73%\n",
            "Epoch 333 | T: 36s | Train: 1.8520 (55.8%) | Phys: 0.00 | Val: 1.0669 (72.55%) | Best: 72.73%\n",
            "Epoch 334 | T: 36s | Train: 1.8320 (56.6%) | Phys: 0.00 | Val: 1.0686 (72.63%) | Best: 72.73%\n",
            "Epoch 335 | T: 37s | Train: 1.8872 (54.9%) | Phys: 0.00 | Val: 1.0763 (72.32%) | Best: 72.73%\n",
            "Epoch 336 | T: 37s | Train: 1.8541 (56.0%) | Phys: 0.00 | Val: 1.0435 (73.27%) | Best: 73.27%\n",
            "Epoch 337 | T: 36s | Train: 1.8273 (56.8%) | Phys: 0.00 | Val: 1.0482 (73.28%) | Best: 73.28%\n",
            "Epoch 338 | T: 36s | Train: 1.8054 (57.1%) | Phys: 0.00 | Val: 1.0452 (73.27%) | Best: 73.28%\n",
            "Epoch 339 | T: 36s | Train: 1.7706 (58.0%) | Phys: 0.00 | Val: 1.0640 (72.52%) | Best: 73.28%\n",
            "Epoch 340 | T: 36s | Train: 1.8566 (56.2%) | Phys: 0.00 | Val: 1.0541 (72.70%) | Best: 73.28%\n",
            "Epoch 341 | T: 36s | Train: 1.7831 (57.9%) | Phys: 0.00 | Val: 1.0509 (73.09%) | Best: 73.28%\n",
            "Epoch 342 | T: 37s | Train: 1.7467 (59.4%) | Phys: 0.00 | Val: 1.0449 (73.19%) | Best: 73.28%\n",
            "Epoch 343 | T: 36s | Train: 1.7818 (58.3%) | Phys: 0.00 | Val: 1.0526 (73.26%) | Best: 73.28%\n",
            "Epoch 344 | T: 36s | Train: 1.7493 (59.1%) | Phys: 0.00 | Val: 1.0500 (73.25%) | Best: 73.28%\n",
            "Epoch 345 | T: 36s | Train: 1.7518 (58.7%) | Phys: 0.00 | Val: 1.0372 (73.50%) | Best: 73.50%\n",
            "Epoch 346 | T: 37s | Train: 1.8476 (55.9%) | Phys: 0.00 | Val: 1.0477 (73.43%) | Best: 73.50%\n",
            "Epoch 347 | T: 37s | Train: 1.7501 (58.7%) | Phys: 0.00 | Val: 1.0317 (73.66%) | Best: 73.66%\n",
            "Epoch 348 | T: 36s | Train: 1.7468 (59.0%) | Phys: 0.00 | Val: 1.0483 (73.29%) | Best: 73.66%\n",
            "Epoch 349 | T: 36s | Train: 1.6904 (60.4%) | Phys: 0.00 | Val: 1.0433 (73.30%) | Best: 73.66%\n",
            "Epoch 350 | T: 36s | Train: 1.7880 (57.7%) | Phys: 0.00 | Val: 1.0288 (73.87%) | Best: 73.87%\n",
            "Epoch 351 | T: 36s | Train: 1.7641 (58.6%) | Phys: 0.00 | Val: 1.0334 (73.82%) | Best: 73.87%\n",
            "Epoch 352 | T: 36s | Train: 1.7459 (58.8%) | Phys: 0.00 | Val: 1.0126 (74.29%) | Best: 74.29%\n",
            "Epoch 353 | T: 36s | Train: 1.7129 (59.7%) | Phys: 0.00 | Val: 1.0389 (73.97%) | Best: 74.29%\n",
            "Epoch 354 | T: 36s | Train: 1.7306 (59.4%) | Phys: 0.00 | Val: 1.0330 (73.75%) | Best: 74.29%\n",
            "Epoch 355 | T: 36s | Train: 1.7492 (58.7%) | Phys: 0.00 | Val: 1.0114 (74.09%) | Best: 74.29%\n",
            "Epoch 356 | T: 36s | Train: 1.7611 (58.8%) | Phys: 0.00 | Val: 1.0262 (74.20%) | Best: 74.29%\n",
            "Epoch 357 | T: 37s | Train: 1.6914 (60.6%) | Phys: 0.00 | Val: 1.0104 (74.49%) | Best: 74.49%\n",
            "Epoch 358 | T: 36s | Train: 1.7519 (58.9%) | Phys: 0.00 | Val: 1.0146 (74.76%) | Best: 74.76%\n",
            "Epoch 359 | T: 36s | Train: 1.6833 (60.6%) | Phys: 0.00 | Val: 1.0114 (74.39%) | Best: 74.76%\n",
            "Epoch 360 | T: 36s | Train: 1.7655 (58.4%) | Phys: 0.00 | Val: 1.0213 (74.32%) | Best: 74.76%\n",
            "Epoch 361 | T: 36s | Train: 1.6728 (61.1%) | Phys: 0.00 | Val: 1.0124 (74.33%) | Best: 74.76%\n",
            "Epoch 362 | T: 36s | Train: 1.6873 (60.5%) | Phys: 0.00 | Val: 1.0339 (73.75%) | Best: 74.76%\n",
            "Epoch 363 | T: 37s | Train: 1.6719 (61.0%) | Phys: 0.00 | Val: 1.0059 (74.58%) | Best: 74.76%\n",
            "Epoch 364 | T: 37s | Train: 1.6667 (61.1%) | Phys: 0.00 | Val: 1.0042 (74.51%) | Best: 74.76%\n",
            "Epoch 365 | T: 36s | Train: 1.7361 (59.6%) | Phys: 0.00 | Val: 0.9990 (74.88%) | Best: 74.88%\n",
            "Epoch 366 | T: 36s | Train: 1.6387 (61.9%) | Phys: 0.00 | Val: 0.9912 (74.71%) | Best: 74.88%\n",
            "Epoch 367 | T: 36s | Train: 1.7695 (57.8%) | Phys: 0.00 | Val: 0.9994 (74.95%) | Best: 74.95%\n",
            "Epoch 368 | T: 37s | Train: 1.7309 (59.2%) | Phys: 0.00 | Val: 0.9936 (74.73%) | Best: 74.95%\n",
            "Epoch 369 | T: 36s | Train: 1.6371 (62.2%) | Phys: 0.00 | Val: 0.9906 (74.85%) | Best: 74.95%\n",
            "Epoch 370 | T: 36s | Train: 1.7328 (59.1%) | Phys: 0.00 | Val: 0.9867 (74.99%) | Best: 74.99%\n",
            "Epoch 371 | T: 37s | Train: 1.6383 (62.2%) | Phys: 0.00 | Val: 0.9932 (74.81%) | Best: 74.99%\n",
            "Epoch 372 | T: 36s | Train: 1.5816 (63.4%) | Phys: 0.00 | Val: 0.9749 (75.36%) | Best: 75.36%\n",
            "Epoch 373 | T: 36s | Train: 1.7018 (60.1%) | Phys: 0.00 | Val: 0.9875 (75.20%) | Best: 75.36%\n",
            "Epoch 374 | T: 36s | Train: 1.6259 (62.4%) | Phys: 0.00 | Val: 0.9818 (75.03%) | Best: 75.36%\n",
            "Epoch 375 | T: 37s | Train: 1.6415 (61.8%) | Phys: 0.00 | Val: 0.9783 (75.02%) | Best: 75.36%\n",
            "Epoch 376 | T: 36s | Train: 1.6403 (62.1%) | Phys: 0.00 | Val: 0.9808 (75.36%) | Best: 75.36%\n",
            "Epoch 377 | T: 36s | Train: 1.6119 (62.7%) | Phys: 0.00 | Val: 0.9761 (75.25%) | Best: 75.36%\n",
            "Epoch 378 | T: 37s | Train: 1.6569 (61.6%) | Phys: 0.00 | Val: 0.9837 (75.32%) | Best: 75.36%\n",
            "Epoch 379 | T: 36s | Train: 1.5385 (64.7%) | Phys: 0.00 | Val: 0.9710 (75.26%) | Best: 75.36%\n",
            "Epoch 380 | T: 36s | Train: 1.6260 (62.6%) | Phys: 0.00 | Val: 0.9845 (75.11%) | Best: 75.36%\n",
            "Epoch 381 | T: 36s | Train: 1.6377 (61.7%) | Phys: 0.00 | Val: 0.9793 (75.17%) | Best: 75.36%\n",
            "Epoch 382 | T: 36s | Train: 1.6007 (62.7%) | Phys: 0.00 | Val: 0.9712 (75.30%) | Best: 75.36%\n",
            "Epoch 383 | T: 36s | Train: 1.6344 (62.1%) | Phys: 0.00 | Val: 0.9787 (75.26%) | Best: 75.36%\n",
            "Epoch 384 | T: 37s | Train: 1.6285 (62.4%) | Phys: 0.00 | Val: 0.9756 (75.37%) | Best: 75.37%\n",
            "Epoch 385 | T: 37s | Train: 1.6381 (61.9%) | Phys: 0.00 | Val: 0.9756 (75.68%) | Best: 75.68%\n",
            "Epoch 386 | T: 36s | Train: 1.6447 (61.6%) | Phys: 0.00 | Val: 0.9702 (75.87%) | Best: 75.87%\n",
            "Epoch 387 | T: 36s | Train: 1.5919 (63.3%) | Phys: 0.00 | Val: 0.9757 (75.72%) | Best: 75.87%\n",
            "Epoch 388 | T: 36s | Train: 1.6060 (62.4%) | Phys: 0.00 | Val: 0.9643 (75.79%) | Best: 75.87%\n",
            "Epoch 389 | T: 36s | Train: 1.6066 (62.9%) | Phys: 0.00 | Val: 0.9713 (75.77%) | Best: 75.87%\n",
            "Epoch 390 | T: 36s | Train: 1.5608 (63.8%) | Phys: 0.00 | Val: 0.9660 (75.53%) | Best: 75.87%\n",
            "Epoch 391 | T: 37s | Train: 1.6588 (61.1%) | Phys: 0.00 | Val: 0.9750 (75.55%) | Best: 75.87%\n",
            "Epoch 392 | T: 36s | Train: 1.5908 (63.1%) | Phys: 0.00 | Val: 0.9711 (75.62%) | Best: 75.87%\n",
            "Epoch 393 | T: 36s | Train: 1.6330 (61.8%) | Phys: 0.00 | Val: 0.9695 (75.75%) | Best: 75.87%\n",
            "Epoch 394 | T: 36s | Train: 1.6166 (62.7%) | Phys: 0.00 | Val: 0.9714 (75.71%) | Best: 75.87%\n",
            "Epoch 395 | T: 36s | Train: 1.5747 (63.8%) | Phys: 0.00 | Val: 0.9733 (75.53%) | Best: 75.87%\n",
            "Epoch 396 | T: 36s | Train: 1.6099 (62.3%) | Phys: 0.00 | Val: 0.9721 (75.66%) | Best: 75.87%\n",
            "Epoch 397 | T: 37s | Train: 1.5836 (63.0%) | Phys: 0.00 | Val: 0.9662 (75.81%) | Best: 75.87%\n",
            "Epoch 398 | T: 36s | Train: 1.5618 (63.9%) | Phys: 0.00 | Val: 0.9692 (75.67%) | Best: 75.87%\n",
            "Epoch 399 | T: 37s | Train: 1.6510 (61.6%) | Phys: 0.00 | Val: 0.9696 (75.72%) | Best: 75.87%\n",
            "Epoch 400 | T: 37s | Train: 1.5095 (65.3%) | Phys: 0.00 | Val: 0.9645 (75.84%) | Best: 75.87%\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": [],
      "name": "CIFAR-100 ViT",
      "gpuType": "L4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}