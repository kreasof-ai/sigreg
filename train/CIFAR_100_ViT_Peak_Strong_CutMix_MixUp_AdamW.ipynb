{
  "cells": [
    {
      "cell_type": "code",
      "id": "pBxX3I1kYwojaamntSxiNZTK",
      "metadata": {
        "tags": [],
        "id": "pBxX3I1kYwojaamntSxiNZTK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ae78d71-edcf-493c-ebcb-071120e288e7"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "import math\n",
        "import time\n",
        "\n",
        "# ==========================================\n",
        "# 1. Configuration (Tuned for ViT on CIFAR)\n",
        "# ==========================================\n",
        "REG_MODE = 'strong'\n",
        "SIGR_ALPHA = 0.01   # Strength of the physics constraint\n",
        "SKETCH_DIM = 64    # Dimension of the random observer\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "LEARNING_RATE = 1e-3  # Slightly higher initial LR for AdamW with cosine schedule\n",
        "EPOCHS = 400\n",
        "WEIGHT_DECAY = 0.05\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "if torch.backends.mps.is_available(): DEVICE = 'mps'\n",
        "\n",
        "# Regularization Config\n",
        "MIXUP_ALPHA = 0.8\n",
        "CUTMIX_ALPHA = 1.0\n",
        "DROP_PATH_RATE = 0.1\n",
        "\n",
        "print(f\"Training on device: {DEVICE}\")\n",
        "\n",
        "# ==========================================\n",
        "# 2. Data Preparation (THE FIX: Strong Augmentation)\n",
        "# ==========================================\n",
        "def get_data_loaders():\n",
        "    print('==> Preparing data with Strong Augmentation...')\n",
        "\n",
        "    mean = (0.5071, 0.4867, 0.4408)\n",
        "    std = (0.2675, 0.2565, 0.2761)\n",
        "\n",
        "    # FIX 1: Add RandAugment\n",
        "    transform_train = transforms.Compose([\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandAugment(num_ops=2, magnitude=9), # <--- CRITICAL FOR ViT\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean, std),\n",
        "    ])\n",
        "\n",
        "    transform_test = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean, std),\n",
        "    ])\n",
        "\n",
        "    trainset = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=transform_train)\n",
        "    # Increase workers to handle augmentation load\n",
        "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
        "\n",
        "    testset = torchvision.datasets.CIFAR100(root='./data', train=False, download=True, transform=transform_test)\n",
        "    testloader = torch.utils.data.DataLoader(testset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
        "\n",
        "    return trainloader, testloader\n",
        "\n",
        "\n",
        "# ------------------------------------------\n",
        "# Physics Engine: The Regularizers\n",
        "# ------------------------------------------\n",
        "\n",
        "def sigreg_weak_loss(x, sketch_dim=64):\n",
        "    \"\"\"\n",
        "    Forces Covariance(x) ~ Identity.\n",
        "    Matches the 2nd Moment (Spherical Cloud).\n",
        "    \"\"\"\n",
        "    N, C = x.size()\n",
        "    # 1. Sketching (Optional for C=512, but good for consistency)\n",
        "    if C > sketch_dim:\n",
        "        S = torch.randn(sketch_dim, C, device=x.device) / (C ** 0.5)\n",
        "        x = x @ S.T  # [N, sketch_dim]\n",
        "    else:\n",
        "        sketch_dim = C\n",
        "\n",
        "    # 2. Centering & Covariance\n",
        "    x = x - x.mean(dim=0, keepdim=True)\n",
        "    cov = (x.T @ x) / (N - 1 + 1e-6)\n",
        "\n",
        "    # 3. Target Identity\n",
        "    target = torch.eye(sketch_dim, device=x.device)\n",
        "\n",
        "    # 4. Off-diagonal suppression + Diagonal maintenance\n",
        "    return torch.norm(cov - target, p='fro')\n",
        "\n",
        "def sigreg_strong_loss(x, sketch_dim=64):\n",
        "    \"\"\"\n",
        "    Forces ECF(x) ~ ECF(Gaussian).\n",
        "    Matches ALL Moments (Maximum Entropy Cloud).\n",
        "    Exact implementation of LeJEPA Algorithm 1.\n",
        "    \"\"\"\n",
        "    N, C = x.size()\n",
        "\n",
        "    # 1. Projection (The Observer)\n",
        "    # Project channels down to sketch_dim\n",
        "    A = torch.randn(C, sketch_dim, device=x.device)\n",
        "    A = A / (A.norm(p=2, dim=0, keepdim=True) + 1e-6)\n",
        "\n",
        "    # 2. Integration Points\n",
        "    t = torch.linspace(-5, 5, 17, device=x.device)\n",
        "\n",
        "    # 3. Theoretical Gaussian CF\n",
        "    exp_f = torch.exp(-0.5 * t**2)\n",
        "\n",
        "    # 4. Empirical CF\n",
        "    # proj: [N, sketch_dim]\n",
        "    proj = x @ A\n",
        "\n",
        "    # args: [N, sketch_dim, T]\n",
        "    args = proj.unsqueeze(2) * t.view(1, 1, -1)\n",
        "\n",
        "    # ecf: [sketch_dim, T] (Mean over batch)\n",
        "    ecf = torch.exp(1j * args).mean(dim=0)\n",
        "\n",
        "    # 5. Weighted L2 Distance\n",
        "    # |ecf - gauss|^2 * gauss_weight\n",
        "    diff_sq = (ecf - exp_f.unsqueeze(0)).abs().square()\n",
        "    err = diff_sq * exp_f.unsqueeze(0)\n",
        "\n",
        "    # 6. Integrate\n",
        "    loss = torch.trapz(err, t, dim=1) * N\n",
        "\n",
        "    return loss.mean()\n",
        "\n",
        "# ==========================================\n",
        "# 3. Mixup / CutMix Utilities\n",
        "# ==========================================\n",
        "def rand_bbox(size, lam):\n",
        "    W = size[2]\n",
        "    H = size[3]\n",
        "    cut_rat = np.sqrt(1. - lam)\n",
        "    cut_w = int(W * cut_rat)\n",
        "    cut_h = int(H * cut_rat)\n",
        "\n",
        "    cx = np.random.randint(W)\n",
        "    cy = np.random.randint(H)\n",
        "\n",
        "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
        "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
        "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
        "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
        "\n",
        "    return bbx1, bby1, bbx2, bby2\n",
        "\n",
        "def drop_path(x, drop_prob: float = 0., training: bool = False):\n",
        "    if drop_prob == 0. or not training: return x\n",
        "    keep_prob = 1 - drop_prob\n",
        "    shape = (x.shape[0],) + (1,) * (x.ndim - 1)\n",
        "    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
        "    random_tensor.floor_()\n",
        "    return x.div(keep_prob) * random_tensor\n",
        "\n",
        "class DropPath(nn.Module):\n",
        "    def __init__(self, drop_prob=None):\n",
        "        super(DropPath, self).__init__()\n",
        "        self.drop_prob = drop_prob\n",
        "    def forward(self, x): return drop_path(x, self.drop_prob, self.training)\n",
        "\n",
        "class PatchEmbedding(nn.Module):\n",
        "    def __init__(self, img_size=32, patch_size=4, in_chans=3, embed_dim=192):\n",
        "        super().__init__()\n",
        "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "    def forward(self, x):\n",
        "        x = self.proj(x).flatten(2).transpose(1, 2)\n",
        "        return x\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, dim, num_heads=8, qkv_bias=False, attn_drop=0., proj_drop=0.):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        head_dim = dim // num_heads\n",
        "        self.scale = head_dim ** -0.5\n",
        "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
        "        self.attn_drop = nn.Dropout(attn_drop)\n",
        "        self.proj = nn.Linear(dim, dim)\n",
        "        self.proj_drop = nn.Dropout(proj_drop)\n",
        "    def forward(self, x):\n",
        "        B, N, C = x.shape\n",
        "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        attn = self.attn_drop(attn)\n",
        "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
        "        x = self.proj(x)\n",
        "        x = self.proj_drop(x)\n",
        "        return x\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, dim, hidden_dim, drop=0.):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(dim, hidden_dim)\n",
        "        self.act = nn.GELU()\n",
        "        self.fc2 = nn.Linear(hidden_dim, dim)\n",
        "        self.drop = nn.Dropout(drop)\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.drop(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.drop(x)\n",
        "        return x\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, dim, num_heads, mlp_ratio=4., drop=0., attn_drop=0., drop_path=0., reg_mode='strong', sketch_dim=64):\n",
        "        super().__init__()\n",
        "        self.norm1 = nn.LayerNorm(dim)\n",
        "        self.attn = MultiHeadAttention(dim, num_heads=num_heads, qkv_bias=True, attn_drop=attn_drop, proj_drop=drop)\n",
        "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
        "        self.norm2 = nn.LayerNorm(dim)\n",
        "        self.mlp = MLP(dim, int(dim * mlp_ratio), drop=drop)\n",
        "\n",
        "        self.reg_mode = reg_mode\n",
        "        self.sketch_dim = sketch_dim\n",
        "    def forward(self, x):\n",
        "        x = x + self.drop_path(self.attn(self.norm1(x)))\n",
        "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
        "\n",
        "\n",
        "        # --- PHYSICS INJECTION ---\n",
        "        reg_loss = torch.tensor(0.0, device=x.device)\n",
        "        if self.reg_mode != 'baseline':\n",
        "            # Global Average Pool of the tokens [B, N, C] -> [B, C]\n",
        "            # This represents the \"Image Vector\" at this depth\n",
        "            flat_rep = x.mean(dim=1)\n",
        "\n",
        "            # Crucial: Pre-Norm vs Post-Norm context.\n",
        "            # LayerNorm forces variance=1. SIGReg forces Distribution=Gaussian.\n",
        "            # They are compatible.\n",
        "            if self.reg_mode == 'weak':\n",
        "                reg_loss = sigreg_weak_loss(flat_rep, self.sketch_dim)\n",
        "            elif self.reg_mode == 'strong':\n",
        "                reg_loss = sigreg_strong_loss(flat_rep, self.sketch_dim)\n",
        "\n",
        "        return x, reg_loss\n",
        "\n",
        "class VisionTransformer(nn.Module):\n",
        "    def __init__(self, img_size=32, patch_size=4, in_chans=3, num_classes=100, embed_dim=192, depth=9, num_heads=3, mlp_ratio=4., drop_rate=0., attn_drop_rate=0., drop_path_rate=0.1, reg_mode='strong', sketch_dim=64):\n",
        "        super().__init__()\n",
        "        self.patch_embed = PatchEmbedding(img_size, patch_size, in_chans, embed_dim)\n",
        "        num_patches = self.patch_embed.proj.weight.shape[2] # Just a hack to get patch count logic\n",
        "        num_patches = (img_size // patch_size) ** 2\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n",
        "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]\n",
        "        self.blocks = nn.ModuleList([\n",
        "            TransformerBlock(embed_dim, num_heads, mlp_ratio, drop_rate, attn_drop_rate, dpr[i], reg_mode, sketch_dim)\n",
        "            for i in range(depth)])\n",
        "        self.norm = nn.LayerNorm(embed_dim)\n",
        "        self.head = nn.Linear(embed_dim, num_classes)\n",
        "        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n",
        "        nn.init.trunc_normal_(self.cls_token, std=0.02)\n",
        "        nn.init.xavier_uniform_(self.head.weight)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B = x.shape[0]\n",
        "        x = self.patch_embed(x)\n",
        "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
        "        x = torch.cat((cls_tokens, x), dim=1)\n",
        "        x = x + self.pos_embed\n",
        "\n",
        "        total_phys_loss = 0.0\n",
        "\n",
        "        # Pass through blocks\n",
        "        for blk in self.blocks:\n",
        "            x, l_loss = blk(x, )\n",
        "            total_phys_loss += l_loss\n",
        "\n",
        "        x = self.norm(x)\n",
        "        out = self.head(x[:, 0])\n",
        "        return out, (total_phys_loss / len(self.blocks))\n",
        "\n",
        "# ==========================================\n",
        "# 5. Training Engine (Updated for Mixup/CutMix)\n",
        "# ==========================================\n",
        "def train(epoch, net, trainloader, optimizer, criterion):\n",
        "    net.train()\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    phys_loss_meter = 0\n",
        "\n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
        "\n",
        "        # Apply Mixup/CutMix\n",
        "        r = np.random.rand(1)\n",
        "        if r < 0.5: # Mixup\n",
        "            lam = np.random.beta(MIXUP_ALPHA, MIXUP_ALPHA)\n",
        "            index = torch.randperm(inputs.size(0)).to(DEVICE)\n",
        "            inputs = lam * inputs + (1 - lam) * inputs[index, :]\n",
        "            targets_a, targets_b = targets, targets[index]\n",
        "        else: # CutMix\n",
        "            lam = np.random.beta(CUTMIX_ALPHA, CUTMIX_ALPHA)\n",
        "            rand_index = torch.randperm(inputs.size(0)).to(DEVICE)\n",
        "            target_a = targets\n",
        "            target_b = targets[rand_index]\n",
        "            bbx1, bby1, bbx2, bby2 = rand_bbox(inputs.size(), lam)\n",
        "            inputs[:, :, bbx1:bbx2, bby1:bby2] = inputs[rand_index, :, bbx1:bbx2, bby1:bby2]\n",
        "            lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (inputs.size()[-1] * inputs.size()[-2]))\n",
        "            targets_a, targets_b = target_a, target_b\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward\n",
        "        outputs, p_loss = net(inputs)\n",
        "\n",
        "        # Task Loss\n",
        "        c_loss = criterion(outputs, targets_a) * lam + criterion(outputs, targets_b) * (1. - lam)\n",
        "\n",
        "        # Total Loss\n",
        "        loss = (1 - SIGR_ALPHA) * c_loss + (SIGR_ALPHA * p_loss)\n",
        "\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(net.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += ((1 - SIGR_ALPHA) * c_loss).item() # Log only task loss for comparison\n",
        "        phys_loss_meter += (SIGR_ALPHA * p_loss).item()\n",
        "\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += (lam * predicted.eq(targets_a).float() + (1 - lam) * predicted.eq(targets_b).float()).sum().item()\n",
        "\n",
        "    acc = 100. * correct / total\n",
        "    return train_loss / (batch_idx + 1), acc, phys_loss_meter / (batch_idx + 1)\n",
        "\n",
        "def test(epoch, net, testloader, criterion):\n",
        "    net.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "            inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
        "            outputs, _ = net(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "    acc = 100. * correct / total\n",
        "    return test_loss / (batch_idx + 1), acc\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    trainloader, testloader = get_data_loaders()\n",
        "\n",
        "    net = VisionTransformer(img_size=32, patch_size=4, embed_dim=192, depth=9, num_heads=3, drop_path_rate=0.1, reg_mode=REG_MODE, sketch_dim=SKETCH_DIM)\n",
        "    net = net.to(DEVICE)\n",
        "\n",
        "    # Standard CrossEntropy for final eval, SoftLabel for training is handled by Mixup logic\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # FIX 3: Robust AdamW Baseline\n",
        "    optimizer = optim.AdamW(net.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
        "\n",
        "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS, eta_min=1e-5)\n",
        "\n",
        "    print(f\"Starting training for {EPOCHS} epochs with RandAugment + Mixup/CutMix...\")\n",
        "\n",
        "    best_acc = 0\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        start_time = time.time()\n",
        "\n",
        "        train_loss, train_acc, physics_loss = train(epoch, net, trainloader, optimizer, criterion)\n",
        "        test_loss, test_acc = test(epoch, net, testloader, criterion)\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        if test_acc > best_acc:\n",
        "            best_acc = test_acc\n",
        "            # torch.save(net.state_dict(), f'thermo_resnet_{REG_MODE}.pth')\n",
        "\n",
        "        epoch_time = time.time() - start_time\n",
        "\n",
        "        print(f\"Epoch {epoch+1} | T: {epoch_time:.0f}s | \"\n",
        "              f\"Train: {train_loss:.4f} ({train_acc:.1f}%) | \"\n",
        "              f\"Phys: {physics_loss:.2f} | \"\n",
        "              f\"Val: {test_loss:.4f} ({test_acc:.2f}%) | \"\n",
        "              f\"Best: {best_acc:.2f}%\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on device: cuda\n",
            "==> Preparing data with Strong Augmentation...\n",
            "Starting training for 400 epochs with RandAugment + Mixup/CutMix...\n",
            "Epoch 1 | T: 30s | Train: 4.4109 (3.9%) | Phys: 0.10 | Val: 3.9540 (8.48%) | Best: 8.48%\n",
            "Epoch 2 | T: 31s | Train: 4.2285 (6.0%) | Phys: 0.05 | Val: 3.7388 (13.14%) | Best: 13.14%\n",
            "Epoch 3 | T: 30s | Train: 4.1407 (7.5%) | Phys: 0.04 | Val: 3.6273 (15.43%) | Best: 15.43%\n",
            "Epoch 4 | T: 30s | Train: 4.0744 (8.6%) | Phys: 0.04 | Val: 3.4626 (17.57%) | Best: 17.57%\n",
            "Epoch 5 | T: 30s | Train: 4.0105 (9.7%) | Phys: 0.04 | Val: 3.3792 (20.46%) | Best: 20.46%\n",
            "Epoch 6 | T: 30s | Train: 3.9505 (10.8%) | Phys: 0.04 | Val: 3.3006 (20.83%) | Best: 20.83%\n",
            "Epoch 7 | T: 29s | Train: 3.9105 (11.6%) | Phys: 0.04 | Val: 3.2053 (24.22%) | Best: 24.22%\n",
            "Epoch 8 | T: 30s | Train: 3.8778 (12.3%) | Phys: 0.04 | Val: 3.0719 (25.09%) | Best: 25.09%\n",
            "Epoch 9 | T: 29s | Train: 3.8033 (13.5%) | Phys: 0.04 | Val: 2.9801 (27.57%) | Best: 27.57%\n",
            "Epoch 10 | T: 30s | Train: 3.7859 (13.9%) | Phys: 0.04 | Val: 2.9328 (28.15%) | Best: 28.15%\n",
            "Epoch 11 | T: 29s | Train: 3.7640 (14.3%) | Phys: 0.04 | Val: 2.9138 (29.11%) | Best: 29.11%\n",
            "Epoch 12 | T: 30s | Train: 3.7633 (14.5%) | Phys: 0.04 | Val: 2.8983 (29.50%) | Best: 29.50%\n",
            "Epoch 13 | T: 30s | Train: 3.6926 (15.7%) | Phys: 0.04 | Val: 2.8934 (28.78%) | Best: 29.50%\n",
            "Epoch 14 | T: 30s | Train: 3.7002 (15.6%) | Phys: 0.04 | Val: 2.7782 (31.74%) | Best: 31.74%\n",
            "Epoch 15 | T: 29s | Train: 3.6773 (16.0%) | Phys: 0.03 | Val: 2.8052 (31.31%) | Best: 31.74%\n",
            "Epoch 16 | T: 30s | Train: 3.6710 (16.6%) | Phys: 0.03 | Val: 2.7126 (32.76%) | Best: 32.76%\n",
            "Epoch 17 | T: 29s | Train: 3.6228 (17.3%) | Phys: 0.03 | Val: 2.6763 (33.83%) | Best: 33.83%\n",
            "Epoch 18 | T: 30s | Train: 3.6190 (17.5%) | Phys: 0.03 | Val: 2.7371 (32.94%) | Best: 33.83%\n",
            "Epoch 19 | T: 30s | Train: 3.5917 (17.9%) | Phys: 0.03 | Val: 2.6070 (34.81%) | Best: 34.81%\n",
            "Epoch 20 | T: 30s | Train: 3.5978 (18.1%) | Phys: 0.04 | Val: 2.6197 (34.33%) | Best: 34.81%\n",
            "Epoch 21 | T: 30s | Train: 3.5682 (18.5%) | Phys: 0.04 | Val: 2.5536 (36.23%) | Best: 36.23%\n",
            "Epoch 22 | T: 30s | Train: 3.5151 (19.6%) | Phys: 0.04 | Val: 2.4847 (37.30%) | Best: 37.30%\n",
            "Epoch 23 | T: 29s | Train: 3.5211 (19.6%) | Phys: 0.04 | Val: 2.4905 (37.17%) | Best: 37.30%\n",
            "Epoch 24 | T: 30s | Train: 3.4775 (20.4%) | Phys: 0.03 | Val: 2.4273 (39.45%) | Best: 39.45%\n",
            "Epoch 25 | T: 30s | Train: 3.5187 (19.6%) | Phys: 0.04 | Val: 2.4619 (38.58%) | Best: 39.45%\n",
            "Epoch 26 | T: 30s | Train: 3.5077 (19.9%) | Phys: 0.04 | Val: 2.4499 (38.60%) | Best: 39.45%\n",
            "Epoch 27 | T: 30s | Train: 3.4759 (20.5%) | Phys: 0.04 | Val: 2.4317 (39.77%) | Best: 39.77%\n",
            "Epoch 28 | T: 30s | Train: 3.4613 (20.9%) | Phys: 0.04 | Val: 2.4238 (38.98%) | Best: 39.77%\n",
            "Epoch 29 | T: 30s | Train: 3.4358 (21.3%) | Phys: 0.04 | Val: 2.3658 (40.91%) | Best: 40.91%\n",
            "Epoch 30 | T: 30s | Train: 3.4019 (22.1%) | Phys: 0.04 | Val: 2.3276 (41.99%) | Best: 41.99%\n",
            "Epoch 31 | T: 29s | Train: 3.3623 (23.1%) | Phys: 0.04 | Val: 2.2897 (41.64%) | Best: 41.99%\n",
            "Epoch 32 | T: 30s | Train: 3.4055 (22.2%) | Phys: 0.04 | Val: 2.2707 (42.83%) | Best: 42.83%\n",
            "Epoch 33 | T: 30s | Train: 3.3856 (22.5%) | Phys: 0.04 | Val: 2.3166 (42.25%) | Best: 42.83%\n",
            "Epoch 34 | T: 29s | Train: 3.3353 (23.5%) | Phys: 0.04 | Val: 2.2392 (43.21%) | Best: 43.21%\n",
            "Epoch 35 | T: 29s | Train: 3.3542 (23.2%) | Phys: 0.04 | Val: 2.1746 (44.24%) | Best: 44.24%\n",
            "Epoch 36 | T: 30s | Train: 3.2851 (24.8%) | Phys: 0.04 | Val: 2.1313 (45.73%) | Best: 45.73%\n",
            "Epoch 37 | T: 30s | Train: 3.3052 (24.3%) | Phys: 0.04 | Val: 2.1915 (44.75%) | Best: 45.73%\n",
            "Epoch 38 | T: 30s | Train: 3.2827 (24.8%) | Phys: 0.04 | Val: 2.1576 (45.69%) | Best: 45.73%\n",
            "Epoch 39 | T: 29s | Train: 3.2426 (25.7%) | Phys: 0.04 | Val: 2.0747 (46.87%) | Best: 46.87%\n",
            "Epoch 40 | T: 30s | Train: 3.2260 (26.1%) | Phys: 0.04 | Val: 2.1012 (46.11%) | Best: 46.87%\n",
            "Epoch 41 | T: 29s | Train: 3.2024 (26.7%) | Phys: 0.04 | Val: 2.0390 (47.72%) | Best: 47.72%\n",
            "Epoch 42 | T: 29s | Train: 3.1924 (26.7%) | Phys: 0.04 | Val: 2.0766 (46.37%) | Best: 47.72%\n",
            "Epoch 43 | T: 30s | Train: 3.1732 (27.1%) | Phys: 0.04 | Val: 1.9828 (48.88%) | Best: 48.88%\n",
            "Epoch 44 | T: 29s | Train: 3.1307 (28.1%) | Phys: 0.04 | Val: 1.9611 (49.55%) | Best: 49.55%\n",
            "Epoch 45 | T: 30s | Train: 3.1413 (28.1%) | Phys: 0.04 | Val: 1.9448 (50.11%) | Best: 50.11%\n",
            "Epoch 46 | T: 30s | Train: 3.1273 (28.4%) | Phys: 0.04 | Val: 1.9448 (50.26%) | Best: 50.26%\n",
            "Epoch 47 | T: 29s | Train: 3.1747 (27.5%) | Phys: 0.04 | Val: 1.9482 (50.09%) | Best: 50.26%\n",
            "Epoch 48 | T: 29s | Train: 3.0402 (30.3%) | Phys: 0.04 | Val: 1.9272 (50.62%) | Best: 50.62%\n",
            "Epoch 49 | T: 30s | Train: 3.1490 (28.0%) | Phys: 0.04 | Val: 1.9647 (49.59%) | Best: 50.62%\n",
            "Epoch 50 | T: 29s | Train: 3.0460 (30.3%) | Phys: 0.04 | Val: 1.8350 (52.58%) | Best: 52.58%\n",
            "Epoch 51 | T: 29s | Train: 3.0953 (29.4%) | Phys: 0.04 | Val: 1.9053 (51.35%) | Best: 52.58%\n",
            "Epoch 52 | T: 29s | Train: 3.0303 (30.7%) | Phys: 0.04 | Val: 1.8603 (52.93%) | Best: 52.93%\n",
            "Epoch 53 | T: 30s | Train: 3.0799 (29.5%) | Phys: 0.04 | Val: 1.8536 (52.25%) | Best: 52.93%\n",
            "Epoch 54 | T: 29s | Train: 3.0697 (29.9%) | Phys: 0.04 | Val: 1.8173 (53.19%) | Best: 53.19%\n",
            "Epoch 55 | T: 30s | Train: 3.0864 (29.5%) | Phys: 0.04 | Val: 1.8844 (51.68%) | Best: 53.19%\n",
            "Epoch 56 | T: 29s | Train: 3.0133 (30.9%) | Phys: 0.04 | Val: 1.7884 (54.02%) | Best: 54.02%\n",
            "Epoch 57 | T: 30s | Train: 3.0364 (30.8%) | Phys: 0.04 | Val: 1.7877 (54.10%) | Best: 54.10%\n",
            "Epoch 58 | T: 30s | Train: 2.9736 (31.9%) | Phys: 0.04 | Val: 1.7642 (54.85%) | Best: 54.85%\n",
            "Epoch 59 | T: 30s | Train: 2.9669 (32.3%) | Phys: 0.04 | Val: 1.7865 (54.25%) | Best: 54.85%\n",
            "Epoch 60 | T: 30s | Train: 2.9390 (32.9%) | Phys: 0.04 | Val: 1.7121 (55.49%) | Best: 55.49%\n",
            "Epoch 61 | T: 30s | Train: 2.9557 (32.6%) | Phys: 0.04 | Val: 1.7121 (56.25%) | Best: 56.25%\n",
            "Epoch 62 | T: 30s | Train: 2.9490 (32.8%) | Phys: 0.04 | Val: 1.6901 (56.21%) | Best: 56.25%\n",
            "Epoch 63 | T: 30s | Train: 2.9984 (31.7%) | Phys: 0.04 | Val: 1.7457 (55.71%) | Best: 56.25%\n",
            "Epoch 64 | T: 30s | Train: 2.9362 (32.9%) | Phys: 0.04 | Val: 1.7082 (56.14%) | Best: 56.25%\n",
            "Epoch 65 | T: 30s | Train: 2.9217 (33.5%) | Phys: 0.04 | Val: 1.6948 (56.89%) | Best: 56.89%\n",
            "Epoch 66 | T: 29s | Train: 2.9607 (32.8%) | Phys: 0.04 | Val: 1.6792 (56.33%) | Best: 56.89%\n",
            "Epoch 67 | T: 30s | Train: 2.9400 (33.0%) | Phys: 0.04 | Val: 1.6491 (57.49%) | Best: 57.49%\n",
            "Epoch 68 | T: 29s | Train: 2.8967 (34.2%) | Phys: 0.04 | Val: 1.6223 (57.51%) | Best: 57.51%\n",
            "Epoch 69 | T: 29s | Train: 2.8841 (34.5%) | Phys: 0.04 | Val: 1.6625 (57.11%) | Best: 57.51%\n",
            "Epoch 70 | T: 30s | Train: 2.8950 (34.1%) | Phys: 0.04 | Val: 1.7186 (55.97%) | Best: 57.51%\n",
            "Epoch 71 | T: 30s | Train: 2.8784 (34.6%) | Phys: 0.04 | Val: 1.6527 (57.09%) | Best: 57.51%\n",
            "Epoch 72 | T: 30s | Train: 2.9166 (33.6%) | Phys: 0.04 | Val: 1.6521 (57.75%) | Best: 57.75%\n",
            "Epoch 73 | T: 29s | Train: 2.8433 (35.2%) | Phys: 0.04 | Val: 1.6206 (57.98%) | Best: 57.98%\n",
            "Epoch 74 | T: 29s | Train: 2.9062 (34.1%) | Phys: 0.04 | Val: 1.6251 (57.98%) | Best: 57.98%\n",
            "Epoch 75 | T: 30s | Train: 2.8483 (35.5%) | Phys: 0.04 | Val: 1.6905 (56.94%) | Best: 57.98%\n",
            "Epoch 76 | T: 30s | Train: 2.8722 (34.8%) | Phys: 0.04 | Val: 1.6222 (57.71%) | Best: 57.98%\n",
            "Epoch 77 | T: 30s | Train: 2.8488 (35.1%) | Phys: 0.04 | Val: 1.6440 (57.58%) | Best: 57.98%\n",
            "Epoch 78 | T: 30s | Train: 2.8388 (35.5%) | Phys: 0.04 | Val: 1.6586 (57.09%) | Best: 57.98%\n",
            "Epoch 79 | T: 30s | Train: 2.8342 (35.6%) | Phys: 0.04 | Val: 1.5787 (58.72%) | Best: 58.72%\n",
            "Epoch 80 | T: 30s | Train: 2.8559 (35.3%) | Phys: 0.04 | Val: 1.5842 (58.55%) | Best: 58.72%\n",
            "Epoch 81 | T: 29s | Train: 2.8378 (35.5%) | Phys: 0.04 | Val: 1.6025 (58.50%) | Best: 58.72%\n",
            "Epoch 82 | T: 30s | Train: 2.7570 (37.3%) | Phys: 0.04 | Val: 1.5847 (58.79%) | Best: 58.79%\n",
            "Epoch 83 | T: 29s | Train: 2.7730 (37.1%) | Phys: 0.04 | Val: 1.5770 (59.20%) | Best: 59.20%\n",
            "Epoch 84 | T: 30s | Train: 2.8856 (34.7%) | Phys: 0.04 | Val: 1.6054 (58.24%) | Best: 59.20%\n",
            "Epoch 85 | T: 29s | Train: 2.7588 (37.4%) | Phys: 0.04 | Val: 1.5588 (59.42%) | Best: 59.42%\n",
            "Epoch 86 | T: 29s | Train: 2.7959 (36.8%) | Phys: 0.04 | Val: 1.5640 (59.24%) | Best: 59.42%\n",
            "Epoch 87 | T: 29s | Train: 2.8021 (36.7%) | Phys: 0.04 | Val: 1.5583 (60.13%) | Best: 60.13%\n",
            "Epoch 88 | T: 29s | Train: 2.7646 (37.4%) | Phys: 0.04 | Val: 1.4924 (60.48%) | Best: 60.48%\n",
            "Epoch 89 | T: 30s | Train: 2.8734 (35.1%) | Phys: 0.04 | Val: 1.5656 (59.46%) | Best: 60.48%\n",
            "Epoch 90 | T: 30s | Train: 2.7695 (37.4%) | Phys: 0.04 | Val: 1.5287 (60.01%) | Best: 60.48%\n",
            "Epoch 91 | T: 30s | Train: 2.7454 (37.6%) | Phys: 0.04 | Val: 1.5095 (60.27%) | Best: 60.48%\n",
            "Epoch 92 | T: 30s | Train: 2.7141 (38.3%) | Phys: 0.04 | Val: 1.5609 (59.96%) | Best: 60.48%\n",
            "Epoch 93 | T: 29s | Train: 2.7520 (37.7%) | Phys: 0.04 | Val: 1.5644 (59.52%) | Best: 60.48%\n",
            "Epoch 94 | T: 30s | Train: 2.7292 (38.5%) | Phys: 0.04 | Val: 1.5043 (60.99%) | Best: 60.99%\n",
            "Epoch 95 | T: 29s | Train: 2.7100 (38.5%) | Phys: 0.04 | Val: 1.5120 (60.50%) | Best: 60.99%\n",
            "Epoch 96 | T: 30s | Train: 2.7242 (38.3%) | Phys: 0.04 | Val: 1.5050 (60.44%) | Best: 60.99%\n",
            "Epoch 97 | T: 30s | Train: 2.7583 (37.6%) | Phys: 0.04 | Val: 1.5058 (60.10%) | Best: 60.99%\n",
            "Epoch 98 | T: 29s | Train: 2.7383 (38.3%) | Phys: 0.04 | Val: 1.5217 (60.33%) | Best: 60.99%\n",
            "Epoch 99 | T: 29s | Train: 2.7255 (38.6%) | Phys: 0.04 | Val: 1.5274 (60.62%) | Best: 60.99%\n",
            "Epoch 100 | T: 29s | Train: 2.7624 (37.6%) | Phys: 0.04 | Val: 1.4927 (61.48%) | Best: 61.48%\n",
            "Epoch 101 | T: 30s | Train: 2.7326 (38.8%) | Phys: 0.04 | Val: 1.5362 (59.60%) | Best: 61.48%\n",
            "Epoch 102 | T: 30s | Train: 2.7344 (38.6%) | Phys: 0.04 | Val: 1.4712 (61.80%) | Best: 61.80%\n",
            "Epoch 103 | T: 29s | Train: 2.8193 (36.4%) | Phys: 0.04 | Val: 1.4845 (61.40%) | Best: 61.80%\n",
            "Epoch 104 | T: 29s | Train: 2.7323 (38.5%) | Phys: 0.04 | Val: 1.4824 (61.03%) | Best: 61.80%\n",
            "Epoch 105 | T: 29s | Train: 2.6902 (39.4%) | Phys: 0.04 | Val: 1.4597 (61.50%) | Best: 61.80%\n",
            "Epoch 106 | T: 29s | Train: 2.7220 (38.6%) | Phys: 0.04 | Val: 1.4696 (61.54%) | Best: 61.80%\n",
            "Epoch 107 | T: 30s | Train: 2.6725 (39.7%) | Phys: 0.04 | Val: 1.4757 (61.67%) | Best: 61.80%\n",
            "Epoch 108 | T: 29s | Train: 2.6869 (39.3%) | Phys: 0.04 | Val: 1.4694 (61.75%) | Best: 61.80%\n",
            "Epoch 109 | T: 30s | Train: 2.6737 (39.7%) | Phys: 0.04 | Val: 1.4638 (61.98%) | Best: 61.98%\n",
            "Epoch 110 | T: 29s | Train: 2.6484 (40.4%) | Phys: 0.04 | Val: 1.4494 (62.23%) | Best: 62.23%\n",
            "Epoch 111 | T: 29s | Train: 2.6851 (39.8%) | Phys: 0.04 | Val: 1.4356 (62.23%) | Best: 62.23%\n",
            "Epoch 112 | T: 29s | Train: 2.6497 (40.3%) | Phys: 0.04 | Val: 1.4130 (63.67%) | Best: 63.67%\n",
            "Epoch 113 | T: 30s | Train: 2.6366 (40.7%) | Phys: 0.04 | Val: 1.4274 (62.48%) | Best: 63.67%\n",
            "Epoch 114 | T: 29s | Train: 2.6153 (41.1%) | Phys: 0.04 | Val: 1.4537 (61.67%) | Best: 63.67%\n",
            "Epoch 115 | T: 30s | Train: 2.6098 (41.7%) | Phys: 0.04 | Val: 1.4738 (62.40%) | Best: 63.67%\n",
            "Epoch 116 | T: 29s | Train: 2.6575 (40.3%) | Phys: 0.04 | Val: 1.4650 (61.95%) | Best: 63.67%\n",
            "Epoch 117 | T: 29s | Train: 2.6856 (39.8%) | Phys: 0.04 | Val: 1.4536 (62.43%) | Best: 63.67%\n",
            "Epoch 118 | T: 29s | Train: 2.6742 (39.9%) | Phys: 0.04 | Val: 1.4131 (63.02%) | Best: 63.67%\n",
            "Epoch 119 | T: 29s | Train: 2.6238 (41.0%) | Phys: 0.04 | Val: 1.4703 (61.37%) | Best: 63.67%\n",
            "Epoch 120 | T: 29s | Train: 2.5941 (41.9%) | Phys: 0.04 | Val: 1.4043 (63.08%) | Best: 63.67%\n",
            "Epoch 121 | T: 29s | Train: 2.5990 (41.8%) | Phys: 0.04 | Val: 1.4105 (62.94%) | Best: 63.67%\n",
            "Epoch 122 | T: 29s | Train: 2.5652 (42.4%) | Phys: 0.04 | Val: 1.4441 (62.62%) | Best: 63.67%\n",
            "Epoch 123 | T: 29s | Train: 2.5890 (41.9%) | Phys: 0.04 | Val: 1.3817 (63.75%) | Best: 63.75%\n",
            "Epoch 124 | T: 29s | Train: 2.5803 (42.0%) | Phys: 0.04 | Val: 1.3791 (64.24%) | Best: 64.24%\n",
            "Epoch 125 | T: 30s | Train: 2.5962 (41.7%) | Phys: 0.04 | Val: 1.3910 (63.45%) | Best: 64.24%\n",
            "Epoch 126 | T: 29s | Train: 2.4989 (44.5%) | Phys: 0.04 | Val: 1.3835 (63.08%) | Best: 64.24%\n",
            "Epoch 127 | T: 29s | Train: 2.5446 (43.1%) | Phys: 0.04 | Val: 1.3911 (63.77%) | Best: 64.24%\n",
            "Epoch 128 | T: 30s | Train: 2.6253 (41.3%) | Phys: 0.04 | Val: 1.3656 (64.23%) | Best: 64.24%\n",
            "Epoch 129 | T: 30s | Train: 2.6019 (41.9%) | Phys: 0.04 | Val: 1.4270 (62.84%) | Best: 64.24%\n",
            "Epoch 130 | T: 29s | Train: 2.5438 (43.2%) | Phys: 0.04 | Val: 1.3536 (63.89%) | Best: 64.24%\n",
            "Epoch 131 | T: 29s | Train: 2.6103 (41.8%) | Phys: 0.04 | Val: 1.3903 (64.13%) | Best: 64.24%\n",
            "Epoch 132 | T: 30s | Train: 2.4989 (43.9%) | Phys: 0.04 | Val: 1.3816 (64.11%) | Best: 64.24%\n",
            "Epoch 133 | T: 29s | Train: 2.5268 (43.5%) | Phys: 0.04 | Val: 1.3488 (64.54%) | Best: 64.54%\n",
            "Epoch 134 | T: 29s | Train: 2.5288 (43.7%) | Phys: 0.04 | Val: 1.3699 (64.15%) | Best: 64.54%\n",
            "Epoch 135 | T: 30s | Train: 2.5697 (42.5%) | Phys: 0.04 | Val: 1.3758 (64.25%) | Best: 64.54%\n",
            "Epoch 136 | T: 29s | Train: 2.5015 (44.5%) | Phys: 0.04 | Val: 1.3690 (64.48%) | Best: 64.54%\n",
            "Epoch 137 | T: 29s | Train: 2.6301 (41.2%) | Phys: 0.04 | Val: 1.3671 (64.00%) | Best: 64.54%\n",
            "Epoch 138 | T: 29s | Train: 2.5429 (43.2%) | Phys: 0.04 | Val: 1.4063 (63.49%) | Best: 64.54%\n",
            "Epoch 139 | T: 29s | Train: 2.6138 (42.0%) | Phys: 0.04 | Val: 1.3711 (63.64%) | Best: 64.54%\n",
            "Epoch 140 | T: 29s | Train: 2.5612 (42.9%) | Phys: 0.04 | Val: 1.3492 (64.31%) | Best: 64.54%\n",
            "Epoch 141 | T: 29s | Train: 2.5770 (42.3%) | Phys: 0.04 | Val: 1.3754 (63.93%) | Best: 64.54%\n",
            "Epoch 142 | T: 29s | Train: 2.5349 (43.3%) | Phys: 0.04 | Val: 1.3720 (64.59%) | Best: 64.59%\n",
            "Epoch 143 | T: 30s | Train: 2.5431 (43.4%) | Phys: 0.04 | Val: 1.4109 (62.86%) | Best: 64.59%\n",
            "Epoch 144 | T: 29s | Train: 2.5286 (43.8%) | Phys: 0.04 | Val: 1.3421 (64.91%) | Best: 64.91%\n",
            "Epoch 145 | T: 30s | Train: 2.5332 (43.6%) | Phys: 0.04 | Val: 1.3776 (64.04%) | Best: 64.91%\n",
            "Epoch 146 | T: 29s | Train: 2.5581 (43.0%) | Phys: 0.04 | Val: 1.3938 (63.77%) | Best: 64.91%\n",
            "Epoch 147 | T: 30s | Train: 2.5275 (43.6%) | Phys: 0.04 | Val: 1.3576 (64.65%) | Best: 64.91%\n",
            "Epoch 148 | T: 30s | Train: 2.4778 (45.1%) | Phys: 0.04 | Val: 1.3625 (64.13%) | Best: 64.91%\n",
            "Epoch 149 | T: 29s | Train: 2.5034 (44.3%) | Phys: 0.04 | Val: 1.3238 (65.08%) | Best: 65.08%\n",
            "Epoch 150 | T: 30s | Train: 2.5269 (43.8%) | Phys: 0.04 | Val: 1.3281 (65.28%) | Best: 65.28%\n",
            "Epoch 151 | T: 29s | Train: 2.4985 (44.6%) | Phys: 0.04 | Val: 1.3341 (65.45%) | Best: 65.45%\n",
            "Epoch 152 | T: 30s | Train: 2.5519 (43.2%) | Phys: 0.04 | Val: 1.3220 (65.26%) | Best: 65.45%\n",
            "Epoch 153 | T: 30s | Train: 2.4945 (44.5%) | Phys: 0.04 | Val: 1.3346 (64.81%) | Best: 65.45%\n",
            "Epoch 154 | T: 29s | Train: 2.4964 (44.5%) | Phys: 0.04 | Val: 1.2901 (65.86%) | Best: 65.86%\n",
            "Epoch 155 | T: 29s | Train: 2.4469 (45.9%) | Phys: 0.04 | Val: 1.3212 (65.42%) | Best: 65.86%\n",
            "Epoch 156 | T: 30s | Train: 2.4422 (45.9%) | Phys: 0.04 | Val: 1.3286 (64.98%) | Best: 65.86%\n",
            "Epoch 157 | T: 30s | Train: 2.4280 (46.4%) | Phys: 0.04 | Val: 1.3117 (64.52%) | Best: 65.86%\n",
            "Epoch 158 | T: 30s | Train: 2.4926 (44.7%) | Phys: 0.04 | Val: 1.3270 (64.89%) | Best: 65.86%\n",
            "Epoch 159 | T: 30s | Train: 2.4800 (44.8%) | Phys: 0.04 | Val: 1.3193 (65.22%) | Best: 65.86%\n",
            "Epoch 160 | T: 30s | Train: 2.5006 (44.3%) | Phys: 0.04 | Val: 1.3160 (65.22%) | Best: 65.86%\n",
            "Epoch 161 | T: 30s | Train: 2.4565 (45.7%) | Phys: 0.04 | Val: 1.3260 (65.39%) | Best: 65.86%\n",
            "Epoch 162 | T: 30s | Train: 2.4997 (44.5%) | Phys: 0.04 | Val: 1.3042 (65.72%) | Best: 65.86%\n",
            "Epoch 163 | T: 30s | Train: 2.4652 (45.4%) | Phys: 0.04 | Val: 1.3252 (65.46%) | Best: 65.86%\n",
            "Epoch 164 | T: 29s | Train: 2.4440 (46.0%) | Phys: 0.04 | Val: 1.2852 (66.26%) | Best: 66.26%\n",
            "Epoch 165 | T: 29s | Train: 2.4203 (46.6%) | Phys: 0.04 | Val: 1.2808 (65.60%) | Best: 66.26%\n",
            "Epoch 166 | T: 30s | Train: 2.4189 (46.4%) | Phys: 0.04 | Val: 1.3035 (65.49%) | Best: 66.26%\n",
            "Epoch 167 | T: 30s | Train: 2.4714 (45.1%) | Phys: 0.04 | Val: 1.3070 (65.88%) | Best: 66.26%\n",
            "Epoch 168 | T: 29s | Train: 2.4350 (46.2%) | Phys: 0.04 | Val: 1.2915 (66.29%) | Best: 66.29%\n",
            "Epoch 169 | T: 29s | Train: 2.4375 (46.2%) | Phys: 0.04 | Val: 1.3165 (65.69%) | Best: 66.29%\n",
            "Epoch 170 | T: 30s | Train: 2.4032 (47.0%) | Phys: 0.04 | Val: 1.3180 (65.82%) | Best: 66.29%\n",
            "Epoch 171 | T: 30s | Train: 2.4212 (46.9%) | Phys: 0.04 | Val: 1.2757 (66.24%) | Best: 66.29%\n",
            "Epoch 172 | T: 30s | Train: 2.4114 (46.6%) | Phys: 0.04 | Val: 1.3159 (65.55%) | Best: 66.29%\n",
            "Epoch 173 | T: 30s | Train: 2.3853 (47.6%) | Phys: 0.04 | Val: 1.3110 (65.88%) | Best: 66.29%\n",
            "Epoch 174 | T: 30s | Train: 2.4483 (46.0%) | Phys: 0.04 | Val: 1.2896 (66.46%) | Best: 66.46%\n",
            "Epoch 175 | T: 30s | Train: 2.3945 (47.1%) | Phys: 0.04 | Val: 1.2699 (66.33%) | Best: 66.46%\n",
            "Epoch 176 | T: 30s | Train: 2.4144 (46.7%) | Phys: 0.04 | Val: 1.2771 (66.63%) | Best: 66.63%\n",
            "Epoch 177 | T: 30s | Train: 2.3942 (47.6%) | Phys: 0.04 | Val: 1.2884 (66.42%) | Best: 66.63%\n",
            "Epoch 178 | T: 30s | Train: 2.3949 (47.2%) | Phys: 0.04 | Val: 1.2769 (66.66%) | Best: 66.66%\n",
            "Epoch 179 | T: 29s | Train: 2.3721 (47.8%) | Phys: 0.04 | Val: 1.2807 (66.37%) | Best: 66.66%\n",
            "Epoch 180 | T: 30s | Train: 2.3932 (47.4%) | Phys: 0.04 | Val: 1.2710 (66.79%) | Best: 66.79%\n",
            "Epoch 181 | T: 30s | Train: 2.4521 (45.9%) | Phys: 0.04 | Val: 1.2487 (67.33%) | Best: 67.33%\n",
            "Epoch 182 | T: 30s | Train: 2.3729 (47.9%) | Phys: 0.04 | Val: 1.2820 (66.49%) | Best: 67.33%\n",
            "Epoch 183 | T: 30s | Train: 2.3129 (49.7%) | Phys: 0.04 | Val: 1.2868 (66.04%) | Best: 67.33%\n",
            "Epoch 184 | T: 30s | Train: 2.4130 (47.1%) | Phys: 0.04 | Val: 1.2883 (66.69%) | Best: 67.33%\n",
            "Epoch 185 | T: 30s | Train: 2.3820 (47.8%) | Phys: 0.04 | Val: 1.2882 (66.43%) | Best: 67.33%\n",
            "Epoch 186 | T: 29s | Train: 2.3325 (49.0%) | Phys: 0.04 | Val: 1.2678 (66.85%) | Best: 67.33%\n",
            "Epoch 187 | T: 30s | Train: 2.3745 (47.6%) | Phys: 0.04 | Val: 1.2805 (66.42%) | Best: 67.33%\n",
            "Epoch 188 | T: 30s | Train: 2.3876 (47.5%) | Phys: 0.04 | Val: 1.2967 (66.25%) | Best: 67.33%\n",
            "Epoch 189 | T: 29s | Train: 2.3223 (49.0%) | Phys: 0.04 | Val: 1.2898 (66.19%) | Best: 67.33%\n",
            "Epoch 190 | T: 30s | Train: 2.3650 (48.4%) | Phys: 0.04 | Val: 1.3105 (65.54%) | Best: 67.33%\n",
            "Epoch 191 | T: 29s | Train: 2.3003 (49.9%) | Phys: 0.04 | Val: 1.2465 (67.54%) | Best: 67.54%\n",
            "Epoch 192 | T: 30s | Train: 2.3454 (48.6%) | Phys: 0.04 | Val: 1.2540 (67.62%) | Best: 67.62%\n",
            "Epoch 193 | T: 29s | Train: 2.3962 (47.3%) | Phys: 0.04 | Val: 1.2679 (67.05%) | Best: 67.62%\n",
            "Epoch 194 | T: 30s | Train: 2.3892 (47.6%) | Phys: 0.04 | Val: 1.2818 (66.24%) | Best: 67.62%\n",
            "Epoch 195 | T: 30s | Train: 2.3713 (48.2%) | Phys: 0.04 | Val: 1.2477 (67.53%) | Best: 67.62%\n",
            "Epoch 196 | T: 29s | Train: 2.3366 (48.8%) | Phys: 0.04 | Val: 1.2312 (67.91%) | Best: 67.91%\n",
            "Epoch 197 | T: 29s | Train: 2.3594 (48.5%) | Phys: 0.04 | Val: 1.2617 (67.43%) | Best: 67.91%\n",
            "Epoch 198 | T: 30s | Train: 2.3749 (48.2%) | Phys: 0.04 | Val: 1.2574 (67.22%) | Best: 67.91%\n",
            "Epoch 199 | T: 29s | Train: 2.3655 (48.0%) | Phys: 0.04 | Val: 1.2690 (66.50%) | Best: 67.91%\n",
            "Epoch 200 | T: 29s | Train: 2.3166 (49.8%) | Phys: 0.04 | Val: 1.2534 (66.96%) | Best: 67.91%\n",
            "Epoch 201 | T: 30s | Train: 2.4150 (47.0%) | Phys: 0.04 | Val: 1.2599 (67.06%) | Best: 67.91%\n",
            "Epoch 202 | T: 29s | Train: 2.3706 (48.2%) | Phys: 0.04 | Val: 1.3060 (66.24%) | Best: 67.91%\n",
            "Epoch 203 | T: 30s | Train: 2.3003 (49.7%) | Phys: 0.04 | Val: 1.2266 (67.61%) | Best: 67.91%\n",
            "Epoch 204 | T: 30s | Train: 2.2763 (50.2%) | Phys: 0.04 | Val: 1.2624 (66.98%) | Best: 67.91%\n",
            "Epoch 205 | T: 30s | Train: 2.2872 (50.3%) | Phys: 0.04 | Val: 1.2205 (67.96%) | Best: 67.96%\n",
            "Epoch 206 | T: 30s | Train: 2.3212 (49.4%) | Phys: 0.04 | Val: 1.2365 (68.09%) | Best: 68.09%\n",
            "Epoch 207 | T: 30s | Train: 2.2576 (51.0%) | Phys: 0.04 | Val: 1.2469 (67.35%) | Best: 68.09%\n",
            "Epoch 208 | T: 30s | Train: 2.3638 (48.6%) | Phys: 0.04 | Val: 1.2572 (67.37%) | Best: 68.09%\n",
            "Epoch 209 | T: 29s | Train: 2.2808 (50.5%) | Phys: 0.04 | Val: 1.2528 (67.32%) | Best: 68.09%\n",
            "Epoch 210 | T: 29s | Train: 2.3241 (49.6%) | Phys: 0.04 | Val: 1.2485 (67.28%) | Best: 68.09%\n",
            "Epoch 211 | T: 30s | Train: 2.3625 (48.6%) | Phys: 0.04 | Val: 1.2370 (68.02%) | Best: 68.09%\n",
            "Epoch 212 | T: 29s | Train: 2.2368 (51.5%) | Phys: 0.04 | Val: 1.2541 (67.70%) | Best: 68.09%\n",
            "Epoch 213 | T: 30s | Train: 2.3491 (49.2%) | Phys: 0.04 | Val: 1.2773 (66.95%) | Best: 68.09%\n",
            "Epoch 214 | T: 30s | Train: 2.3288 (49.3%) | Phys: 0.04 | Val: 1.2436 (67.57%) | Best: 68.09%\n",
            "Epoch 215 | T: 30s | Train: 2.2942 (50.1%) | Phys: 0.04 | Val: 1.2419 (67.44%) | Best: 68.09%\n",
            "Epoch 216 | T: 30s | Train: 2.2995 (50.2%) | Phys: 0.04 | Val: 1.2364 (68.00%) | Best: 68.09%\n",
            "Epoch 217 | T: 29s | Train: 2.2489 (51.5%) | Phys: 0.04 | Val: 1.2517 (66.96%) | Best: 68.09%\n",
            "Epoch 218 | T: 30s | Train: 2.3183 (49.4%) | Phys: 0.04 | Val: 1.2784 (67.05%) | Best: 68.09%\n",
            "Epoch 219 | T: 29s | Train: 2.2467 (51.3%) | Phys: 0.04 | Val: 1.2143 (68.35%) | Best: 68.35%\n",
            "Epoch 220 | T: 29s | Train: 2.2079 (52.5%) | Phys: 0.04 | Val: 1.2156 (68.78%) | Best: 68.78%\n",
            "Epoch 221 | T: 30s | Train: 2.2110 (52.4%) | Phys: 0.04 | Val: 1.2307 (67.94%) | Best: 68.78%\n",
            "Epoch 222 | T: 30s | Train: 2.2762 (50.8%) | Phys: 0.04 | Val: 1.2507 (67.58%) | Best: 68.78%\n",
            "Epoch 223 | T: 30s | Train: 2.2683 (50.9%) | Phys: 0.04 | Val: 1.2277 (68.05%) | Best: 68.78%\n",
            "Epoch 224 | T: 30s | Train: 2.3021 (50.1%) | Phys: 0.04 | Val: 1.2471 (67.77%) | Best: 68.78%\n",
            "Epoch 225 | T: 29s | Train: 2.1823 (53.1%) | Phys: 0.04 | Val: 1.2337 (67.72%) | Best: 68.78%\n",
            "Epoch 226 | T: 30s | Train: 2.2770 (50.8%) | Phys: 0.04 | Val: 1.2585 (67.88%) | Best: 68.78%\n",
            "Epoch 227 | T: 29s | Train: 2.2025 (52.7%) | Phys: 0.04 | Val: 1.2142 (68.15%) | Best: 68.78%\n",
            "Epoch 228 | T: 30s | Train: 2.1931 (53.0%) | Phys: 0.04 | Val: 1.1991 (68.65%) | Best: 68.78%\n",
            "Epoch 229 | T: 30s | Train: 2.2406 (51.7%) | Phys: 0.04 | Val: 1.2265 (68.58%) | Best: 68.78%\n",
            "Epoch 230 | T: 29s | Train: 2.2727 (51.0%) | Phys: 0.04 | Val: 1.2369 (68.03%) | Best: 68.78%\n",
            "Epoch 231 | T: 29s | Train: 2.2742 (50.7%) | Phys: 0.04 | Val: 1.2392 (68.37%) | Best: 68.78%\n",
            "Epoch 232 | T: 30s | Train: 2.2143 (52.3%) | Phys: 0.04 | Val: 1.2325 (68.47%) | Best: 68.78%\n",
            "Epoch 233 | T: 29s | Train: 2.2826 (50.7%) | Phys: 0.04 | Val: 1.2291 (68.29%) | Best: 68.78%\n",
            "Epoch 234 | T: 29s | Train: 2.2108 (52.4%) | Phys: 0.04 | Val: 1.2232 (68.42%) | Best: 68.78%\n",
            "Epoch 235 | T: 29s | Train: 2.2115 (52.6%) | Phys: 0.04 | Val: 1.2279 (68.02%) | Best: 68.78%\n",
            "Epoch 236 | T: 30s | Train: 2.1994 (52.8%) | Phys: 0.04 | Val: 1.2233 (68.55%) | Best: 68.78%\n",
            "Epoch 237 | T: 29s | Train: 2.2121 (52.1%) | Phys: 0.04 | Val: 1.2270 (68.56%) | Best: 68.78%\n",
            "Epoch 238 | T: 29s | Train: 2.2810 (50.8%) | Phys: 0.04 | Val: 1.2127 (68.34%) | Best: 68.78%\n",
            "Epoch 239 | T: 29s | Train: 2.1392 (54.2%) | Phys: 0.04 | Val: 1.1796 (69.37%) | Best: 69.37%\n",
            "Epoch 240 | T: 30s | Train: 2.2759 (51.0%) | Phys: 0.04 | Val: 1.2453 (68.36%) | Best: 69.37%\n",
            "Epoch 241 | T: 30s | Train: 2.1125 (54.7%) | Phys: 0.04 | Val: 1.2027 (69.24%) | Best: 69.37%\n",
            "Epoch 242 | T: 30s | Train: 2.1916 (53.0%) | Phys: 0.04 | Val: 1.2192 (69.02%) | Best: 69.37%\n",
            "Epoch 243 | T: 30s | Train: 2.2255 (52.3%) | Phys: 0.04 | Val: 1.1962 (69.69%) | Best: 69.69%\n",
            "Epoch 244 | T: 30s | Train: 2.2347 (52.0%) | Phys: 0.04 | Val: 1.2139 (69.07%) | Best: 69.69%\n",
            "Epoch 245 | T: 30s | Train: 2.2076 (52.5%) | Phys: 0.04 | Val: 1.1941 (69.23%) | Best: 69.69%\n",
            "Epoch 246 | T: 30s | Train: 2.1262 (54.5%) | Phys: 0.04 | Val: 1.2008 (68.93%) | Best: 69.69%\n",
            "Epoch 247 | T: 29s | Train: 2.1988 (52.4%) | Phys: 0.04 | Val: 1.1873 (69.52%) | Best: 69.69%\n",
            "Epoch 248 | T: 30s | Train: 2.1662 (53.5%) | Phys: 0.04 | Val: 1.1847 (69.11%) | Best: 69.69%\n",
            "Epoch 249 | T: 30s | Train: 2.2272 (51.7%) | Phys: 0.04 | Val: 1.1927 (69.14%) | Best: 69.69%\n",
            "Epoch 250 | T: 30s | Train: 2.1939 (52.9%) | Phys: 0.04 | Val: 1.1979 (69.24%) | Best: 69.69%\n",
            "Epoch 251 | T: 30s | Train: 2.1615 (53.5%) | Phys: 0.04 | Val: 1.2007 (69.02%) | Best: 69.69%\n",
            "Epoch 252 | T: 29s | Train: 2.1276 (54.8%) | Phys: 0.04 | Val: 1.1827 (69.48%) | Best: 69.69%\n",
            "Epoch 253 | T: 30s | Train: 2.1062 (55.1%) | Phys: 0.04 | Val: 1.1804 (69.76%) | Best: 69.76%\n",
            "Epoch 254 | T: 29s | Train: 2.1520 (53.8%) | Phys: 0.04 | Val: 1.1781 (70.04%) | Best: 70.04%\n",
            "Epoch 255 | T: 29s | Train: 2.1755 (53.8%) | Phys: 0.04 | Val: 1.1959 (69.37%) | Best: 70.04%\n",
            "Epoch 256 | T: 30s | Train: 2.1574 (53.8%) | Phys: 0.04 | Val: 1.1709 (69.57%) | Best: 70.04%\n",
            "Epoch 257 | T: 30s | Train: 2.2295 (52.1%) | Phys: 0.04 | Val: 1.2110 (69.21%) | Best: 70.04%\n",
            "Epoch 258 | T: 29s | Train: 2.1986 (53.0%) | Phys: 0.04 | Val: 1.1841 (69.78%) | Best: 70.04%\n",
            "Epoch 259 | T: 29s | Train: 2.1219 (54.9%) | Phys: 0.04 | Val: 1.1829 (69.62%) | Best: 70.04%\n",
            "Epoch 260 | T: 29s | Train: 2.1627 (53.8%) | Phys: 0.04 | Val: 1.2025 (69.26%) | Best: 70.04%\n",
            "Epoch 261 | T: 30s | Train: 2.0763 (56.1%) | Phys: 0.04 | Val: 1.1902 (69.53%) | Best: 70.04%\n",
            "Epoch 262 | T: 29s | Train: 2.1928 (53.0%) | Phys: 0.05 | Val: 1.2087 (69.34%) | Best: 70.04%\n",
            "Epoch 263 | T: 30s | Train: 2.1496 (53.9%) | Phys: 0.04 | Val: 1.2012 (69.77%) | Best: 70.04%\n",
            "Epoch 264 | T: 29s | Train: 2.1372 (54.6%) | Phys: 0.04 | Val: 1.1839 (69.93%) | Best: 70.04%\n",
            "Epoch 265 | T: 30s | Train: 2.1958 (52.7%) | Phys: 0.05 | Val: 1.1956 (69.64%) | Best: 70.04%\n",
            "Epoch 266 | T: 29s | Train: 2.1091 (55.2%) | Phys: 0.04 | Val: 1.1894 (69.50%) | Best: 70.04%\n",
            "Epoch 267 | T: 30s | Train: 2.1544 (54.1%) | Phys: 0.04 | Val: 1.1582 (70.42%) | Best: 70.42%\n",
            "Epoch 268 | T: 29s | Train: 2.0285 (57.4%) | Phys: 0.04 | Val: 1.1751 (69.95%) | Best: 70.42%\n",
            "Epoch 269 | T: 29s | Train: 2.1624 (53.7%) | Phys: 0.05 | Val: 1.1799 (70.20%) | Best: 70.42%\n",
            "Epoch 270 | T: 30s | Train: 2.1594 (53.7%) | Phys: 0.04 | Val: 1.1678 (70.44%) | Best: 70.44%\n",
            "Epoch 271 | T: 30s | Train: 2.1467 (54.2%) | Phys: 0.04 | Val: 1.1713 (70.09%) | Best: 70.44%\n",
            "Epoch 272 | T: 29s | Train: 2.0904 (55.4%) | Phys: 0.05 | Val: 1.1807 (70.20%) | Best: 70.44%\n",
            "Epoch 273 | T: 30s | Train: 2.1322 (54.6%) | Phys: 0.05 | Val: 1.1833 (70.04%) | Best: 70.44%\n",
            "Epoch 274 | T: 30s | Train: 2.0883 (55.5%) | Phys: 0.04 | Val: 1.1601 (70.14%) | Best: 70.44%\n",
            "Epoch 275 | T: 30s | Train: 2.1657 (53.6%) | Phys: 0.05 | Val: 1.1967 (69.13%) | Best: 70.44%\n",
            "Epoch 276 | T: 30s | Train: 2.0788 (55.8%) | Phys: 0.04 | Val: 1.1660 (70.16%) | Best: 70.44%\n",
            "Epoch 277 | T: 29s | Train: 2.0961 (55.4%) | Phys: 0.05 | Val: 1.1757 (70.11%) | Best: 70.44%\n",
            "Epoch 278 | T: 30s | Train: 2.0704 (56.3%) | Phys: 0.05 | Val: 1.1579 (70.27%) | Best: 70.44%\n",
            "Epoch 279 | T: 30s | Train: 2.0414 (57.1%) | Phys: 0.04 | Val: 1.1622 (70.37%) | Best: 70.44%\n",
            "Epoch 280 | T: 29s | Train: 2.1307 (54.4%) | Phys: 0.05 | Val: 1.1904 (69.69%) | Best: 70.44%\n",
            "Epoch 281 | T: 29s | Train: 2.0479 (56.8%) | Phys: 0.04 | Val: 1.1755 (70.19%) | Best: 70.44%\n",
            "Epoch 282 | T: 29s | Train: 2.0538 (56.2%) | Phys: 0.05 | Val: 1.1490 (70.65%) | Best: 70.65%\n",
            "Epoch 283 | T: 29s | Train: 2.0758 (56.0%) | Phys: 0.05 | Val: 1.1643 (70.41%) | Best: 70.65%\n",
            "Epoch 284 | T: 29s | Train: 2.0678 (56.5%) | Phys: 0.05 | Val: 1.1456 (71.09%) | Best: 71.09%\n",
            "Epoch 285 | T: 29s | Train: 1.9970 (58.1%) | Phys: 0.04 | Val: 1.1633 (70.33%) | Best: 71.09%\n",
            "Epoch 286 | T: 30s | Train: 2.0298 (57.2%) | Phys: 0.05 | Val: 1.1620 (70.37%) | Best: 71.09%\n",
            "Epoch 287 | T: 30s | Train: 2.0147 (57.9%) | Phys: 0.05 | Val: 1.1482 (70.66%) | Best: 71.09%\n",
            "Epoch 288 | T: 30s | Train: 2.1240 (54.8%) | Phys: 0.05 | Val: 1.1396 (71.11%) | Best: 71.11%\n",
            "Epoch 289 | T: 30s | Train: 2.1495 (54.0%) | Phys: 0.05 | Val: 1.1588 (70.76%) | Best: 71.11%\n",
            "Epoch 290 | T: 30s | Train: 2.0962 (55.6%) | Phys: 0.05 | Val: 1.1725 (70.26%) | Best: 71.11%\n",
            "Epoch 291 | T: 30s | Train: 2.1070 (55.3%) | Phys: 0.05 | Val: 1.1635 (70.28%) | Best: 71.11%\n",
            "Epoch 292 | T: 29s | Train: 2.0612 (56.5%) | Phys: 0.05 | Val: 1.1537 (70.42%) | Best: 71.11%\n",
            "Epoch 293 | T: 29s | Train: 2.0988 (55.5%) | Phys: 0.05 | Val: 1.1509 (70.41%) | Best: 71.11%\n",
            "Epoch 294 | T: 30s | Train: 2.1236 (54.9%) | Phys: 0.05 | Val: 1.1551 (70.41%) | Best: 71.11%\n",
            "Epoch 295 | T: 29s | Train: 2.0305 (57.3%) | Phys: 0.05 | Val: 1.1610 (70.91%) | Best: 71.11%\n",
            "Epoch 296 | T: 30s | Train: 2.0106 (57.9%) | Phys: 0.05 | Val: 1.1539 (70.99%) | Best: 71.11%\n",
            "Epoch 297 | T: 30s | Train: 2.0866 (55.8%) | Phys: 0.05 | Val: 1.1533 (70.91%) | Best: 71.11%\n",
            "Epoch 298 | T: 30s | Train: 2.0144 (58.0%) | Phys: 0.05 | Val: 1.1480 (71.27%) | Best: 71.27%\n",
            "Epoch 299 | T: 30s | Train: 2.0878 (55.8%) | Phys: 0.05 | Val: 1.1384 (70.95%) | Best: 71.27%\n",
            "Epoch 300 | T: 30s | Train: 2.0500 (57.0%) | Phys: 0.05 | Val: 1.1607 (70.55%) | Best: 71.27%\n",
            "Epoch 301 | T: 29s | Train: 1.9926 (58.2%) | Phys: 0.05 | Val: 1.1634 (70.53%) | Best: 71.27%\n",
            "Epoch 302 | T: 30s | Train: 2.0060 (57.9%) | Phys: 0.05 | Val: 1.1598 (70.50%) | Best: 71.27%\n",
            "Epoch 303 | T: 30s | Train: 1.9631 (59.0%) | Phys: 0.05 | Val: 1.1551 (70.63%) | Best: 71.27%\n",
            "Epoch 304 | T: 30s | Train: 2.0592 (56.5%) | Phys: 0.05 | Val: 1.1550 (70.61%) | Best: 71.27%\n",
            "Epoch 305 | T: 30s | Train: 2.0094 (57.7%) | Phys: 0.05 | Val: 1.1600 (70.54%) | Best: 71.27%\n",
            "Epoch 306 | T: 30s | Train: 1.9891 (58.5%) | Phys: 0.05 | Val: 1.1319 (71.33%) | Best: 71.33%\n",
            "Epoch 307 | T: 30s | Train: 1.9884 (58.7%) | Phys: 0.05 | Val: 1.1519 (70.64%) | Best: 71.33%\n",
            "Epoch 308 | T: 30s | Train: 2.0053 (58.1%) | Phys: 0.05 | Val: 1.1461 (70.89%) | Best: 71.33%\n",
            "Epoch 309 | T: 29s | Train: 2.0537 (56.7%) | Phys: 0.05 | Val: 1.1543 (71.05%) | Best: 71.33%\n",
            "Epoch 310 | T: 30s | Train: 1.9790 (58.9%) | Phys: 0.05 | Val: 1.1421 (71.27%) | Best: 71.33%\n",
            "Epoch 311 | T: 30s | Train: 2.0836 (56.0%) | Phys: 0.05 | Val: 1.1353 (71.14%) | Best: 71.33%\n",
            "Epoch 312 | T: 29s | Train: 2.0493 (56.8%) | Phys: 0.05 | Val: 1.1304 (70.83%) | Best: 71.33%\n",
            "Epoch 313 | T: 29s | Train: 1.9820 (58.4%) | Phys: 0.05 | Val: 1.1444 (71.05%) | Best: 71.33%\n",
            "Epoch 314 | T: 30s | Train: 1.9736 (58.8%) | Phys: 0.05 | Val: 1.1453 (70.88%) | Best: 71.33%\n",
            "Epoch 315 | T: 30s | Train: 2.0314 (57.1%) | Phys: 0.05 | Val: 1.1442 (70.95%) | Best: 71.33%\n",
            "Epoch 316 | T: 30s | Train: 1.9761 (58.9%) | Phys: 0.05 | Val: 1.1317 (71.14%) | Best: 71.33%\n",
            "Epoch 317 | T: 29s | Train: 2.0928 (55.5%) | Phys: 0.05 | Val: 1.1296 (70.99%) | Best: 71.33%\n",
            "Epoch 318 | T: 30s | Train: 2.0264 (57.3%) | Phys: 0.05 | Val: 1.1277 (71.29%) | Best: 71.33%\n",
            "Epoch 319 | T: 30s | Train: 1.9190 (60.4%) | Phys: 0.05 | Val: 1.1299 (71.40%) | Best: 71.40%\n",
            "Epoch 320 | T: 30s | Train: 2.0130 (57.9%) | Phys: 0.05 | Val: 1.1216 (71.26%) | Best: 71.40%\n",
            "Epoch 321 | T: 30s | Train: 1.9172 (60.1%) | Phys: 0.05 | Val: 1.1439 (70.81%) | Best: 71.40%\n",
            "Epoch 322 | T: 30s | Train: 1.9026 (60.1%) | Phys: 0.05 | Val: 1.1372 (71.04%) | Best: 71.40%\n",
            "Epoch 323 | T: 30s | Train: 1.9357 (59.7%) | Phys: 0.05 | Val: 1.1340 (71.40%) | Best: 71.40%\n",
            "Epoch 324 | T: 30s | Train: 1.9884 (58.3%) | Phys: 0.05 | Val: 1.1321 (71.27%) | Best: 71.40%\n",
            "Epoch 325 | T: 30s | Train: 2.0058 (57.7%) | Phys: 0.05 | Val: 1.1251 (71.31%) | Best: 71.40%\n",
            "Epoch 326 | T: 29s | Train: 2.0396 (56.9%) | Phys: 0.05 | Val: 1.1422 (70.94%) | Best: 71.40%\n",
            "Epoch 327 | T: 29s | Train: 1.9983 (58.1%) | Phys: 0.05 | Val: 1.1385 (71.47%) | Best: 71.47%\n",
            "Epoch 328 | T: 30s | Train: 2.0352 (57.0%) | Phys: 0.05 | Val: 1.1350 (71.60%) | Best: 71.60%\n",
            "Epoch 329 | T: 30s | Train: 1.9709 (58.9%) | Phys: 0.05 | Val: 1.1352 (71.48%) | Best: 71.60%\n",
            "Epoch 330 | T: 29s | Train: 2.0350 (57.2%) | Phys: 0.05 | Val: 1.1249 (71.49%) | Best: 71.60%\n",
            "Epoch 331 | T: 30s | Train: 1.9586 (59.0%) | Phys: 0.05 | Val: 1.1304 (71.23%) | Best: 71.60%\n",
            "Epoch 332 | T: 30s | Train: 1.9869 (59.1%) | Phys: 0.05 | Val: 1.1411 (71.44%) | Best: 71.60%\n",
            "Epoch 333 | T: 30s | Train: 1.9645 (58.6%) | Phys: 0.05 | Val: 1.1200 (71.76%) | Best: 71.76%\n",
            "Epoch 334 | T: 29s | Train: 1.9510 (59.5%) | Phys: 0.05 | Val: 1.1318 (71.48%) | Best: 71.76%\n",
            "Epoch 335 | T: 30s | Train: 1.8838 (61.2%) | Phys: 0.05 | Val: 1.1310 (71.32%) | Best: 71.76%\n",
            "Epoch 336 | T: 30s | Train: 2.0208 (57.4%) | Phys: 0.05 | Val: 1.1193 (71.72%) | Best: 71.76%\n",
            "Epoch 337 | T: 30s | Train: 2.0457 (57.2%) | Phys: 0.05 | Val: 1.1196 (71.75%) | Best: 71.76%\n",
            "Epoch 338 | T: 30s | Train: 1.9865 (58.3%) | Phys: 0.05 | Val: 1.1267 (71.61%) | Best: 71.76%\n",
            "Epoch 339 | T: 30s | Train: 1.8956 (60.6%) | Phys: 0.05 | Val: 1.1181 (71.68%) | Best: 71.76%\n",
            "Epoch 340 | T: 30s | Train: 2.0026 (57.8%) | Phys: 0.05 | Val: 1.1211 (71.78%) | Best: 71.78%\n",
            "Epoch 341 | T: 30s | Train: 1.9548 (59.4%) | Phys: 0.05 | Val: 1.1206 (71.74%) | Best: 71.78%\n",
            "Epoch 342 | T: 30s | Train: 1.9536 (59.2%) | Phys: 0.05 | Val: 1.1175 (71.57%) | Best: 71.78%\n",
            "Epoch 343 | T: 30s | Train: 1.9902 (58.2%) | Phys: 0.05 | Val: 1.1195 (71.71%) | Best: 71.78%\n",
            "Epoch 344 | T: 30s | Train: 2.0381 (56.4%) | Phys: 0.05 | Val: 1.1114 (71.84%) | Best: 71.84%\n",
            "Epoch 345 | T: 30s | Train: 2.0029 (58.0%) | Phys: 0.05 | Val: 1.1199 (71.90%) | Best: 71.90%\n",
            "Epoch 346 | T: 29s | Train: 2.0045 (57.9%) | Phys: 0.05 | Val: 1.1195 (71.93%) | Best: 71.93%\n",
            "Epoch 347 | T: 30s | Train: 1.9070 (60.5%) | Phys: 0.05 | Val: 1.1139 (72.07%) | Best: 72.07%\n",
            "Epoch 348 | T: 30s | Train: 1.8909 (61.0%) | Phys: 0.05 | Val: 1.1174 (72.05%) | Best: 72.07%\n",
            "Epoch 349 | T: 29s | Train: 2.0078 (57.7%) | Phys: 0.05 | Val: 1.1066 (71.98%) | Best: 72.07%\n",
            "Epoch 350 | T: 30s | Train: 1.9695 (58.7%) | Phys: 0.05 | Val: 1.1022 (72.11%) | Best: 72.11%\n",
            "Epoch 351 | T: 30s | Train: 1.9282 (60.2%) | Phys: 0.05 | Val: 1.1126 (71.99%) | Best: 72.11%\n",
            "Epoch 352 | T: 29s | Train: 1.9232 (60.0%) | Phys: 0.05 | Val: 1.1089 (72.17%) | Best: 72.17%\n",
            "Epoch 353 | T: 29s | Train: 1.9181 (59.9%) | Phys: 0.05 | Val: 1.1019 (72.19%) | Best: 72.19%\n",
            "Epoch 354 | T: 30s | Train: 1.9902 (58.3%) | Phys: 0.05 | Val: 1.1162 (72.07%) | Best: 72.19%\n",
            "Epoch 355 | T: 30s | Train: 1.8820 (61.2%) | Phys: 0.05 | Val: 1.1223 (71.67%) | Best: 72.19%\n",
            "Epoch 356 | T: 30s | Train: 1.9253 (60.1%) | Phys: 0.05 | Val: 1.1071 (72.18%) | Best: 72.19%\n",
            "Epoch 357 | T: 30s | Train: 1.9737 (58.8%) | Phys: 0.05 | Val: 1.1121 (71.74%) | Best: 72.19%\n",
            "Epoch 358 | T: 29s | Train: 1.9744 (58.8%) | Phys: 0.05 | Val: 1.1096 (72.09%) | Best: 72.19%\n",
            "Epoch 359 | T: 30s | Train: 1.8924 (61.1%) | Phys: 0.05 | Val: 1.1171 (72.07%) | Best: 72.19%\n",
            "Epoch 360 | T: 30s | Train: 1.9203 (60.1%) | Phys: 0.05 | Val: 1.1056 (72.24%) | Best: 72.24%\n",
            "Epoch 361 | T: 30s | Train: 1.9590 (59.0%) | Phys: 0.05 | Val: 1.1111 (71.77%) | Best: 72.24%\n",
            "Epoch 362 | T: 30s | Train: 1.8958 (61.0%) | Phys: 0.05 | Val: 1.1079 (72.19%) | Best: 72.24%\n",
            "Epoch 363 | T: 30s | Train: 1.9048 (60.3%) | Phys: 0.05 | Val: 1.1103 (72.01%) | Best: 72.24%\n",
            "Epoch 364 | T: 30s | Train: 1.9086 (60.7%) | Phys: 0.05 | Val: 1.1142 (72.16%) | Best: 72.24%\n",
            "Epoch 365 | T: 29s | Train: 1.9255 (59.8%) | Phys: 0.05 | Val: 1.1114 (72.01%) | Best: 72.24%\n",
            "Epoch 366 | T: 30s | Train: 1.9286 (59.6%) | Phys: 0.05 | Val: 1.1137 (71.83%) | Best: 72.24%\n",
            "Epoch 367 | T: 30s | Train: 1.9326 (59.8%) | Phys: 0.05 | Val: 1.1089 (72.13%) | Best: 72.24%\n",
            "Epoch 368 | T: 29s | Train: 1.8797 (61.7%) | Phys: 0.05 | Val: 1.1046 (72.15%) | Best: 72.24%\n",
            "Epoch 369 | T: 30s | Train: 1.8681 (61.9%) | Phys: 0.05 | Val: 1.1182 (72.33%) | Best: 72.33%\n",
            "Epoch 370 | T: 29s | Train: 1.9064 (60.6%) | Phys: 0.05 | Val: 1.1108 (72.18%) | Best: 72.33%\n",
            "Epoch 371 | T: 30s | Train: 1.9061 (60.4%) | Phys: 0.05 | Val: 1.0985 (72.37%) | Best: 72.37%\n",
            "Epoch 372 | T: 29s | Train: 1.8635 (62.1%) | Phys: 0.05 | Val: 1.1056 (72.36%) | Best: 72.37%\n",
            "Epoch 373 | T: 30s | Train: 1.9773 (59.0%) | Phys: 0.05 | Val: 1.1187 (72.15%) | Best: 72.37%\n",
            "Epoch 374 | T: 29s | Train: 1.8891 (61.0%) | Phys: 0.05 | Val: 1.1045 (72.08%) | Best: 72.37%\n",
            "Epoch 375 | T: 29s | Train: 1.9424 (59.5%) | Phys: 0.05 | Val: 1.1094 (72.38%) | Best: 72.38%\n",
            "Epoch 376 | T: 30s | Train: 1.9474 (59.6%) | Phys: 0.05 | Val: 1.1070 (72.21%) | Best: 72.38%\n",
            "Epoch 377 | T: 29s | Train: 1.9338 (59.8%) | Phys: 0.05 | Val: 1.1119 (72.13%) | Best: 72.38%\n",
            "Epoch 378 | T: 29s | Train: 1.9125 (60.4%) | Phys: 0.05 | Val: 1.0984 (72.37%) | Best: 72.38%\n",
            "Epoch 379 | T: 30s | Train: 2.0326 (57.1%) | Phys: 0.05 | Val: 1.1066 (72.69%) | Best: 72.69%\n",
            "Epoch 380 | T: 30s | Train: 1.8702 (61.3%) | Phys: 0.05 | Val: 1.1029 (72.40%) | Best: 72.69%\n",
            "Epoch 381 | T: 29s | Train: 1.9403 (59.4%) | Phys: 0.05 | Val: 1.0993 (72.66%) | Best: 72.69%\n",
            "Epoch 382 | T: 30s | Train: 1.8525 (62.2%) | Phys: 0.05 | Val: 1.1037 (72.41%) | Best: 72.69%\n",
            "Epoch 383 | T: 29s | Train: 1.9321 (59.9%) | Phys: 0.05 | Val: 1.0996 (72.59%) | Best: 72.69%\n",
            "Epoch 384 | T: 30s | Train: 1.8979 (60.7%) | Phys: 0.05 | Val: 1.0987 (72.55%) | Best: 72.69%\n",
            "Epoch 385 | T: 29s | Train: 1.8456 (62.3%) | Phys: 0.05 | Val: 1.0993 (72.71%) | Best: 72.71%\n",
            "Epoch 386 | T: 30s | Train: 1.9336 (59.5%) | Phys: 0.05 | Val: 1.0954 (72.41%) | Best: 72.71%\n",
            "Epoch 387 | T: 29s | Train: 1.9203 (60.1%) | Phys: 0.05 | Val: 1.1063 (72.08%) | Best: 72.71%\n",
            "Epoch 388 | T: 30s | Train: 1.8707 (61.6%) | Phys: 0.05 | Val: 1.1022 (72.49%) | Best: 72.71%\n",
            "Epoch 389 | T: 30s | Train: 1.9188 (60.1%) | Phys: 0.05 | Val: 1.0992 (72.56%) | Best: 72.71%\n",
            "Epoch 390 | T: 30s | Train: 1.9198 (60.0%) | Phys: 0.05 | Val: 1.0948 (72.58%) | Best: 72.71%\n",
            "Epoch 391 | T: 30s | Train: 1.9297 (59.9%) | Phys: 0.05 | Val: 1.1039 (72.41%) | Best: 72.71%\n",
            "Epoch 392 | T: 30s | Train: 1.9236 (59.9%) | Phys: 0.05 | Val: 1.1045 (72.64%) | Best: 72.71%\n",
            "Epoch 393 | T: 30s | Train: 1.9484 (58.9%) | Phys: 0.05 | Val: 1.1030 (72.38%) | Best: 72.71%\n",
            "Epoch 394 | T: 30s | Train: 1.9151 (60.3%) | Phys: 0.05 | Val: 1.1032 (72.41%) | Best: 72.71%\n",
            "Epoch 395 | T: 29s | Train: 1.9235 (60.1%) | Phys: 0.05 | Val: 1.1100 (72.17%) | Best: 72.71%\n",
            "Epoch 396 | T: 30s | Train: 1.9258 (60.0%) | Phys: 0.05 | Val: 1.1010 (72.46%) | Best: 72.71%\n",
            "Epoch 397 | T: 30s | Train: 1.8728 (61.8%) | Phys: 0.05 | Val: 1.1049 (72.49%) | Best: 72.71%\n",
            "Epoch 398 | T: 30s | Train: 1.8835 (60.9%) | Phys: 0.05 | Val: 1.0965 (72.49%) | Best: 72.71%\n",
            "Epoch 399 | T: 30s | Train: 1.8353 (62.3%) | Phys: 0.05 | Val: 1.0981 (72.47%) | Best: 72.71%\n",
            "Epoch 400 | T: 30s | Train: 1.8430 (61.9%) | Phys: 0.05 | Val: 1.0966 (72.55%) | Best: 72.71%\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": [],
      "name": "CIFAR-100 ViT SIGReg Strong",
      "gpuType": "L4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}