{
  "cells": [
    {
      "cell_type": "code",
      "id": "pBxX3I1kYwojaamntSxiNZTK",
      "metadata": {
        "tags": [],
        "id": "pBxX3I1kYwojaamntSxiNZTK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d07b5e02-d214-4923-94fc-c80272722904"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "import math\n",
        "import time\n",
        "\n",
        "REG_MODE = 'strong'\n",
        "SIGR_ALPHA = 0.01   # Strength of the physics constraint\n",
        "SKETCH_DIM = 64    # Dimension of the random observer\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "LEARNING_RATE = 0.1\n",
        "EPOCHS = 400\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "if torch.backends.mps.is_available(): DEVICE = 'mps'\n",
        "\n",
        "# Regularization Config\n",
        "MIXUP_ALPHA = 0.8\n",
        "CUTMIX_ALPHA = 1.0\n",
        "\n",
        "print(f\"Training on device: {DEVICE}\")\n",
        "\n",
        "def get_data_loaders():\n",
        "    print('==> Preparing data with Strong Augmentation...')\n",
        "\n",
        "    mean = (0.5071, 0.4867, 0.4408)\n",
        "    std = (0.2675, 0.2565, 0.2761)\n",
        "\n",
        "    # FIX 1: Add RandAugment\n",
        "    transform_train = transforms.Compose([\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandAugment(num_ops=2, magnitude=9),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean, std),\n",
        "    ])\n",
        "\n",
        "    transform_test = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean, std),\n",
        "    ])\n",
        "\n",
        "    trainset = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=transform_train)\n",
        "    # Increase workers to handle augmentation load\n",
        "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
        "\n",
        "    testset = torchvision.datasets.CIFAR100(root='./data', train=False, download=True, transform=transform_test)\n",
        "    testloader = torch.utils.data.DataLoader(testset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
        "\n",
        "    return trainloader, testloader\n",
        "\n",
        "\n",
        "# ------------------------------------------\n",
        "# Physics Engine: The Regularizers\n",
        "# ------------------------------------------\n",
        "\n",
        "def sigreg_weak_loss(x, sketch_dim=64):\n",
        "    \"\"\"\n",
        "    Forces Covariance(x) ~ Identity.\n",
        "    Matches the 2nd Moment (Spherical Cloud).\n",
        "    \"\"\"\n",
        "    N, C = x.size()\n",
        "    # 1. Sketching (Optional for C=512, but good for consistency)\n",
        "    if C > sketch_dim:\n",
        "        S = torch.randn(sketch_dim, C, device=x.device) / (C ** 0.5)\n",
        "        x = x @ S.T  # [N, sketch_dim]\n",
        "    else:\n",
        "        sketch_dim = C\n",
        "\n",
        "    # 2. Centering & Covariance\n",
        "    x = x - x.mean(dim=0, keepdim=True)\n",
        "    cov = (x.T @ x) / (N - 1 + 1e-6)\n",
        "\n",
        "    # 3. Target Identity\n",
        "    target = torch.eye(sketch_dim, device=x.device)\n",
        "\n",
        "    # 4. Off-diagonal suppression + Diagonal maintenance\n",
        "    return torch.norm(cov - target, p='fro')\n",
        "\n",
        "def sigreg_strong_loss(x, sketch_dim=64):\n",
        "    \"\"\"\n",
        "    Forces ECF(x) ~ ECF(Gaussian).\n",
        "    Matches ALL Moments (Maximum Entropy Cloud).\n",
        "    Exact implementation of LeJEPA Algorithm 1.\n",
        "    \"\"\"\n",
        "    N, C = x.size()\n",
        "\n",
        "    # 1. Projection (The Observer)\n",
        "    # Project channels down to sketch_dim\n",
        "    A = torch.randn(C, sketch_dim, device=x.device)\n",
        "    A = A / (A.norm(p=2, dim=0, keepdim=True) + 1e-6)\n",
        "\n",
        "    # 2. Integration Points\n",
        "    t = torch.linspace(-5, 5, 17, device=x.device)\n",
        "\n",
        "    # 3. Theoretical Gaussian CF\n",
        "    exp_f = torch.exp(-0.5 * t**2)\n",
        "\n",
        "    # 4. Empirical CF\n",
        "    # proj: [N, sketch_dim]\n",
        "    proj = x @ A\n",
        "\n",
        "    # args: [N, sketch_dim, T]\n",
        "    args = proj.unsqueeze(2) * t.view(1, 1, -1)\n",
        "\n",
        "    # ecf: [sketch_dim, T] (Mean over batch)\n",
        "    ecf = torch.exp(1j * args).mean(dim=0)\n",
        "\n",
        "    # 5. Weighted L2 Distance\n",
        "    # |ecf - gauss|^2 * gauss_weight\n",
        "    diff_sq = (ecf - exp_f.unsqueeze(0)).abs().square()\n",
        "    err = diff_sq * exp_f.unsqueeze(0)\n",
        "\n",
        "    # 6. Integrate\n",
        "    loss = torch.trapz(err, t, dim=1) * N\n",
        "\n",
        "    return loss.mean()\n",
        "\n",
        "# ==========================================\n",
        "# 3. Mixup / CutMix Utilities\n",
        "# ==========================================\n",
        "def rand_bbox(size, lam):\n",
        "    W = size[2]\n",
        "    H = size[3]\n",
        "    cut_rat = np.sqrt(1. - lam)\n",
        "    cut_w = int(W * cut_rat)\n",
        "    cut_h = int(H * cut_rat)\n",
        "\n",
        "    cx = np.random.randint(W)\n",
        "    cy = np.random.randint(H)\n",
        "\n",
        "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
        "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
        "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
        "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
        "\n",
        "    return bbx1, bby1, bbx2, bby2\n",
        "\n",
        "class LinearBlock(nn.Module):\n",
        "    def __init__(self, dim, hidden_dim, reg_mode='baseline', sketch_dim=64):\n",
        "        super().__init__()\n",
        "        self.fc = nn.Linear(dim, hidden_dim)\n",
        "        # Note: NO BATCH NORM. We rely purely on SIGReg.\n",
        "        self.reg_mode = reg_mode\n",
        "        self.sketch_dim = sketch_dim\n",
        "\n",
        "    def forward(self, x):\n",
        "        pre_act = self.fc(x)\n",
        "\n",
        "        reg_loss = torch.tensor(0.0, device=x.device)\n",
        "        if self.reg_mode != 'baseline':\n",
        "            if self.reg_mode == 'weak':\n",
        "                reg_loss = sigreg_weak_loss(pre_act, self.sketch_dim)\n",
        "            elif self.reg_mode == 'strong':\n",
        "                reg_loss = sigreg_strong_loss(pre_act, self.sketch_dim)\n",
        "\n",
        "        out = F.relu(pre_act)\n",
        "\n",
        "        return out, reg_loss\n",
        "\n",
        "class ThermoMLP(nn.Module):\n",
        "    def __init__(self, input_dim=3072, hidden_dim=1024, num_classes=100, depth=6, reg_mode='weak', sketch_dim=64):\n",
        "        super().__init__()\n",
        "\n",
        "        layers = []\n",
        "        # Input Layer\n",
        "        layers.append(LinearBlock(input_dim, hidden_dim, reg_mode))\n",
        "\n",
        "        # Deep Layers (No Residuals!)\n",
        "        for _ in range(depth - 2):\n",
        "            layers.append(LinearBlock(hidden_dim, hidden_dim, reg_mode, sketch_dim))\n",
        "\n",
        "        self.layers = nn.ModuleList(layers)\n",
        "        self.classifier = nn.Linear(hidden_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Flatten: [B, C, H, W] -> [B, 3072]\n",
        "        x = x.flatten(1)\n",
        "\n",
        "        total_phys_loss = 0.0\n",
        "\n",
        "        for layer in self.layers:\n",
        "            x, l_loss = layer(x)\n",
        "            total_phys_loss += l_loss\n",
        "\n",
        "        out = self.classifier(x)\n",
        "\n",
        "        # Normalize loss scale\n",
        "        return out, (total_phys_loss / len(self.layers))\n",
        "\n",
        "# ==========================================\n",
        "# 5. Training Engine (Updated for Mixup/CutMix)\n",
        "# ==========================================\n",
        "def train(epoch, net, trainloader, optimizer, criterion):\n",
        "    net.train()\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    phys_loss_meter = 0\n",
        "\n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
        "\n",
        "        # Apply Mixup/CutMix\n",
        "        r = np.random.rand(1)\n",
        "        if r < 0.5: # Mixup\n",
        "            lam = np.random.beta(MIXUP_ALPHA, MIXUP_ALPHA)\n",
        "            index = torch.randperm(inputs.size(0)).to(DEVICE)\n",
        "            inputs = lam * inputs + (1 - lam) * inputs[index, :]\n",
        "            targets_a, targets_b = targets, targets[index]\n",
        "        else: # CutMix\n",
        "            lam = np.random.beta(CUTMIX_ALPHA, CUTMIX_ALPHA)\n",
        "            rand_index = torch.randperm(inputs.size(0)).to(DEVICE)\n",
        "            target_a = targets\n",
        "            target_b = targets[rand_index]\n",
        "            bbx1, bby1, bbx2, bby2 = rand_bbox(inputs.size(), lam)\n",
        "            inputs[:, :, bbx1:bbx2, bby1:bby2] = inputs[rand_index, :, bbx1:bbx2, bby1:bby2]\n",
        "            lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (inputs.size()[-1] * inputs.size()[-2]))\n",
        "            targets_a, targets_b = target_a, target_b\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward\n",
        "        outputs, p_loss = net(inputs)\n",
        "\n",
        "        # Task Loss\n",
        "        c_loss = criterion(outputs, targets_a) * lam + criterion(outputs, targets_b) * (1. - lam)\n",
        "\n",
        "        # Total Loss\n",
        "        loss = (1 - SIGR_ALPHA) * c_loss + (SIGR_ALPHA * p_loss)\n",
        "\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(net.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += ((1 - SIGR_ALPHA) * c_loss).item() # Log only task loss for comparison\n",
        "        phys_loss_meter += (SIGR_ALPHA * p_loss).item()\n",
        "\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += (lam * predicted.eq(targets_a).float() + (1 - lam) * predicted.eq(targets_b).float()).sum().item()\n",
        "\n",
        "    acc = 100. * correct / total\n",
        "    return train_loss / (batch_idx + 1), acc, phys_loss_meter / (batch_idx + 1)\n",
        "\n",
        "def test(epoch, net, testloader, criterion):\n",
        "    net.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "            inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
        "            outputs, _ = net(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "    acc = 100. * correct / total\n",
        "    return test_loss / (batch_idx + 1), acc\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    trainloader, testloader = get_data_loaders()\n",
        "\n",
        "    net = ThermoMLP(reg_mode=REG_MODE, sketch_dim=SKETCH_DIM).to(DEVICE)\n",
        "    net = net.to(DEVICE)\n",
        "\n",
        "    # Standard CrossEntropy for final eval, SoftLabel for training is handled by Mixup logic\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    optimizer = optim.SGD(net.parameters(), lr=LEARNING_RATE, momentum=0.0, weight_decay=0.0)\n",
        "\n",
        "    print(f\"Starting training for {EPOCHS} epochs with RandAugment + Mixup/CutMix...\")\n",
        "\n",
        "    best_acc = 0\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        start_time = time.time()\n",
        "\n",
        "        train_loss, train_acc, physics_loss = train(epoch, net, trainloader, optimizer, criterion)\n",
        "        test_loss, test_acc = test(epoch, net, testloader, criterion)\n",
        "\n",
        "        if test_acc > best_acc:\n",
        "            best_acc = test_acc\n",
        "            # torch.save(net.state_dict(), f'thermo_resnet_{REG_MODE}.pth')\n",
        "\n",
        "        epoch_time = time.time() - start_time\n",
        "\n",
        "        print(f\"Epoch {epoch+1} | T: {epoch_time:.0f}s | \"\n",
        "              f\"Train: {train_loss:.4f} ({train_acc:.1f}%) | \"\n",
        "              f\"Phys: {physics_loss:.2f} | \"\n",
        "              f\"Val: {test_loss:.4f} ({test_acc:.2f}%) | \"\n",
        "              f\"Best: {best_acc:.2f}%\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on device: cuda\n",
            "==> Preparing data with Strong Augmentation...\n",
            "Starting training for 400 epochs with RandAugment + Mixup/CutMix...\n",
            "Epoch 1 | T: 19s | Train: 4.5508 (1.3%) | Phys: 0.33 | Val: 4.5239 (2.30%) | Best: 2.30%\n",
            "Epoch 2 | T: 19s | Train: 4.5200 (1.8%) | Phys: 0.17 | Val: 4.4048 (3.24%) | Best: 3.24%\n",
            "Epoch 3 | T: 19s | Train: 4.4558 (2.4%) | Phys: 0.13 | Val: 4.2692 (3.96%) | Best: 3.96%\n",
            "Epoch 4 | T: 19s | Train: 4.4151 (3.1%) | Phys: 0.11 | Val: 4.1996 (5.53%) | Best: 5.53%\n",
            "Epoch 5 | T: 19s | Train: 4.3776 (3.6%) | Phys: 0.10 | Val: 4.1133 (6.36%) | Best: 6.36%\n",
            "Epoch 6 | T: 19s | Train: 4.3421 (4.2%) | Phys: 0.10 | Val: 4.0502 (7.36%) | Best: 7.36%\n",
            "Epoch 7 | T: 19s | Train: 4.3210 (4.5%) | Phys: 0.09 | Val: 4.0155 (7.83%) | Best: 7.83%\n",
            "Epoch 8 | T: 19s | Train: 4.3037 (4.9%) | Phys: 0.09 | Val: 3.9932 (7.78%) | Best: 7.83%\n",
            "Epoch 9 | T: 19s | Train: 4.2932 (5.0%) | Phys: 0.08 | Val: 3.9408 (8.93%) | Best: 8.93%\n",
            "Epoch 10 | T: 19s | Train: 4.2853 (5.0%) | Phys: 0.08 | Val: 3.9339 (9.27%) | Best: 9.27%\n",
            "Epoch 11 | T: 19s | Train: 4.2600 (5.4%) | Phys: 0.08 | Val: 3.8905 (9.85%) | Best: 9.85%\n",
            "Epoch 12 | T: 19s | Train: 4.2688 (5.6%) | Phys: 0.08 | Val: 3.8820 (10.57%) | Best: 10.57%\n",
            "Epoch 13 | T: 19s | Train: 4.2426 (6.0%) | Phys: 0.07 | Val: 3.8790 (11.20%) | Best: 11.20%\n",
            "Epoch 14 | T: 19s | Train: 4.2413 (6.1%) | Phys: 0.07 | Val: 3.8228 (11.57%) | Best: 11.57%\n",
            "Epoch 15 | T: 18s | Train: 4.2176 (6.4%) | Phys: 0.07 | Val: 3.8159 (11.89%) | Best: 11.89%\n",
            "Epoch 16 | T: 19s | Train: 4.2145 (6.4%) | Phys: 0.07 | Val: 3.8132 (12.35%) | Best: 12.35%\n",
            "Epoch 17 | T: 19s | Train: 4.1946 (6.8%) | Phys: 0.07 | Val: 3.7966 (12.29%) | Best: 12.35%\n",
            "Epoch 18 | T: 19s | Train: 4.1916 (6.7%) | Phys: 0.07 | Val: 3.7579 (12.82%) | Best: 12.82%\n",
            "Epoch 19 | T: 19s | Train: 4.1859 (7.0%) | Phys: 0.07 | Val: 3.7692 (12.92%) | Best: 12.92%\n",
            "Epoch 20 | T: 18s | Train: 4.1820 (7.1%) | Phys: 0.07 | Val: 3.7766 (12.83%) | Best: 12.92%\n",
            "Epoch 21 | T: 19s | Train: 4.1715 (7.4%) | Phys: 0.06 | Val: 3.7216 (13.71%) | Best: 13.71%\n",
            "Epoch 22 | T: 19s | Train: 4.1565 (7.5%) | Phys: 0.06 | Val: 3.7116 (13.79%) | Best: 13.79%\n",
            "Epoch 23 | T: 19s | Train: 4.1585 (7.5%) | Phys: 0.06 | Val: 3.7338 (13.87%) | Best: 13.87%\n",
            "Epoch 24 | T: 19s | Train: 4.1397 (7.8%) | Phys: 0.06 | Val: 3.7046 (14.56%) | Best: 14.56%\n",
            "Epoch 25 | T: 19s | Train: 4.1331 (8.0%) | Phys: 0.06 | Val: 3.6868 (14.67%) | Best: 14.67%\n",
            "Epoch 26 | T: 19s | Train: 4.1428 (7.9%) | Phys: 0.06 | Val: 3.6800 (14.51%) | Best: 14.67%\n",
            "Epoch 27 | T: 19s | Train: 4.1328 (7.9%) | Phys: 0.06 | Val: 3.6769 (14.89%) | Best: 14.89%\n",
            "Epoch 28 | T: 19s | Train: 4.1159 (8.3%) | Phys: 0.06 | Val: 3.6518 (15.28%) | Best: 15.28%\n",
            "Epoch 29 | T: 19s | Train: 4.1209 (8.3%) | Phys: 0.06 | Val: 3.6629 (15.09%) | Best: 15.28%\n",
            "Epoch 30 | T: 19s | Train: 4.1028 (8.5%) | Phys: 0.06 | Val: 3.6346 (16.00%) | Best: 16.00%\n",
            "Epoch 31 | T: 19s | Train: 4.1121 (8.5%) | Phys: 0.06 | Val: 3.6617 (15.43%) | Best: 16.00%\n",
            "Epoch 32 | T: 19s | Train: 4.1031 (8.5%) | Phys: 0.06 | Val: 3.6337 (15.37%) | Best: 16.00%\n",
            "Epoch 33 | T: 19s | Train: 4.0969 (8.7%) | Phys: 0.06 | Val: 3.5964 (16.16%) | Best: 16.16%\n",
            "Epoch 34 | T: 19s | Train: 4.0859 (8.9%) | Phys: 0.06 | Val: 3.6017 (16.00%) | Best: 16.16%\n",
            "Epoch 35 | T: 19s | Train: 4.0809 (8.8%) | Phys: 0.06 | Val: 3.6131 (16.39%) | Best: 16.39%\n",
            "Epoch 36 | T: 19s | Train: 4.0548 (9.5%) | Phys: 0.06 | Val: 3.5760 (16.93%) | Best: 16.93%\n",
            "Epoch 37 | T: 19s | Train: 4.0753 (9.1%) | Phys: 0.06 | Val: 3.5977 (16.21%) | Best: 16.93%\n",
            "Epoch 38 | T: 19s | Train: 4.0774 (9.1%) | Phys: 0.06 | Val: 3.6024 (16.84%) | Best: 16.93%\n",
            "Epoch 39 | T: 19s | Train: 4.0613 (9.3%) | Phys: 0.06 | Val: 3.5731 (17.47%) | Best: 17.47%\n",
            "Epoch 40 | T: 19s | Train: 4.0366 (9.6%) | Phys: 0.06 | Val: 3.5616 (17.48%) | Best: 17.48%\n",
            "Epoch 41 | T: 19s | Train: 4.0663 (9.3%) | Phys: 0.06 | Val: 3.5565 (17.52%) | Best: 17.52%\n",
            "Epoch 42 | T: 19s | Train: 4.0510 (9.5%) | Phys: 0.06 | Val: 3.5584 (17.07%) | Best: 17.52%\n",
            "Epoch 43 | T: 19s | Train: 4.0548 (9.5%) | Phys: 0.06 | Val: 3.5771 (17.44%) | Best: 17.52%\n",
            "Epoch 44 | T: 19s | Train: 4.0437 (9.8%) | Phys: 0.06 | Val: 3.5571 (17.24%) | Best: 17.52%\n",
            "Epoch 45 | T: 19s | Train: 4.0553 (9.6%) | Phys: 0.06 | Val: 3.5510 (17.88%) | Best: 17.88%\n",
            "Epoch 46 | T: 19s | Train: 4.0491 (9.6%) | Phys: 0.06 | Val: 3.5644 (18.43%) | Best: 18.43%\n",
            "Epoch 47 | T: 18s | Train: 4.0187 (10.0%) | Phys: 0.06 | Val: 3.5334 (18.98%) | Best: 18.98%\n",
            "Epoch 48 | T: 19s | Train: 4.0321 (9.9%) | Phys: 0.06 | Val: 3.5706 (18.38%) | Best: 18.98%\n",
            "Epoch 49 | T: 19s | Train: 4.0415 (9.9%) | Phys: 0.06 | Val: 3.5061 (18.15%) | Best: 18.98%\n",
            "Epoch 50 | T: 18s | Train: 4.0513 (9.5%) | Phys: 0.06 | Val: 3.5293 (17.95%) | Best: 18.98%\n",
            "Epoch 51 | T: 19s | Train: 4.0115 (10.3%) | Phys: 0.06 | Val: 3.4766 (18.30%) | Best: 18.98%\n",
            "Epoch 52 | T: 19s | Train: 4.0039 (10.5%) | Phys: 0.06 | Val: 3.4715 (19.21%) | Best: 19.21%\n",
            "Epoch 53 | T: 19s | Train: 4.0157 (10.3%) | Phys: 0.06 | Val: 3.4860 (19.31%) | Best: 19.31%\n",
            "Epoch 54 | T: 18s | Train: 3.9985 (10.4%) | Phys: 0.06 | Val: 3.4986 (19.00%) | Best: 19.31%\n",
            "Epoch 55 | T: 18s | Train: 4.0200 (10.0%) | Phys: 0.06 | Val: 3.5135 (19.24%) | Best: 19.31%\n",
            "Epoch 56 | T: 19s | Train: 4.0041 (10.5%) | Phys: 0.06 | Val: 3.4687 (19.05%) | Best: 19.31%\n",
            "Epoch 57 | T: 19s | Train: 3.9872 (10.9%) | Phys: 0.06 | Val: 3.4692 (19.48%) | Best: 19.48%\n",
            "Epoch 58 | T: 19s | Train: 3.9845 (10.7%) | Phys: 0.06 | Val: 3.4737 (19.85%) | Best: 19.85%\n",
            "Epoch 59 | T: 19s | Train: 4.0030 (10.4%) | Phys: 0.06 | Val: 3.4461 (19.97%) | Best: 19.97%\n",
            "Epoch 60 | T: 18s | Train: 3.9548 (11.4%) | Phys: 0.06 | Val: 3.4422 (20.14%) | Best: 20.14%\n",
            "Epoch 61 | T: 19s | Train: 4.0019 (10.4%) | Phys: 0.06 | Val: 3.4804 (19.62%) | Best: 20.14%\n",
            "Epoch 62 | T: 19s | Train: 3.9787 (10.9%) | Phys: 0.06 | Val: 3.4611 (19.90%) | Best: 20.14%\n",
            "Epoch 63 | T: 18s | Train: 3.9701 (11.2%) | Phys: 0.06 | Val: 3.4217 (20.61%) | Best: 20.61%\n",
            "Epoch 64 | T: 19s | Train: 3.9922 (11.0%) | Phys: 0.06 | Val: 3.4538 (20.72%) | Best: 20.72%\n",
            "Epoch 65 | T: 18s | Train: 3.9638 (11.2%) | Phys: 0.05 | Val: 3.4336 (20.68%) | Best: 20.72%\n",
            "Epoch 66 | T: 19s | Train: 3.9536 (11.2%) | Phys: 0.05 | Val: 3.4012 (20.94%) | Best: 20.94%\n",
            "Epoch 67 | T: 19s | Train: 3.9608 (11.4%) | Phys: 0.06 | Val: 3.4118 (20.71%) | Best: 20.94%\n",
            "Epoch 68 | T: 19s | Train: 3.9533 (11.3%) | Phys: 0.05 | Val: 3.3826 (21.06%) | Best: 21.06%\n",
            "Epoch 69 | T: 19s | Train: 3.9673 (11.3%) | Phys: 0.06 | Val: 3.4193 (21.53%) | Best: 21.53%\n",
            "Epoch 70 | T: 18s | Train: 3.9552 (11.2%) | Phys: 0.05 | Val: 3.3898 (21.62%) | Best: 21.62%\n",
            "Epoch 71 | T: 19s | Train: 3.9644 (11.2%) | Phys: 0.06 | Val: 3.4028 (20.44%) | Best: 21.62%\n",
            "Epoch 72 | T: 19s | Train: 3.9455 (11.5%) | Phys: 0.05 | Val: 3.4362 (21.33%) | Best: 21.62%\n",
            "Epoch 73 | T: 18s | Train: 3.9309 (11.6%) | Phys: 0.05 | Val: 3.4219 (21.13%) | Best: 21.62%\n",
            "Epoch 74 | T: 19s | Train: 3.9180 (12.0%) | Phys: 0.05 | Val: 3.3790 (20.88%) | Best: 21.62%\n",
            "Epoch 75 | T: 18s | Train: 3.9316 (11.9%) | Phys: 0.05 | Val: 3.4028 (21.37%) | Best: 21.62%\n",
            "Epoch 76 | T: 19s | Train: 3.9264 (11.9%) | Phys: 0.05 | Val: 3.3371 (21.63%) | Best: 21.63%\n",
            "Epoch 77 | T: 18s | Train: 3.9299 (11.8%) | Phys: 0.05 | Val: 3.3608 (21.45%) | Best: 21.63%\n",
            "Epoch 78 | T: 19s | Train: 3.9482 (11.6%) | Phys: 0.05 | Val: 3.3634 (22.30%) | Best: 22.30%\n",
            "Epoch 79 | T: 19s | Train: 3.9272 (11.9%) | Phys: 0.05 | Val: 3.3329 (22.80%) | Best: 22.80%\n",
            "Epoch 80 | T: 18s | Train: 3.9590 (11.4%) | Phys: 0.06 | Val: 3.3561 (21.98%) | Best: 22.80%\n",
            "Epoch 81 | T: 18s | Train: 3.8941 (12.4%) | Phys: 0.05 | Val: 3.3317 (22.50%) | Best: 22.80%\n",
            "Epoch 82 | T: 19s | Train: 3.9082 (12.3%) | Phys: 0.05 | Val: 3.3312 (22.32%) | Best: 22.80%\n",
            "Epoch 83 | T: 18s | Train: 3.9292 (11.8%) | Phys: 0.05 | Val: 3.3641 (22.52%) | Best: 22.80%\n",
            "Epoch 84 | T: 19s | Train: 3.8992 (12.3%) | Phys: 0.05 | Val: 3.3460 (21.92%) | Best: 22.80%\n",
            "Epoch 85 | T: 18s | Train: 3.9037 (12.3%) | Phys: 0.05 | Val: 3.2991 (22.86%) | Best: 22.86%\n",
            "Epoch 86 | T: 19s | Train: 3.9106 (12.2%) | Phys: 0.05 | Val: 3.3323 (22.64%) | Best: 22.86%\n",
            "Epoch 87 | T: 18s | Train: 3.8891 (12.7%) | Phys: 0.05 | Val: 3.3109 (22.31%) | Best: 22.86%\n",
            "Epoch 88 | T: 19s | Train: 3.9019 (12.4%) | Phys: 0.05 | Val: 3.2657 (23.17%) | Best: 23.17%\n",
            "Epoch 89 | T: 19s | Train: 3.8654 (12.9%) | Phys: 0.05 | Val: 3.3429 (22.47%) | Best: 23.17%\n",
            "Epoch 90 | T: 18s | Train: 3.8979 (12.4%) | Phys: 0.05 | Val: 3.2913 (23.28%) | Best: 23.28%\n",
            "Epoch 91 | T: 19s | Train: 3.8894 (12.6%) | Phys: 0.05 | Val: 3.3165 (22.68%) | Best: 23.28%\n",
            "Epoch 92 | T: 19s | Train: 3.9064 (12.1%) | Phys: 0.05 | Val: 3.2933 (23.67%) | Best: 23.67%\n",
            "Epoch 93 | T: 18s | Train: 3.8937 (12.6%) | Phys: 0.05 | Val: 3.2648 (23.29%) | Best: 23.67%\n",
            "Epoch 94 | T: 19s | Train: 3.8690 (12.9%) | Phys: 0.05 | Val: 3.2826 (23.70%) | Best: 23.70%\n",
            "Epoch 95 | T: 18s | Train: 3.8493 (13.3%) | Phys: 0.05 | Val: 3.2257 (23.89%) | Best: 23.89%\n",
            "Epoch 96 | T: 19s | Train: 3.8696 (13.0%) | Phys: 0.05 | Val: 3.2766 (23.49%) | Best: 23.89%\n",
            "Epoch 97 | T: 19s | Train: 3.8833 (12.7%) | Phys: 0.05 | Val: 3.2297 (23.82%) | Best: 23.89%\n",
            "Epoch 98 | T: 19s | Train: 3.8790 (12.8%) | Phys: 0.05 | Val: 3.2152 (24.12%) | Best: 24.12%\n",
            "Epoch 99 | T: 19s | Train: 3.8804 (12.9%) | Phys: 0.05 | Val: 3.2264 (24.12%) | Best: 24.12%\n",
            "Epoch 100 | T: 18s | Train: 3.8764 (12.9%) | Phys: 0.05 | Val: 3.2377 (24.45%) | Best: 24.45%\n",
            "Epoch 101 | T: 19s | Train: 3.8621 (12.9%) | Phys: 0.05 | Val: 3.2154 (24.35%) | Best: 24.45%\n",
            "Epoch 102 | T: 19s | Train: 3.8609 (13.3%) | Phys: 0.05 | Val: 3.2691 (24.48%) | Best: 24.48%\n",
            "Epoch 103 | T: 18s | Train: 3.8361 (13.5%) | Phys: 0.05 | Val: 3.2105 (24.86%) | Best: 24.86%\n",
            "Epoch 104 | T: 19s | Train: 3.8701 (13.1%) | Phys: 0.05 | Val: 3.2062 (24.43%) | Best: 24.86%\n",
            "Epoch 105 | T: 19s | Train: 3.8500 (13.5%) | Phys: 0.05 | Val: 3.2667 (23.99%) | Best: 24.86%\n",
            "Epoch 106 | T: 19s | Train: 3.8848 (12.8%) | Phys: 0.05 | Val: 3.1946 (24.26%) | Best: 24.86%\n",
            "Epoch 107 | T: 19s | Train: 3.8632 (13.3%) | Phys: 0.05 | Val: 3.2055 (24.54%) | Best: 24.86%\n",
            "Epoch 108 | T: 19s | Train: 3.8617 (13.3%) | Phys: 0.05 | Val: 3.2161 (24.27%) | Best: 24.86%\n",
            "Epoch 109 | T: 19s | Train: 3.8617 (13.4%) | Phys: 0.05 | Val: 3.1843 (25.35%) | Best: 25.35%\n",
            "Epoch 110 | T: 18s | Train: 3.8414 (13.5%) | Phys: 0.05 | Val: 3.2183 (24.67%) | Best: 25.35%\n",
            "Epoch 111 | T: 19s | Train: 3.8429 (13.4%) | Phys: 0.05 | Val: 3.1795 (25.04%) | Best: 25.35%\n",
            "Epoch 112 | T: 19s | Train: 3.8361 (13.6%) | Phys: 0.05 | Val: 3.2002 (24.93%) | Best: 25.35%\n",
            "Epoch 113 | T: 19s | Train: 3.8751 (13.1%) | Phys: 0.05 | Val: 3.1716 (25.35%) | Best: 25.35%\n",
            "Epoch 114 | T: 19s | Train: 3.8470 (13.5%) | Phys: 0.05 | Val: 3.1752 (25.71%) | Best: 25.71%\n",
            "Epoch 115 | T: 19s | Train: 3.8256 (14.0%) | Phys: 0.05 | Val: 3.1752 (25.13%) | Best: 25.71%\n",
            "Epoch 116 | T: 19s | Train: 3.8441 (13.6%) | Phys: 0.05 | Val: 3.1779 (24.84%) | Best: 25.71%\n",
            "Epoch 117 | T: 19s | Train: 3.8146 (13.9%) | Phys: 0.05 | Val: 3.1641 (25.62%) | Best: 25.71%\n",
            "Epoch 118 | T: 19s | Train: 3.8050 (14.3%) | Phys: 0.05 | Val: 3.1383 (25.79%) | Best: 25.79%\n",
            "Epoch 119 | T: 19s | Train: 3.8360 (13.8%) | Phys: 0.05 | Val: 3.1384 (25.93%) | Best: 25.93%\n",
            "Epoch 120 | T: 19s | Train: 3.8232 (14.0%) | Phys: 0.05 | Val: 3.1111 (26.16%) | Best: 26.16%\n",
            "Epoch 121 | T: 19s | Train: 3.8315 (13.9%) | Phys: 0.05 | Val: 3.1610 (26.10%) | Best: 26.16%\n",
            "Epoch 122 | T: 19s | Train: 3.8426 (13.6%) | Phys: 0.05 | Val: 3.1229 (26.29%) | Best: 26.29%\n",
            "Epoch 123 | T: 19s | Train: 3.8102 (14.2%) | Phys: 0.05 | Val: 3.1187 (26.66%) | Best: 26.66%\n",
            "Epoch 124 | T: 19s | Train: 3.8328 (13.9%) | Phys: 0.05 | Val: 3.0989 (26.64%) | Best: 26.66%\n",
            "Epoch 125 | T: 19s | Train: 3.8050 (14.5%) | Phys: 0.05 | Val: 3.1299 (26.63%) | Best: 26.66%\n",
            "Epoch 126 | T: 19s | Train: 3.8465 (13.6%) | Phys: 0.05 | Val: 3.1325 (25.75%) | Best: 26.66%\n",
            "Epoch 127 | T: 19s | Train: 3.8164 (14.1%) | Phys: 0.05 | Val: 3.1062 (26.58%) | Best: 26.66%\n",
            "Epoch 128 | T: 19s | Train: 3.7992 (14.6%) | Phys: 0.05 | Val: 3.1208 (26.46%) | Best: 26.66%\n",
            "Epoch 129 | T: 19s | Train: 3.7976 (14.6%) | Phys: 0.05 | Val: 3.1349 (26.70%) | Best: 26.70%\n",
            "Epoch 130 | T: 19s | Train: 3.8092 (14.3%) | Phys: 0.05 | Val: 3.0961 (27.20%) | Best: 27.20%\n",
            "Epoch 131 | T: 19s | Train: 3.8227 (13.9%) | Phys: 0.05 | Val: 3.1141 (26.76%) | Best: 27.20%\n",
            "Epoch 132 | T: 19s | Train: 3.8442 (13.7%) | Phys: 0.05 | Val: 3.1298 (26.90%) | Best: 27.20%\n",
            "Epoch 133 | T: 18s | Train: 3.8027 (14.5%) | Phys: 0.05 | Val: 3.1337 (26.75%) | Best: 27.20%\n",
            "Epoch 134 | T: 19s | Train: 3.7767 (14.9%) | Phys: 0.05 | Val: 3.0518 (27.32%) | Best: 27.32%\n",
            "Epoch 135 | T: 18s | Train: 3.7820 (14.6%) | Phys: 0.05 | Val: 3.1031 (26.98%) | Best: 27.32%\n",
            "Epoch 136 | T: 19s | Train: 3.7831 (14.7%) | Phys: 0.05 | Val: 3.0713 (27.42%) | Best: 27.42%\n",
            "Epoch 137 | T: 19s | Train: 3.8112 (14.4%) | Phys: 0.05 | Val: 3.0767 (27.18%) | Best: 27.42%\n",
            "Epoch 138 | T: 18s | Train: 3.8322 (14.0%) | Phys: 0.05 | Val: 3.1015 (26.62%) | Best: 27.42%\n",
            "Epoch 139 | T: 18s | Train: 3.8214 (14.2%) | Phys: 0.05 | Val: 3.1091 (26.94%) | Best: 27.42%\n",
            "Epoch 140 | T: 18s | Train: 3.7786 (14.6%) | Phys: 0.05 | Val: 3.0808 (26.96%) | Best: 27.42%\n",
            "Epoch 141 | T: 19s | Train: 3.7808 (14.9%) | Phys: 0.05 | Val: 3.0826 (27.18%) | Best: 27.42%\n",
            "Epoch 142 | T: 19s | Train: 3.7442 (15.4%) | Phys: 0.05 | Val: 3.0415 (27.71%) | Best: 27.71%\n",
            "Epoch 143 | T: 18s | Train: 3.7991 (14.4%) | Phys: 0.05 | Val: 3.0478 (27.82%) | Best: 27.82%\n",
            "Epoch 144 | T: 19s | Train: 3.8014 (14.4%) | Phys: 0.05 | Val: 3.0392 (27.49%) | Best: 27.82%\n",
            "Epoch 145 | T: 18s | Train: 3.7486 (15.4%) | Phys: 0.05 | Val: 3.0524 (28.46%) | Best: 28.46%\n",
            "Epoch 146 | T: 18s | Train: 3.8238 (14.0%) | Phys: 0.05 | Val: 3.0390 (28.04%) | Best: 28.46%\n",
            "Epoch 147 | T: 18s | Train: 3.7665 (15.2%) | Phys: 0.05 | Val: 3.0374 (28.07%) | Best: 28.46%\n",
            "Epoch 148 | T: 18s | Train: 3.7953 (14.8%) | Phys: 0.05 | Val: 3.0441 (28.02%) | Best: 28.46%\n",
            "Epoch 149 | T: 19s | Train: 3.7542 (15.6%) | Phys: 0.05 | Val: 3.0387 (28.33%) | Best: 28.46%\n",
            "Epoch 150 | T: 18s | Train: 3.7422 (15.7%) | Phys: 0.05 | Val: 3.0496 (28.35%) | Best: 28.46%\n",
            "Epoch 151 | T: 19s | Train: 3.7832 (15.0%) | Phys: 0.05 | Val: 3.0117 (29.25%) | Best: 29.25%\n",
            "Epoch 152 | T: 18s | Train: 3.7577 (15.4%) | Phys: 0.05 | Val: 3.0160 (28.33%) | Best: 29.25%\n",
            "Epoch 153 | T: 18s | Train: 3.7578 (15.2%) | Phys: 0.05 | Val: 2.9986 (29.02%) | Best: 29.25%\n",
            "Epoch 154 | T: 19s | Train: 3.7590 (15.4%) | Phys: 0.05 | Val: 3.0283 (28.65%) | Best: 29.25%\n",
            "Epoch 155 | T: 19s | Train: 3.7692 (15.3%) | Phys: 0.05 | Val: 3.0709 (28.38%) | Best: 29.25%\n",
            "Epoch 156 | T: 19s | Train: 3.7727 (15.2%) | Phys: 0.05 | Val: 3.0115 (28.68%) | Best: 29.25%\n",
            "Epoch 157 | T: 19s | Train: 3.7393 (15.7%) | Phys: 0.05 | Val: 2.9932 (28.48%) | Best: 29.25%\n",
            "Epoch 158 | T: 18s | Train: 3.7794 (15.0%) | Phys: 0.05 | Val: 3.0107 (28.85%) | Best: 29.25%\n",
            "Epoch 159 | T: 19s | Train: 3.7609 (15.2%) | Phys: 0.05 | Val: 2.9729 (29.45%) | Best: 29.45%\n",
            "Epoch 160 | T: 18s | Train: 3.7415 (15.6%) | Phys: 0.05 | Val: 2.9907 (29.38%) | Best: 29.45%\n",
            "Epoch 161 | T: 19s | Train: 3.7587 (15.3%) | Phys: 0.05 | Val: 3.0199 (28.13%) | Best: 29.45%\n",
            "Epoch 162 | T: 19s | Train: 3.7682 (15.2%) | Phys: 0.05 | Val: 2.9967 (28.80%) | Best: 29.45%\n",
            "Epoch 163 | T: 18s | Train: 3.7240 (16.0%) | Phys: 0.05 | Val: 2.9841 (29.81%) | Best: 29.81%\n",
            "Epoch 164 | T: 19s | Train: 3.7724 (15.3%) | Phys: 0.05 | Val: 3.0010 (29.25%) | Best: 29.81%\n",
            "Epoch 165 | T: 18s | Train: 3.7147 (16.2%) | Phys: 0.05 | Val: 2.9714 (29.15%) | Best: 29.81%\n",
            "Epoch 166 | T: 19s | Train: 3.7696 (15.3%) | Phys: 0.05 | Val: 3.0099 (28.92%) | Best: 29.81%\n",
            "Epoch 167 | T: 19s | Train: 3.7370 (15.8%) | Phys: 0.05 | Val: 2.9795 (29.26%) | Best: 29.81%\n",
            "Epoch 168 | T: 19s | Train: 3.7337 (15.8%) | Phys: 0.05 | Val: 2.9440 (29.74%) | Best: 29.81%\n",
            "Epoch 169 | T: 19s | Train: 3.7385 (15.7%) | Phys: 0.05 | Val: 2.9738 (29.55%) | Best: 29.81%\n",
            "Epoch 170 | T: 18s | Train: 3.7258 (16.1%) | Phys: 0.05 | Val: 2.9596 (29.57%) | Best: 29.81%\n",
            "Epoch 171 | T: 19s | Train: 3.7653 (15.2%) | Phys: 0.05 | Val: 2.9711 (29.64%) | Best: 29.81%\n",
            "Epoch 172 | T: 19s | Train: 3.7346 (15.8%) | Phys: 0.05 | Val: 2.9648 (30.13%) | Best: 30.13%\n",
            "Epoch 173 | T: 18s | Train: 3.7672 (15.3%) | Phys: 0.05 | Val: 2.9928 (29.82%) | Best: 30.13%\n",
            "Epoch 174 | T: 18s | Train: 3.7235 (16.0%) | Phys: 0.05 | Val: 2.9558 (29.98%) | Best: 30.13%\n",
            "Epoch 175 | T: 18s | Train: 3.7330 (16.1%) | Phys: 0.05 | Val: 3.0016 (29.03%) | Best: 30.13%\n",
            "Epoch 176 | T: 18s | Train: 3.7420 (15.8%) | Phys: 0.05 | Val: 2.9660 (29.62%) | Best: 30.13%\n",
            "Epoch 177 | T: 18s | Train: 3.7280 (16.1%) | Phys: 0.05 | Val: 2.9478 (30.18%) | Best: 30.18%\n",
            "Epoch 178 | T: 18s | Train: 3.7385 (15.9%) | Phys: 0.05 | Val: 2.9744 (29.95%) | Best: 30.18%\n",
            "Epoch 179 | T: 19s | Train: 3.7303 (16.0%) | Phys: 0.05 | Val: 2.9206 (30.69%) | Best: 30.69%\n",
            "Epoch 180 | T: 18s | Train: 3.7312 (15.9%) | Phys: 0.05 | Val: 2.9575 (29.51%) | Best: 30.69%\n",
            "Epoch 181 | T: 19s | Train: 3.7102 (16.4%) | Phys: 0.05 | Val: 2.9257 (30.00%) | Best: 30.69%\n",
            "Epoch 182 | T: 19s | Train: 3.7218 (16.0%) | Phys: 0.05 | Val: 2.9414 (30.15%) | Best: 30.69%\n",
            "Epoch 183 | T: 18s | Train: 3.7665 (15.4%) | Phys: 0.05 | Val: 2.9866 (30.26%) | Best: 30.69%\n",
            "Epoch 184 | T: 18s | Train: 3.7075 (16.4%) | Phys: 0.05 | Val: 2.9665 (30.29%) | Best: 30.69%\n",
            "Epoch 185 | T: 19s | Train: 3.6722 (17.0%) | Phys: 0.05 | Val: 2.9048 (30.39%) | Best: 30.69%\n",
            "Epoch 186 | T: 18s | Train: 3.7046 (16.4%) | Phys: 0.05 | Val: 2.9194 (30.58%) | Best: 30.69%\n",
            "Epoch 187 | T: 18s | Train: 3.6841 (16.7%) | Phys: 0.05 | Val: 2.9204 (30.55%) | Best: 30.69%\n",
            "Epoch 188 | T: 18s | Train: 3.7119 (16.5%) | Phys: 0.05 | Val: 2.9374 (29.81%) | Best: 30.69%\n",
            "Epoch 189 | T: 19s | Train: 3.6860 (16.8%) | Phys: 0.05 | Val: 2.8676 (31.03%) | Best: 31.03%\n",
            "Epoch 190 | T: 18s | Train: 3.6639 (17.0%) | Phys: 0.05 | Val: 2.9505 (30.33%) | Best: 31.03%\n",
            "Epoch 191 | T: 19s | Train: 3.7197 (16.2%) | Phys: 0.05 | Val: 2.9079 (31.12%) | Best: 31.12%\n",
            "Epoch 192 | T: 18s | Train: 3.7058 (16.5%) | Phys: 0.05 | Val: 2.9238 (30.54%) | Best: 31.12%\n",
            "Epoch 193 | T: 18s | Train: 3.6818 (17.0%) | Phys: 0.05 | Val: 2.9184 (31.09%) | Best: 31.12%\n",
            "Epoch 194 | T: 19s | Train: 3.6964 (16.6%) | Phys: 0.05 | Val: 2.8891 (31.25%) | Best: 31.25%\n",
            "Epoch 195 | T: 19s | Train: 3.7092 (16.4%) | Phys: 0.05 | Val: 2.8968 (31.02%) | Best: 31.25%\n",
            "Epoch 196 | T: 18s | Train: 3.7001 (16.6%) | Phys: 0.05 | Val: 2.8968 (31.10%) | Best: 31.25%\n",
            "Epoch 197 | T: 18s | Train: 3.6998 (16.6%) | Phys: 0.05 | Val: 2.9344 (31.30%) | Best: 31.30%\n",
            "Epoch 198 | T: 19s | Train: 3.6543 (17.4%) | Phys: 0.05 | Val: 2.8936 (31.62%) | Best: 31.62%\n",
            "Epoch 199 | T: 19s | Train: 3.6876 (16.9%) | Phys: 0.05 | Val: 2.8988 (30.80%) | Best: 31.62%\n",
            "Epoch 200 | T: 18s | Train: 3.6841 (17.0%) | Phys: 0.05 | Val: 2.9272 (30.45%) | Best: 31.62%\n",
            "Epoch 201 | T: 19s | Train: 3.6899 (17.0%) | Phys: 0.05 | Val: 2.8677 (31.11%) | Best: 31.62%\n",
            "Epoch 202 | T: 18s | Train: 3.6797 (17.0%) | Phys: 0.05 | Val: 2.8845 (30.94%) | Best: 31.62%\n",
            "Epoch 203 | T: 18s | Train: 3.6478 (17.7%) | Phys: 0.05 | Val: 2.8738 (31.36%) | Best: 31.62%\n",
            "Epoch 204 | T: 18s | Train: 3.6813 (17.0%) | Phys: 0.05 | Val: 2.8816 (31.98%) | Best: 31.98%\n",
            "Epoch 205 | T: 18s | Train: 3.6804 (17.1%) | Phys: 0.05 | Val: 2.8656 (31.32%) | Best: 31.98%\n",
            "Epoch 206 | T: 18s | Train: 3.6543 (17.6%) | Phys: 0.05 | Val: 2.8964 (31.54%) | Best: 31.98%\n",
            "Epoch 207 | T: 18s | Train: 3.6670 (17.4%) | Phys: 0.05 | Val: 2.8531 (31.86%) | Best: 31.98%\n",
            "Epoch 208 | T: 18s | Train: 3.7035 (16.6%) | Phys: 0.05 | Val: 2.8632 (31.71%) | Best: 31.98%\n",
            "Epoch 209 | T: 18s | Train: 3.6439 (17.7%) | Phys: 0.05 | Val: 2.9052 (31.05%) | Best: 31.98%\n",
            "Epoch 210 | T: 18s | Train: 3.6399 (17.8%) | Phys: 0.05 | Val: 2.8726 (31.49%) | Best: 31.98%\n",
            "Epoch 211 | T: 19s | Train: 3.6757 (17.0%) | Phys: 0.05 | Val: 2.8363 (31.89%) | Best: 31.98%\n",
            "Epoch 212 | T: 18s | Train: 3.6554 (17.6%) | Phys: 0.05 | Val: 2.8560 (31.74%) | Best: 31.98%\n",
            "Epoch 213 | T: 18s | Train: 3.6699 (17.4%) | Phys: 0.05 | Val: 2.8785 (31.61%) | Best: 31.98%\n",
            "Epoch 214 | T: 19s | Train: 3.6420 (17.8%) | Phys: 0.05 | Val: 2.8233 (32.58%) | Best: 32.58%\n",
            "Epoch 215 | T: 18s | Train: 3.6967 (16.8%) | Phys: 0.05 | Val: 2.8709 (31.70%) | Best: 32.58%\n",
            "Epoch 216 | T: 19s | Train: 3.6142 (18.3%) | Phys: 0.05 | Val: 2.8593 (32.05%) | Best: 32.58%\n",
            "Epoch 217 | T: 18s | Train: 3.6282 (18.1%) | Phys: 0.05 | Val: 2.8114 (32.43%) | Best: 32.58%\n",
            "Epoch 218 | T: 19s | Train: 3.6340 (18.1%) | Phys: 0.05 | Val: 2.8591 (32.05%) | Best: 32.58%\n",
            "Epoch 219 | T: 19s | Train: 3.6769 (17.2%) | Phys: 0.05 | Val: 2.8663 (32.01%) | Best: 32.58%\n",
            "Epoch 220 | T: 18s | Train: 3.6425 (17.6%) | Phys: 0.05 | Val: 2.8417 (32.18%) | Best: 32.58%\n",
            "Epoch 221 | T: 19s | Train: 3.6633 (17.5%) | Phys: 0.05 | Val: 2.8346 (32.05%) | Best: 32.58%\n",
            "Epoch 222 | T: 18s | Train: 3.6669 (17.3%) | Phys: 0.05 | Val: 2.8357 (32.23%) | Best: 32.58%\n",
            "Epoch 223 | T: 19s | Train: 3.6591 (17.4%) | Phys: 0.05 | Val: 2.8647 (31.38%) | Best: 32.58%\n",
            "Epoch 224 | T: 19s | Train: 3.6534 (17.6%) | Phys: 0.05 | Val: 2.8309 (32.87%) | Best: 32.87%\n",
            "Epoch 225 | T: 18s | Train: 3.6152 (18.3%) | Phys: 0.05 | Val: 2.8397 (32.36%) | Best: 32.87%\n",
            "Epoch 226 | T: 18s | Train: 3.6388 (18.1%) | Phys: 0.05 | Val: 2.8573 (32.74%) | Best: 32.87%\n",
            "Epoch 227 | T: 18s | Train: 3.6272 (18.2%) | Phys: 0.05 | Val: 2.8116 (32.24%) | Best: 32.87%\n",
            "Epoch 228 | T: 18s | Train: 3.6712 (17.3%) | Phys: 0.05 | Val: 2.8574 (32.63%) | Best: 32.87%\n",
            "Epoch 229 | T: 19s | Train: 3.6458 (17.7%) | Phys: 0.05 | Val: 2.8595 (32.08%) | Best: 32.87%\n",
            "Epoch 230 | T: 18s | Train: 3.6477 (17.6%) | Phys: 0.05 | Val: 2.8366 (32.61%) | Best: 32.87%\n",
            "Epoch 231 | T: 19s | Train: 3.5958 (18.9%) | Phys: 0.05 | Val: 2.8441 (32.29%) | Best: 32.87%\n",
            "Epoch 232 | T: 18s | Train: 3.6938 (16.9%) | Phys: 0.06 | Val: 2.8302 (32.49%) | Best: 32.87%\n",
            "Epoch 233 | T: 19s | Train: 3.6338 (18.1%) | Phys: 0.05 | Val: 2.7939 (33.30%) | Best: 33.30%\n",
            "Epoch 234 | T: 19s | Train: 3.6217 (18.1%) | Phys: 0.05 | Val: 2.8255 (32.65%) | Best: 33.30%\n",
            "Epoch 235 | T: 19s | Train: 3.5945 (18.9%) | Phys: 0.05 | Val: 2.7993 (32.95%) | Best: 33.30%\n",
            "Epoch 236 | T: 19s | Train: 3.6594 (17.8%) | Phys: 0.05 | Val: 2.8320 (32.44%) | Best: 33.30%\n",
            "Epoch 237 | T: 19s | Train: 3.6381 (17.9%) | Phys: 0.05 | Val: 2.8490 (32.41%) | Best: 33.30%\n",
            "Epoch 238 | T: 19s | Train: 3.6072 (18.5%) | Phys: 0.05 | Val: 2.8059 (32.89%) | Best: 33.30%\n",
            "Epoch 239 | T: 19s | Train: 3.6299 (18.3%) | Phys: 0.05 | Val: 2.8348 (32.99%) | Best: 33.30%\n",
            "Epoch 240 | T: 19s | Train: 3.6179 (18.6%) | Phys: 0.05 | Val: 2.7950 (33.30%) | Best: 33.30%\n",
            "Epoch 241 | T: 19s | Train: 3.6276 (18.2%) | Phys: 0.05 | Val: 2.7860 (33.92%) | Best: 33.92%\n",
            "Epoch 242 | T: 19s | Train: 3.5867 (19.1%) | Phys: 0.05 | Val: 2.8078 (33.56%) | Best: 33.92%\n",
            "Epoch 243 | T: 19s | Train: 3.6515 (17.9%) | Phys: 0.05 | Val: 2.8152 (32.93%) | Best: 33.92%\n",
            "Epoch 244 | T: 19s | Train: 3.6270 (18.2%) | Phys: 0.05 | Val: 2.7876 (33.18%) | Best: 33.92%\n",
            "Epoch 245 | T: 19s | Train: 3.6168 (18.6%) | Phys: 0.05 | Val: 2.7874 (33.61%) | Best: 33.92%\n",
            "Epoch 246 | T: 18s | Train: 3.6139 (18.5%) | Phys: 0.05 | Val: 2.7916 (33.32%) | Best: 33.92%\n",
            "Epoch 247 | T: 19s | Train: 3.6145 (18.5%) | Phys: 0.05 | Val: 2.7420 (34.00%) | Best: 34.00%\n",
            "Epoch 248 | T: 19s | Train: 3.6134 (18.4%) | Phys: 0.05 | Val: 2.7878 (33.38%) | Best: 34.00%\n",
            "Epoch 249 | T: 19s | Train: 3.5839 (19.0%) | Phys: 0.05 | Val: 2.7848 (33.53%) | Best: 34.00%\n",
            "Epoch 250 | T: 19s | Train: 3.5777 (19.4%) | Phys: 0.05 | Val: 2.8020 (33.60%) | Best: 34.00%\n",
            "Epoch 251 | T: 19s | Train: 3.6281 (18.2%) | Phys: 0.05 | Val: 2.7978 (33.37%) | Best: 34.00%\n",
            "Epoch 252 | T: 18s | Train: 3.5870 (19.2%) | Phys: 0.05 | Val: 2.7609 (33.77%) | Best: 34.00%\n",
            "Epoch 253 | T: 19s | Train: 3.6232 (18.2%) | Phys: 0.05 | Val: 2.7732 (33.88%) | Best: 34.00%\n",
            "Epoch 254 | T: 19s | Train: 3.5726 (19.2%) | Phys: 0.05 | Val: 2.7785 (33.34%) | Best: 34.00%\n",
            "Epoch 255 | T: 19s | Train: 3.6004 (19.0%) | Phys: 0.05 | Val: 2.8032 (33.20%) | Best: 34.00%\n",
            "Epoch 256 | T: 19s | Train: 3.5891 (19.0%) | Phys: 0.05 | Val: 2.7819 (33.64%) | Best: 34.00%\n",
            "Epoch 257 | T: 19s | Train: 3.6410 (18.1%) | Phys: 0.06 | Val: 2.7739 (33.47%) | Best: 34.00%\n",
            "Epoch 258 | T: 19s | Train: 3.5370 (19.9%) | Phys: 0.05 | Val: 2.7354 (34.74%) | Best: 34.74%\n",
            "Epoch 259 | T: 19s | Train: 3.5371 (20.0%) | Phys: 0.05 | Val: 2.8228 (32.96%) | Best: 34.74%\n",
            "Epoch 260 | T: 19s | Train: 3.5715 (19.6%) | Phys: 0.05 | Val: 2.7747 (33.67%) | Best: 34.74%\n",
            "Epoch 261 | T: 19s | Train: 3.6168 (18.8%) | Phys: 0.05 | Val: 2.7980 (33.72%) | Best: 34.74%\n",
            "Epoch 262 | T: 19s | Train: 3.5807 (19.3%) | Phys: 0.05 | Val: 2.7844 (33.71%) | Best: 34.74%\n",
            "Epoch 263 | T: 19s | Train: 3.6243 (18.4%) | Phys: 0.06 | Val: 2.7929 (33.62%) | Best: 34.74%\n",
            "Epoch 264 | T: 19s | Train: 3.5871 (19.2%) | Phys: 0.05 | Val: 2.7738 (33.59%) | Best: 34.74%\n",
            "Epoch 265 | T: 18s | Train: 3.5859 (19.0%) | Phys: 0.05 | Val: 2.7312 (34.25%) | Best: 34.74%\n",
            "Epoch 266 | T: 19s | Train: 3.5765 (19.3%) | Phys: 0.05 | Val: 2.7477 (34.06%) | Best: 34.74%\n",
            "Epoch 267 | T: 18s | Train: 3.5778 (19.5%) | Phys: 0.05 | Val: 2.7078 (34.57%) | Best: 34.74%\n",
            "Epoch 268 | T: 18s | Train: 3.6251 (18.4%) | Phys: 0.06 | Val: 2.7728 (34.39%) | Best: 34.74%\n",
            "Epoch 269 | T: 18s | Train: 3.5716 (19.5%) | Phys: 0.05 | Val: 2.7404 (34.83%) | Best: 34.83%\n",
            "Epoch 270 | T: 18s | Train: 3.5534 (19.7%) | Phys: 0.05 | Val: 2.7662 (34.08%) | Best: 34.83%\n",
            "Epoch 271 | T: 19s | Train: 3.5635 (19.7%) | Phys: 0.06 | Val: 2.7269 (34.82%) | Best: 34.83%\n",
            "Epoch 272 | T: 18s | Train: 3.5802 (19.4%) | Phys: 0.05 | Val: 2.7181 (34.62%) | Best: 34.83%\n",
            "Epoch 273 | T: 19s | Train: 3.6090 (19.0%) | Phys: 0.06 | Val: 2.7624 (34.28%) | Best: 34.83%\n",
            "Epoch 274 | T: 19s | Train: 3.5795 (19.4%) | Phys: 0.05 | Val: 2.7491 (34.16%) | Best: 34.83%\n",
            "Epoch 275 | T: 18s | Train: 3.5634 (19.6%) | Phys: 0.05 | Val: 2.7533 (34.17%) | Best: 34.83%\n",
            "Epoch 276 | T: 19s | Train: 3.5526 (19.8%) | Phys: 0.05 | Val: 2.7689 (33.85%) | Best: 34.83%\n",
            "Epoch 277 | T: 18s | Train: 3.6079 (19.1%) | Phys: 0.05 | Val: 2.7822 (34.31%) | Best: 34.83%\n",
            "Epoch 278 | T: 19s | Train: 3.5751 (19.3%) | Phys: 0.06 | Val: 2.7232 (34.54%) | Best: 34.83%\n",
            "Epoch 279 | T: 19s | Train: 3.5839 (19.5%) | Phys: 0.05 | Val: 2.7168 (34.90%) | Best: 34.90%\n",
            "Epoch 280 | T: 18s | Train: 3.6518 (18.0%) | Phys: 0.06 | Val: 2.7276 (35.03%) | Best: 35.03%\n",
            "Epoch 281 | T: 19s | Train: 3.5729 (19.7%) | Phys: 0.05 | Val: 2.7333 (34.45%) | Best: 35.03%\n",
            "Epoch 282 | T: 18s | Train: 3.5443 (20.1%) | Phys: 0.05 | Val: 2.7231 (35.24%) | Best: 35.24%\n",
            "Epoch 283 | T: 19s | Train: 3.5419 (20.0%) | Phys: 0.05 | Val: 2.7614 (34.15%) | Best: 35.24%\n",
            "Epoch 284 | T: 19s | Train: 3.5632 (19.6%) | Phys: 0.06 | Val: 2.7565 (34.06%) | Best: 35.24%\n",
            "Epoch 285 | T: 19s | Train: 3.5678 (19.7%) | Phys: 0.05 | Val: 2.7127 (35.10%) | Best: 35.24%\n",
            "Epoch 286 | T: 18s | Train: 3.5878 (19.3%) | Phys: 0.05 | Val: 2.7035 (34.43%) | Best: 35.24%\n",
            "Epoch 287 | T: 18s | Train: 3.5365 (20.3%) | Phys: 0.05 | Val: 2.7327 (34.91%) | Best: 35.24%\n",
            "Epoch 288 | T: 19s | Train: 3.5468 (20.0%) | Phys: 0.05 | Val: 2.7275 (34.56%) | Best: 35.24%\n",
            "Epoch 289 | T: 19s | Train: 3.5955 (19.0%) | Phys: 0.06 | Val: 2.7466 (34.41%) | Best: 35.24%\n",
            "Epoch 290 | T: 18s | Train: 3.5308 (20.2%) | Phys: 0.05 | Val: 2.7669 (33.89%) | Best: 35.24%\n",
            "Epoch 291 | T: 19s | Train: 3.5463 (20.0%) | Phys: 0.05 | Val: 2.7001 (35.58%) | Best: 35.58%\n",
            "Epoch 292 | T: 19s | Train: 3.5141 (20.6%) | Phys: 0.05 | Val: 2.7229 (34.92%) | Best: 35.58%\n",
            "Epoch 293 | T: 19s | Train: 3.5508 (19.8%) | Phys: 0.06 | Val: 2.6913 (35.02%) | Best: 35.58%\n",
            "Epoch 294 | T: 19s | Train: 3.5437 (20.2%) | Phys: 0.05 | Val: 2.7290 (35.22%) | Best: 35.58%\n",
            "Epoch 295 | T: 19s | Train: 3.5242 (20.5%) | Phys: 0.05 | Val: 2.7342 (35.29%) | Best: 35.58%\n",
            "Epoch 296 | T: 19s | Train: 3.6240 (18.7%) | Phys: 0.06 | Val: 2.6920 (35.37%) | Best: 35.58%\n",
            "Epoch 297 | T: 18s | Train: 3.5945 (19.0%) | Phys: 0.06 | Val: 2.7123 (34.81%) | Best: 35.58%\n",
            "Epoch 298 | T: 19s | Train: 3.5332 (20.3%) | Phys: 0.05 | Val: 2.7145 (35.49%) | Best: 35.58%\n",
            "Epoch 299 | T: 18s | Train: 3.5631 (19.8%) | Phys: 0.06 | Val: 2.7092 (35.16%) | Best: 35.58%\n",
            "Epoch 300 | T: 19s | Train: 3.5601 (19.9%) | Phys: 0.06 | Val: 2.6689 (36.04%) | Best: 36.04%\n",
            "Epoch 301 | T: 19s | Train: 3.5486 (20.0%) | Phys: 0.06 | Val: 2.7290 (34.68%) | Best: 36.04%\n",
            "Epoch 302 | T: 18s | Train: 3.5473 (20.3%) | Phys: 0.05 | Val: 2.7377 (35.27%) | Best: 36.04%\n",
            "Epoch 303 | T: 19s | Train: 3.5387 (20.2%) | Phys: 0.06 | Val: 2.7065 (35.37%) | Best: 36.04%\n",
            "Epoch 304 | T: 19s | Train: 3.5372 (20.4%) | Phys: 0.06 | Val: 2.7120 (35.43%) | Best: 36.04%\n",
            "Epoch 305 | T: 18s | Train: 3.5098 (20.8%) | Phys: 0.05 | Val: 2.7365 (34.96%) | Best: 36.04%\n",
            "Epoch 306 | T: 19s | Train: 3.4907 (21.2%) | Phys: 0.05 | Val: 2.6713 (35.64%) | Best: 36.04%\n",
            "Epoch 307 | T: 18s | Train: 3.5561 (19.9%) | Phys: 0.06 | Val: 2.6933 (35.74%) | Best: 36.04%\n",
            "Epoch 308 | T: 19s | Train: 3.5114 (20.6%) | Phys: 0.06 | Val: 2.6906 (35.74%) | Best: 36.04%\n",
            "Epoch 309 | T: 19s | Train: 3.5071 (20.8%) | Phys: 0.05 | Val: 2.6628 (36.09%) | Best: 36.09%\n",
            "Epoch 310 | T: 19s | Train: 3.5076 (21.0%) | Phys: 0.05 | Val: 2.7102 (35.32%) | Best: 36.09%\n",
            "Epoch 311 | T: 19s | Train: 3.5485 (20.1%) | Phys: 0.06 | Val: 2.6509 (36.03%) | Best: 36.09%\n",
            "Epoch 312 | T: 19s | Train: 3.5432 (20.3%) | Phys: 0.06 | Val: 2.6384 (36.76%) | Best: 36.76%\n",
            "Epoch 313 | T: 19s | Train: 3.5280 (20.6%) | Phys: 0.05 | Val: 2.6509 (36.21%) | Best: 36.76%\n",
            "Epoch 314 | T: 19s | Train: 3.5643 (19.8%) | Phys: 0.05 | Val: 2.7295 (35.63%) | Best: 36.76%\n",
            "Epoch 315 | T: 19s | Train: 3.5495 (20.3%) | Phys: 0.06 | Val: 2.6696 (36.37%) | Best: 36.76%\n",
            "Epoch 316 | T: 19s | Train: 3.4969 (21.5%) | Phys: 0.05 | Val: 2.6538 (36.00%) | Best: 36.76%\n",
            "Epoch 317 | T: 19s | Train: 3.5102 (21.0%) | Phys: 0.05 | Val: 2.6496 (36.29%) | Best: 36.76%\n",
            "Epoch 318 | T: 19s | Train: 3.5489 (20.1%) | Phys: 0.06 | Val: 2.6814 (35.59%) | Best: 36.76%\n",
            "Epoch 319 | T: 19s | Train: 3.5345 (20.4%) | Phys: 0.05 | Val: 2.6613 (36.48%) | Best: 36.76%\n",
            "Epoch 320 | T: 19s | Train: 3.5179 (20.7%) | Phys: 0.06 | Val: 2.6640 (36.07%) | Best: 36.76%\n",
            "Epoch 321 | T: 19s | Train: 3.5470 (20.2%) | Phys: 0.06 | Val: 2.6559 (36.08%) | Best: 36.76%\n",
            "Epoch 322 | T: 19s | Train: 3.5521 (20.1%) | Phys: 0.06 | Val: 2.6678 (36.19%) | Best: 36.76%\n",
            "Epoch 323 | T: 19s | Train: 3.5523 (20.1%) | Phys: 0.06 | Val: 2.7086 (35.43%) | Best: 36.76%\n",
            "Epoch 324 | T: 19s | Train: 3.5220 (20.6%) | Phys: 0.06 | Val: 2.6579 (36.44%) | Best: 36.76%\n",
            "Epoch 325 | T: 18s | Train: 3.5173 (20.7%) | Phys: 0.06 | Val: 2.6310 (36.85%) | Best: 36.85%\n",
            "Epoch 326 | T: 19s | Train: 3.5318 (20.5%) | Phys: 0.06 | Val: 2.6496 (36.26%) | Best: 36.85%\n",
            "Epoch 327 | T: 18s | Train: 3.5217 (20.7%) | Phys: 0.06 | Val: 2.6933 (35.18%) | Best: 36.85%\n",
            "Epoch 328 | T: 19s | Train: 3.5020 (20.9%) | Phys: 0.06 | Val: 2.6292 (36.33%) | Best: 36.85%\n",
            "Epoch 329 | T: 18s | Train: 3.5329 (20.6%) | Phys: 0.06 | Val: 2.6661 (36.41%) | Best: 36.85%\n",
            "Epoch 330 | T: 18s | Train: 3.5224 (20.5%) | Phys: 0.06 | Val: 2.6725 (36.62%) | Best: 36.85%\n",
            "Epoch 331 | T: 19s | Train: 3.5287 (20.5%) | Phys: 0.06 | Val: 2.6654 (36.51%) | Best: 36.85%\n",
            "Epoch 332 | T: 18s | Train: 3.4955 (21.2%) | Phys: 0.05 | Val: 2.6597 (36.14%) | Best: 36.85%\n",
            "Epoch 333 | T: 18s | Train: 3.5167 (20.8%) | Phys: 0.06 | Val: 2.6549 (36.39%) | Best: 36.85%\n",
            "Epoch 334 | T: 18s | Train: 3.4890 (21.3%) | Phys: 0.06 | Val: 2.6735 (36.26%) | Best: 36.85%\n",
            "Epoch 335 | T: 18s | Train: 3.4865 (21.4%) | Phys: 0.06 | Val: 2.7223 (35.49%) | Best: 36.85%\n",
            "Epoch 336 | T: 18s | Train: 3.5385 (20.4%) | Phys: 0.06 | Val: 2.6574 (35.79%) | Best: 36.85%\n",
            "Epoch 337 | T: 18s | Train: 3.5272 (20.7%) | Phys: 0.06 | Val: 2.6092 (37.10%) | Best: 37.10%\n",
            "Epoch 338 | T: 19s | Train: 3.4704 (21.8%) | Phys: 0.06 | Val: 2.6005 (37.38%) | Best: 37.38%\n",
            "Epoch 339 | T: 18s | Train: 3.4885 (21.4%) | Phys: 0.05 | Val: 2.6374 (37.22%) | Best: 37.38%\n",
            "Epoch 340 | T: 18s | Train: 3.5394 (20.5%) | Phys: 0.06 | Val: 2.6290 (36.53%) | Best: 37.38%\n",
            "Epoch 341 | T: 19s | Train: 3.4984 (21.3%) | Phys: 0.05 | Val: 2.6301 (36.69%) | Best: 37.38%\n",
            "Epoch 342 | T: 18s | Train: 3.4817 (21.7%) | Phys: 0.06 | Val: 2.6232 (36.85%) | Best: 37.38%\n",
            "Epoch 343 | T: 19s | Train: 3.4910 (21.5%) | Phys: 0.06 | Val: 2.6387 (36.53%) | Best: 37.38%\n",
            "Epoch 344 | T: 19s | Train: 3.4376 (22.4%) | Phys: 0.05 | Val: 2.6716 (35.73%) | Best: 37.38%\n",
            "Epoch 345 | T: 18s | Train: 3.4930 (21.5%) | Phys: 0.05 | Val: 2.6366 (36.72%) | Best: 37.38%\n",
            "Epoch 346 | T: 19s | Train: 3.4688 (21.7%) | Phys: 0.06 | Val: 2.6092 (36.88%) | Best: 37.38%\n",
            "Epoch 347 | T: 18s | Train: 3.5134 (20.9%) | Phys: 0.06 | Val: 2.6423 (36.60%) | Best: 37.38%\n",
            "Epoch 348 | T: 18s | Train: 3.5006 (21.3%) | Phys: 0.06 | Val: 2.6180 (37.39%) | Best: 37.39%\n",
            "Epoch 349 | T: 18s | Train: 3.4965 (21.4%) | Phys: 0.05 | Val: 2.6565 (36.54%) | Best: 37.39%\n",
            "Epoch 350 | T: 18s | Train: 3.4582 (22.1%) | Phys: 0.06 | Val: 2.6328 (37.01%) | Best: 37.39%\n",
            "Epoch 351 | T: 19s | Train: 3.4661 (21.9%) | Phys: 0.06 | Val: 2.5690 (37.44%) | Best: 37.44%\n",
            "Epoch 352 | T: 18s | Train: 3.4689 (21.9%) | Phys: 0.06 | Val: 2.6130 (37.20%) | Best: 37.44%\n",
            "Epoch 353 | T: 19s | Train: 3.4548 (22.1%) | Phys: 0.06 | Val: 2.6035 (37.12%) | Best: 37.44%\n",
            "Epoch 354 | T: 19s | Train: 3.4552 (22.1%) | Phys: 0.05 | Val: 2.6333 (36.53%) | Best: 37.44%\n",
            "Epoch 355 | T: 18s | Train: 3.4776 (21.7%) | Phys: 0.06 | Val: 2.6069 (37.32%) | Best: 37.44%\n",
            "Epoch 356 | T: 19s | Train: 3.5095 (21.0%) | Phys: 0.06 | Val: 2.6003 (36.86%) | Best: 37.44%\n",
            "Epoch 357 | T: 19s | Train: 3.4710 (21.8%) | Phys: 0.06 | Val: 2.6054 (37.27%) | Best: 37.44%\n",
            "Epoch 358 | T: 18s | Train: 3.4753 (22.0%) | Phys: 0.06 | Val: 2.5876 (37.14%) | Best: 37.44%\n",
            "Epoch 359 | T: 18s | Train: 3.4548 (22.2%) | Phys: 0.06 | Val: 2.6551 (37.05%) | Best: 37.44%\n",
            "Epoch 360 | T: 19s | Train: 3.4726 (21.9%) | Phys: 0.06 | Val: 2.6269 (37.12%) | Best: 37.44%\n",
            "Epoch 361 | T: 19s | Train: 3.4410 (22.3%) | Phys: 0.06 | Val: 2.6020 (37.58%) | Best: 37.58%\n",
            "Epoch 362 | T: 18s | Train: 3.4966 (21.2%) | Phys: 0.06 | Val: 2.6068 (37.52%) | Best: 37.58%\n",
            "Epoch 363 | T: 19s | Train: 3.4853 (21.7%) | Phys: 0.06 | Val: 2.5993 (37.68%) | Best: 37.68%\n",
            "Epoch 364 | T: 19s | Train: 3.5097 (21.2%) | Phys: 0.06 | Val: 2.6633 (36.64%) | Best: 37.68%\n",
            "Epoch 365 | T: 18s | Train: 3.4886 (21.6%) | Phys: 0.06 | Val: 2.6337 (36.71%) | Best: 37.68%\n",
            "Epoch 366 | T: 19s | Train: 3.4510 (22.4%) | Phys: 0.05 | Val: 2.5701 (38.03%) | Best: 38.03%\n",
            "Epoch 367 | T: 18s | Train: 3.5030 (21.1%) | Phys: 0.06 | Val: 2.6010 (37.42%) | Best: 38.03%\n",
            "Epoch 368 | T: 19s | Train: 3.5009 (21.2%) | Phys: 0.06 | Val: 2.6147 (37.18%) | Best: 38.03%\n",
            "Epoch 369 | T: 18s | Train: 3.4597 (22.0%) | Phys: 0.06 | Val: 2.5439 (37.97%) | Best: 38.03%\n",
            "Epoch 370 | T: 19s | Train: 3.4860 (21.6%) | Phys: 0.06 | Val: 2.6044 (37.69%) | Best: 38.03%\n",
            "Epoch 371 | T: 19s | Train: 3.4888 (21.5%) | Phys: 0.06 | Val: 2.5886 (37.52%) | Best: 38.03%\n",
            "Epoch 372 | T: 18s | Train: 3.4925 (21.5%) | Phys: 0.06 | Val: 2.6085 (37.32%) | Best: 38.03%\n",
            "Epoch 373 | T: 19s | Train: 3.4995 (21.5%) | Phys: 0.06 | Val: 2.5872 (37.84%) | Best: 38.03%\n",
            "Epoch 374 | T: 19s | Train: 3.4871 (21.6%) | Phys: 0.06 | Val: 2.5760 (38.56%) | Best: 38.56%\n",
            "Epoch 375 | T: 18s | Train: 3.4558 (22.0%) | Phys: 0.06 | Val: 2.5763 (37.58%) | Best: 38.56%\n",
            "Epoch 376 | T: 19s | Train: 3.4884 (21.7%) | Phys: 0.06 | Val: 2.6025 (37.38%) | Best: 38.56%\n",
            "Epoch 377 | T: 18s | Train: 3.4302 (22.8%) | Phys: 0.05 | Val: 2.6061 (36.97%) | Best: 38.56%\n",
            "Epoch 378 | T: 18s | Train: 3.4613 (22.1%) | Phys: 0.06 | Val: 2.6184 (37.80%) | Best: 38.56%\n",
            "Epoch 379 | T: 18s | Train: 3.4940 (21.5%) | Phys: 0.06 | Val: 2.6151 (37.59%) | Best: 38.56%\n",
            "Epoch 380 | T: 19s | Train: 3.4400 (22.6%) | Phys: 0.06 | Val: 2.5719 (37.79%) | Best: 38.56%\n",
            "Epoch 381 | T: 19s | Train: 3.4477 (22.4%) | Phys: 0.06 | Val: 2.5854 (37.67%) | Best: 38.56%\n",
            "Epoch 382 | T: 18s | Train: 3.4393 (22.7%) | Phys: 0.06 | Val: 2.6057 (37.63%) | Best: 38.56%\n",
            "Epoch 383 | T: 19s | Train: 3.4774 (21.7%) | Phys: 0.06 | Val: 2.5922 (38.06%) | Best: 38.56%\n",
            "Epoch 384 | T: 18s | Train: 3.4784 (21.7%) | Phys: 0.06 | Val: 2.6275 (36.88%) | Best: 38.56%\n",
            "Epoch 385 | T: 18s | Train: 3.4502 (22.5%) | Phys: 0.06 | Val: 2.5750 (37.80%) | Best: 38.56%\n",
            "Epoch 386 | T: 19s | Train: 3.3954 (23.5%) | Phys: 0.06 | Val: 2.5609 (38.10%) | Best: 38.56%\n",
            "Epoch 387 | T: 18s | Train: 3.4039 (23.4%) | Phys: 0.06 | Val: 2.5981 (37.15%) | Best: 38.56%\n",
            "Epoch 388 | T: 18s | Train: 3.4136 (23.0%) | Phys: 0.06 | Val: 2.5767 (37.80%) | Best: 38.56%\n",
            "Epoch 389 | T: 18s | Train: 3.4930 (21.6%) | Phys: 0.06 | Val: 2.5853 (37.27%) | Best: 38.56%\n",
            "Epoch 390 | T: 19s | Train: 3.4911 (21.7%) | Phys: 0.06 | Val: 2.6003 (37.95%) | Best: 38.56%\n",
            "Epoch 391 | T: 19s | Train: 3.4592 (22.1%) | Phys: 0.06 | Val: 2.5581 (38.27%) | Best: 38.56%\n",
            "Epoch 392 | T: 18s | Train: 3.3945 (23.6%) | Phys: 0.06 | Val: 2.5783 (38.43%) | Best: 38.56%\n",
            "Epoch 393 | T: 19s | Train: 3.4295 (22.8%) | Phys: 0.06 | Val: 2.5843 (37.39%) | Best: 38.56%\n",
            "Epoch 394 | T: 18s | Train: 3.5046 (21.5%) | Phys: 0.06 | Val: 2.5782 (37.80%) | Best: 38.56%\n",
            "Epoch 395 | T: 18s | Train: 3.4472 (22.7%) | Phys: 0.06 | Val: 2.6291 (37.61%) | Best: 38.56%\n",
            "Epoch 396 | T: 19s | Train: 3.5132 (21.2%) | Phys: 0.06 | Val: 2.5758 (38.11%) | Best: 38.56%\n",
            "Epoch 397 | T: 18s | Train: 3.4536 (22.4%) | Phys: 0.06 | Val: 2.5837 (38.19%) | Best: 38.56%\n",
            "Epoch 398 | T: 18s | Train: 3.4691 (22.1%) | Phys: 0.06 | Val: 2.5621 (37.98%) | Best: 38.56%\n",
            "Epoch 399 | T: 19s | Train: 3.4196 (22.9%) | Phys: 0.06 | Val: 2.5732 (38.16%) | Best: 38.56%\n",
            "Epoch 400 | T: 19s | Train: 3.4803 (21.9%) | Phys: 0.06 | Val: 2.5430 (38.70%) | Best: 38.70%\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": [],
      "name": "CIFAR-100 MLP Strong",
      "gpuType": "L4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}