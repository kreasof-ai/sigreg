{
  "cells": [
    {
      "cell_type": "code",
      "id": "pBxX3I1kYwojaamntSxiNZTK",
      "metadata": {
        "tags": [],
        "id": "pBxX3I1kYwojaamntSxiNZTK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d33d569a-761a-4fee-8c8d-ba72345e613b"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "import math\n",
        "import time\n",
        "\n",
        "# ==========================================\n",
        "# 1. Configuration (Tuned for ViT on CIFAR)\n",
        "# ==========================================\n",
        "REG_MODE = 'baseline'\n",
        "SIGR_ALPHA = 0.1   # Strength of the physics constraint\n",
        "SKETCH_DIM = 64    # Dimension of the random observer\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "LEARNING_RATE = 1e-3  # Slightly higher initial LR for AdamW with cosine schedule\n",
        "EPOCHS = 400\n",
        "WEIGHT_DECAY = 0.05\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "if torch.backends.mps.is_available(): DEVICE = 'mps'\n",
        "\n",
        "# Regularization Config\n",
        "MIXUP_ALPHA = 0.8\n",
        "CUTMIX_ALPHA = 1.0\n",
        "DROP_PATH_RATE = 0.1\n",
        "\n",
        "print(f\"Training on device: {DEVICE}\")\n",
        "\n",
        "# ==========================================\n",
        "# 2. Data Preparation (THE FIX: Strong Augmentation)\n",
        "# ==========================================\n",
        "def get_data_loaders():\n",
        "    print('==> Preparing data with Strong Augmentation...')\n",
        "\n",
        "    mean = (0.5071, 0.4867, 0.4408)\n",
        "    std = (0.2675, 0.2565, 0.2761)\n",
        "\n",
        "    # FIX 1: Add RandAugment\n",
        "    transform_train = transforms.Compose([\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandAugment(num_ops=2, magnitude=9), # <--- CRITICAL FOR ViT\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean, std),\n",
        "    ])\n",
        "\n",
        "    transform_test = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean, std),\n",
        "    ])\n",
        "\n",
        "    trainset = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=transform_train)\n",
        "    # Increase workers to handle augmentation load\n",
        "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
        "\n",
        "    testset = torchvision.datasets.CIFAR100(root='./data', train=False, download=True, transform=transform_test)\n",
        "    testloader = torch.utils.data.DataLoader(testset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
        "\n",
        "    return trainloader, testloader\n",
        "\n",
        "\n",
        "# ------------------------------------------\n",
        "# Physics Engine: The Regularizers\n",
        "# ------------------------------------------\n",
        "\n",
        "def sigreg_weak_loss(x, sketch_dim=64):\n",
        "    \"\"\"\n",
        "    Forces Covariance(x) ~ Identity.\n",
        "    Matches the 2nd Moment (Spherical Cloud).\n",
        "    \"\"\"\n",
        "    N, C = x.size()\n",
        "    # 1. Sketching (Optional for C=512, but good for consistency)\n",
        "    if C > sketch_dim:\n",
        "        S = torch.randn(sketch_dim, C, device=x.device) / (C ** 0.5)\n",
        "        x = x @ S.T  # [N, sketch_dim]\n",
        "    else:\n",
        "        sketch_dim = C\n",
        "\n",
        "    # 2. Centering & Covariance\n",
        "    x = x - x.mean(dim=0, keepdim=True)\n",
        "    cov = (x.T @ x) / (N - 1 + 1e-6)\n",
        "\n",
        "    # 3. Target Identity\n",
        "    target = torch.eye(sketch_dim, device=x.device)\n",
        "\n",
        "    # 4. Off-diagonal suppression + Diagonal maintenance\n",
        "    return torch.norm(cov - target, p='fro')\n",
        "\n",
        "def sigreg_strong_loss(x, sketch_dim=64):\n",
        "    \"\"\"\n",
        "    Forces ECF(x) ~ ECF(Gaussian).\n",
        "    Matches ALL Moments (Maximum Entropy Cloud).\n",
        "    Exact implementation of LeJEPA Algorithm 1.\n",
        "    \"\"\"\n",
        "    N, C = x.size()\n",
        "\n",
        "    # 1. Projection (The Observer)\n",
        "    # Project channels down to sketch_dim\n",
        "    A = torch.randn(C, sketch_dim, device=x.device)\n",
        "    A = A / (A.norm(p=2, dim=0, keepdim=True) + 1e-6)\n",
        "\n",
        "    # 2. Integration Points\n",
        "    t = torch.linspace(-5, 5, 17, device=x.device)\n",
        "\n",
        "    # 3. Theoretical Gaussian CF\n",
        "    exp_f = torch.exp(-0.5 * t**2)\n",
        "\n",
        "    # 4. Empirical CF\n",
        "    # proj: [N, sketch_dim]\n",
        "    proj = x @ A\n",
        "\n",
        "    # args: [N, sketch_dim, T]\n",
        "    args = proj.unsqueeze(2) * t.view(1, 1, -1)\n",
        "\n",
        "    # ecf: [sketch_dim, T] (Mean over batch)\n",
        "    ecf = torch.exp(1j * args).mean(dim=0)\n",
        "\n",
        "    # 5. Weighted L2 Distance\n",
        "    # |ecf - gauss|^2 * gauss_weight\n",
        "    diff_sq = (ecf - exp_f.unsqueeze(0)).abs().square()\n",
        "    err = diff_sq * exp_f.unsqueeze(0)\n",
        "\n",
        "    # 6. Integrate\n",
        "    loss = torch.trapz(err, t, dim=1) * N\n",
        "\n",
        "    return loss.mean()\n",
        "\n",
        "# ==========================================\n",
        "# 3. Mixup / CutMix Utilities\n",
        "# ==========================================\n",
        "def rand_bbox(size, lam):\n",
        "    W = size[2]\n",
        "    H = size[3]\n",
        "    cut_rat = np.sqrt(1. - lam)\n",
        "    cut_w = int(W * cut_rat)\n",
        "    cut_h = int(H * cut_rat)\n",
        "\n",
        "    cx = np.random.randint(W)\n",
        "    cy = np.random.randint(H)\n",
        "\n",
        "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
        "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
        "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
        "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
        "\n",
        "    return bbx1, bby1, bbx2, bby2\n",
        "\n",
        "def drop_path(x, drop_prob: float = 0., training: bool = False):\n",
        "    if drop_prob == 0. or not training: return x\n",
        "    keep_prob = 1 - drop_prob\n",
        "    shape = (x.shape[0],) + (1,) * (x.ndim - 1)\n",
        "    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
        "    random_tensor.floor_()\n",
        "    return x.div(keep_prob) * random_tensor\n",
        "\n",
        "class DropPath(nn.Module):\n",
        "    def __init__(self, drop_prob=None):\n",
        "        super(DropPath, self).__init__()\n",
        "        self.drop_prob = drop_prob\n",
        "    def forward(self, x): return drop_path(x, self.drop_prob, self.training)\n",
        "\n",
        "class PatchEmbedding(nn.Module):\n",
        "    def __init__(self, img_size=32, patch_size=4, in_chans=3, embed_dim=192):\n",
        "        super().__init__()\n",
        "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "    def forward(self, x):\n",
        "        x = self.proj(x).flatten(2).transpose(1, 2)\n",
        "        return x\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, dim, num_heads=8, qkv_bias=False, attn_drop=0., proj_drop=0.):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        head_dim = dim // num_heads\n",
        "        self.scale = head_dim ** -0.5\n",
        "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
        "        self.attn_drop = nn.Dropout(attn_drop)\n",
        "        self.proj = nn.Linear(dim, dim)\n",
        "        self.proj_drop = nn.Dropout(proj_drop)\n",
        "    def forward(self, x):\n",
        "        B, N, C = x.shape\n",
        "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        attn = self.attn_drop(attn)\n",
        "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
        "        x = self.proj(x)\n",
        "        x = self.proj_drop(x)\n",
        "        return x\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, dim, hidden_dim, drop=0.):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(dim, hidden_dim)\n",
        "        self.act = nn.GELU()\n",
        "        self.fc2 = nn.Linear(hidden_dim, dim)\n",
        "        self.drop = nn.Dropout(drop)\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.drop(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.drop(x)\n",
        "        return x\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, dim, num_heads, mlp_ratio=4., drop=0., attn_drop=0., drop_path=0., reg_mode='strong', sketch_dim=64):\n",
        "        super().__init__()\n",
        "        self.norm1 = nn.LayerNorm(dim)\n",
        "        self.attn = MultiHeadAttention(dim, num_heads=num_heads, qkv_bias=True, attn_drop=attn_drop, proj_drop=drop)\n",
        "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
        "        self.norm2 = nn.LayerNorm(dim)\n",
        "        self.mlp = MLP(dim, int(dim * mlp_ratio), drop=drop)\n",
        "\n",
        "        self.reg_mode = reg_mode\n",
        "        self.sketch_dim = sketch_dim\n",
        "    def forward(self, x):\n",
        "        x = x + self.drop_path(self.attn(self.norm1(x)))\n",
        "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
        "\n",
        "\n",
        "        # --- PHYSICS INJECTION ---\n",
        "        reg_loss = torch.tensor(0.0, device=x.device)\n",
        "        if self.reg_mode != 'baseline':\n",
        "            # Global Average Pool of the tokens [B, N, C] -> [B, C]\n",
        "            # This represents the \"Image Vector\" at this depth\n",
        "            flat_rep = x.mean(dim=1)\n",
        "\n",
        "            # Crucial: Pre-Norm vs Post-Norm context.\n",
        "            # LayerNorm forces variance=1. SIGReg forces Distribution=Gaussian.\n",
        "            # They are compatible.\n",
        "            if self.reg_mode == 'weak':\n",
        "                reg_loss = sigreg_weak_loss(flat_rep, self.sketch_dim)\n",
        "            elif self.reg_mode == 'strong':\n",
        "                reg_loss = sigreg_strong_loss(flat_rep, self.sketch_dim)\n",
        "\n",
        "        return x, reg_loss\n",
        "\n",
        "class VisionTransformer(nn.Module):\n",
        "    def __init__(self, img_size=32, patch_size=4, in_chans=3, num_classes=100, embed_dim=192, depth=9, num_heads=3, mlp_ratio=4., drop_rate=0., attn_drop_rate=0., drop_path_rate=0.1, reg_mode='strong', sketch_dim=64):\n",
        "        super().__init__()\n",
        "        self.patch_embed = PatchEmbedding(img_size, patch_size, in_chans, embed_dim)\n",
        "        num_patches = self.patch_embed.proj.weight.shape[2] # Just a hack to get patch count logic\n",
        "        num_patches = (img_size // patch_size) ** 2\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n",
        "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]\n",
        "        self.blocks = nn.ModuleList([\n",
        "            TransformerBlock(embed_dim, num_heads, mlp_ratio, drop_rate, attn_drop_rate, dpr[i], reg_mode, sketch_dim)\n",
        "            for i in range(depth)])\n",
        "        self.norm = nn.LayerNorm(embed_dim)\n",
        "        self.head = nn.Linear(embed_dim, num_classes)\n",
        "        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n",
        "        nn.init.trunc_normal_(self.cls_token, std=0.02)\n",
        "        nn.init.xavier_uniform_(self.head.weight)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B = x.shape[0]\n",
        "        x = self.patch_embed(x)\n",
        "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
        "        x = torch.cat((cls_tokens, x), dim=1)\n",
        "        x = x + self.pos_embed\n",
        "\n",
        "        total_phys_loss = 0.0\n",
        "\n",
        "        # Pass through blocks\n",
        "        for blk in self.blocks:\n",
        "            x, l_loss = blk(x, )\n",
        "            total_phys_loss += l_loss\n",
        "\n",
        "        x = self.norm(x)\n",
        "        out = self.head(x[:, 0])\n",
        "        return out, (total_phys_loss / len(self.blocks))\n",
        "\n",
        "# ==========================================\n",
        "# 5. Training Engine (Updated for Mixup/CutMix)\n",
        "# ==========================================\n",
        "def train(epoch, net, trainloader, optimizer, criterion):\n",
        "    net.train()\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    phys_loss_meter = 0\n",
        "\n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
        "\n",
        "        # Apply Mixup/CutMix\n",
        "        r = np.random.rand(1)\n",
        "        if r < 0.5: # Mixup\n",
        "            lam = np.random.beta(MIXUP_ALPHA, MIXUP_ALPHA)\n",
        "            index = torch.randperm(inputs.size(0)).to(DEVICE)\n",
        "            inputs = lam * inputs + (1 - lam) * inputs[index, :]\n",
        "            targets_a, targets_b = targets, targets[index]\n",
        "        else: # CutMix\n",
        "            lam = np.random.beta(CUTMIX_ALPHA, CUTMIX_ALPHA)\n",
        "            rand_index = torch.randperm(inputs.size(0)).to(DEVICE)\n",
        "            target_a = targets\n",
        "            target_b = targets[rand_index]\n",
        "            bbx1, bby1, bbx2, bby2 = rand_bbox(inputs.size(), lam)\n",
        "            inputs[:, :, bbx1:bbx2, bby1:bby2] = inputs[rand_index, :, bbx1:bbx2, bby1:bby2]\n",
        "            lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (inputs.size()[-1] * inputs.size()[-2]))\n",
        "            targets_a, targets_b = target_a, target_b\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward\n",
        "        outputs, p_loss = net(inputs)\n",
        "\n",
        "        # Task Loss\n",
        "        c_loss = criterion(outputs, targets_a) * lam + criterion(outputs, targets_b) * (1. - lam)\n",
        "\n",
        "        # Total Loss\n",
        "        loss = (1 - SIGR_ALPHA) * c_loss + (SIGR_ALPHA * p_loss)\n",
        "\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(net.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += ((1 - SIGR_ALPHA) * c_loss).item() # Log only task loss for comparison\n",
        "        phys_loss_meter += (SIGR_ALPHA * p_loss).item()\n",
        "\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += (lam * predicted.eq(targets_a).float() + (1 - lam) * predicted.eq(targets_b).float()).sum().item()\n",
        "\n",
        "    acc = 100. * correct / total\n",
        "    return train_loss / (batch_idx + 1), acc, phys_loss_meter / (batch_idx + 1)\n",
        "\n",
        "def test(epoch, net, testloader, criterion):\n",
        "    net.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "            inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
        "            outputs, _ = net(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "    acc = 100. * correct / total\n",
        "    return test_loss / (batch_idx + 1), acc\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    trainloader, testloader = get_data_loaders()\n",
        "\n",
        "    net = VisionTransformer(img_size=32, patch_size=4, embed_dim=192, depth=9, num_heads=3, drop_path_rate=0.1, reg_mode=REG_MODE, sketch_dim=SKETCH_DIM)\n",
        "    net = net.to(DEVICE)\n",
        "\n",
        "    # Standard CrossEntropy for final eval, SoftLabel for training is handled by Mixup logic\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # FIX 3: Robust AdamW Baseline\n",
        "    optimizer = optim.AdamW(net.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
        "\n",
        "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS, eta_min=1e-5)\n",
        "\n",
        "    print(f\"Starting training for {EPOCHS} epochs with RandAugment + Mixup/CutMix...\")\n",
        "\n",
        "    best_acc = 0\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        start_time = time.time()\n",
        "\n",
        "        train_loss, train_acc, physics_loss = train(epoch, net, trainloader, optimizer, criterion)\n",
        "        test_loss, test_acc = test(epoch, net, testloader, criterion)\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        if test_acc > best_acc:\n",
        "            best_acc = test_acc\n",
        "            # torch.save(net.state_dict(), f'thermo_resnet_{REG_MODE}.pth')\n",
        "\n",
        "        epoch_time = time.time() - start_time\n",
        "\n",
        "        print(f\"Epoch {epoch+1} | T: {epoch_time:.0f}s | \"\n",
        "              f\"Train: {train_loss:.4f} ({train_acc:.1f}%) | \"\n",
        "              f\"Phys: {physics_loss:.2f} | \"\n",
        "              f\"Val: {test_loss:.4f} ({test_acc:.2f}%) | \"\n",
        "              f\"Best: {best_acc:.2f}%\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on device: cuda\n",
            "==> Preparing data with Strong Augmentation...\n",
            "Starting training for 400 epochs with RandAugment + Mixup/CutMix...\n",
            "Epoch 1 | T: 25s | Train: 4.0297 (3.3%) | Phys: 0.00 | Val: 4.0361 (6.97%) | Best: 6.97%\n",
            "Epoch 2 | T: 25s | Train: 3.8663 (5.3%) | Phys: 0.00 | Val: 3.9067 (10.93%) | Best: 10.93%\n",
            "Epoch 3 | T: 25s | Train: 3.7919 (6.7%) | Phys: 0.00 | Val: 3.7084 (13.56%) | Best: 13.56%\n",
            "Epoch 4 | T: 25s | Train: 3.7467 (7.7%) | Phys: 0.00 | Val: 3.5948 (15.96%) | Best: 15.96%\n",
            "Epoch 5 | T: 25s | Train: 3.7000 (8.6%) | Phys: 0.00 | Val: 3.4576 (17.38%) | Best: 17.38%\n",
            "Epoch 6 | T: 25s | Train: 3.6431 (9.6%) | Phys: 0.00 | Val: 3.3769 (18.93%) | Best: 18.93%\n",
            "Epoch 7 | T: 25s | Train: 3.6302 (9.9%) | Phys: 0.00 | Val: 3.3414 (20.58%) | Best: 20.58%\n",
            "Epoch 8 | T: 25s | Train: 3.5988 (10.6%) | Phys: 0.00 | Val: 3.2972 (21.35%) | Best: 21.35%\n",
            "Epoch 9 | T: 25s | Train: 3.5690 (11.4%) | Phys: 0.00 | Val: 3.1463 (22.97%) | Best: 22.97%\n",
            "Epoch 10 | T: 25s | Train: 3.5645 (11.4%) | Phys: 0.00 | Val: 3.2044 (22.83%) | Best: 22.97%\n",
            "Epoch 11 | T: 25s | Train: 3.5007 (12.6%) | Phys: 0.00 | Val: 3.0743 (25.29%) | Best: 25.29%\n",
            "Epoch 12 | T: 25s | Train: 3.5121 (12.5%) | Phys: 0.00 | Val: 3.0847 (25.11%) | Best: 25.29%\n",
            "Epoch 13 | T: 25s | Train: 3.4640 (13.4%) | Phys: 0.00 | Val: 3.0705 (25.74%) | Best: 25.74%\n",
            "Epoch 14 | T: 25s | Train: 3.4743 (13.5%) | Phys: 0.00 | Val: 3.1150 (25.18%) | Best: 25.74%\n",
            "Epoch 15 | T: 25s | Train: 3.4606 (13.6%) | Phys: 0.00 | Val: 3.0316 (27.49%) | Best: 27.49%\n",
            "Epoch 16 | T: 25s | Train: 3.4239 (14.5%) | Phys: 0.00 | Val: 2.9253 (29.53%) | Best: 29.53%\n",
            "Epoch 17 | T: 25s | Train: 3.3945 (14.9%) | Phys: 0.00 | Val: 2.8615 (30.23%) | Best: 30.23%\n",
            "Epoch 18 | T: 26s | Train: 3.4003 (15.0%) | Phys: 0.00 | Val: 2.8038 (30.63%) | Best: 30.63%\n",
            "Epoch 19 | T: 25s | Train: 3.3591 (15.9%) | Phys: 0.00 | Val: 2.7990 (31.94%) | Best: 31.94%\n",
            "Epoch 20 | T: 25s | Train: 3.3585 (15.9%) | Phys: 0.00 | Val: 2.7590 (31.88%) | Best: 31.94%\n",
            "Epoch 21 | T: 25s | Train: 3.3306 (16.5%) | Phys: 0.00 | Val: 2.7546 (32.33%) | Best: 32.33%\n",
            "Epoch 22 | T: 25s | Train: 3.2906 (17.5%) | Phys: 0.00 | Val: 2.7348 (33.01%) | Best: 33.01%\n",
            "Epoch 23 | T: 25s | Train: 3.3142 (16.9%) | Phys: 0.00 | Val: 2.6335 (34.74%) | Best: 34.74%\n",
            "Epoch 24 | T: 25s | Train: 3.2593 (18.2%) | Phys: 0.00 | Val: 2.6111 (35.40%) | Best: 35.40%\n",
            "Epoch 25 | T: 25s | Train: 3.2386 (18.8%) | Phys: 0.00 | Val: 2.5685 (36.02%) | Best: 36.02%\n",
            "Epoch 26 | T: 25s | Train: 3.2341 (18.8%) | Phys: 0.00 | Val: 2.6036 (35.55%) | Best: 36.02%\n",
            "Epoch 27 | T: 25s | Train: 3.2285 (18.8%) | Phys: 0.00 | Val: 2.6008 (35.79%) | Best: 36.02%\n",
            "Epoch 28 | T: 25s | Train: 3.2270 (19.1%) | Phys: 0.00 | Val: 2.5135 (37.81%) | Best: 37.81%\n",
            "Epoch 29 | T: 25s | Train: 3.1980 (19.6%) | Phys: 0.00 | Val: 2.5486 (37.18%) | Best: 37.81%\n",
            "Epoch 30 | T: 25s | Train: 3.2065 (19.4%) | Phys: 0.00 | Val: 2.5048 (38.46%) | Best: 38.46%\n",
            "Epoch 31 | T: 25s | Train: 3.1766 (20.2%) | Phys: 0.00 | Val: 2.4735 (38.33%) | Best: 38.46%\n",
            "Epoch 32 | T: 25s | Train: 3.1254 (21.4%) | Phys: 0.00 | Val: 2.3574 (40.72%) | Best: 40.72%\n",
            "Epoch 33 | T: 25s | Train: 3.1429 (20.7%) | Phys: 0.00 | Val: 2.4184 (39.58%) | Best: 40.72%\n",
            "Epoch 34 | T: 25s | Train: 3.1132 (21.8%) | Phys: 0.00 | Val: 2.3391 (42.09%) | Best: 42.09%\n",
            "Epoch 35 | T: 25s | Train: 3.0701 (22.7%) | Phys: 0.00 | Val: 2.3623 (40.97%) | Best: 42.09%\n",
            "Epoch 36 | T: 25s | Train: 3.0752 (22.6%) | Phys: 0.00 | Val: 2.2818 (43.13%) | Best: 43.13%\n",
            "Epoch 37 | T: 25s | Train: 3.0850 (22.4%) | Phys: 0.00 | Val: 2.2220 (44.02%) | Best: 44.02%\n",
            "Epoch 38 | T: 25s | Train: 3.0100 (24.1%) | Phys: 0.00 | Val: 2.2637 (43.00%) | Best: 44.02%\n",
            "Epoch 39 | T: 25s | Train: 2.9844 (24.7%) | Phys: 0.00 | Val: 2.1860 (44.82%) | Best: 44.82%\n",
            "Epoch 40 | T: 25s | Train: 3.0640 (22.8%) | Phys: 0.00 | Val: 2.2483 (43.70%) | Best: 44.82%\n",
            "Epoch 41 | T: 25s | Train: 2.9897 (24.8%) | Phys: 0.00 | Val: 2.2184 (44.25%) | Best: 44.82%\n",
            "Epoch 42 | T: 25s | Train: 2.9844 (25.0%) | Phys: 0.00 | Val: 2.1663 (45.77%) | Best: 45.77%\n",
            "Epoch 43 | T: 25s | Train: 2.9704 (25.0%) | Phys: 0.00 | Val: 2.0995 (46.88%) | Best: 46.88%\n",
            "Epoch 44 | T: 25s | Train: 2.9490 (25.8%) | Phys: 0.00 | Val: 2.1058 (46.57%) | Best: 46.88%\n",
            "Epoch 45 | T: 25s | Train: 2.9777 (25.0%) | Phys: 0.00 | Val: 2.1266 (46.36%) | Best: 46.88%\n",
            "Epoch 46 | T: 25s | Train: 2.9681 (25.6%) | Phys: 0.00 | Val: 2.0840 (47.51%) | Best: 47.51%\n",
            "Epoch 47 | T: 25s | Train: 2.8985 (27.1%) | Phys: 0.00 | Val: 2.0387 (48.22%) | Best: 48.22%\n",
            "Epoch 48 | T: 25s | Train: 2.9045 (26.9%) | Phys: 0.00 | Val: 2.0434 (47.87%) | Best: 48.22%\n",
            "Epoch 49 | T: 25s | Train: 2.8689 (27.8%) | Phys: 0.00 | Val: 2.0686 (47.36%) | Best: 48.22%\n",
            "Epoch 50 | T: 25s | Train: 2.9163 (26.8%) | Phys: 0.00 | Val: 2.0685 (47.79%) | Best: 48.22%\n",
            "Epoch 51 | T: 25s | Train: 2.8843 (28.0%) | Phys: 0.00 | Val: 2.0475 (48.25%) | Best: 48.25%\n",
            "Epoch 52 | T: 25s | Train: 2.8518 (28.3%) | Phys: 0.00 | Val: 1.9761 (50.35%) | Best: 50.35%\n",
            "Epoch 53 | T: 25s | Train: 2.8285 (29.0%) | Phys: 0.00 | Val: 1.9559 (49.78%) | Best: 50.35%\n",
            "Epoch 54 | T: 25s | Train: 2.8549 (28.2%) | Phys: 0.00 | Val: 1.9725 (49.87%) | Best: 50.35%\n",
            "Epoch 55 | T: 25s | Train: 2.8447 (28.6%) | Phys: 0.00 | Val: 1.9407 (50.97%) | Best: 50.97%\n",
            "Epoch 56 | T: 25s | Train: 2.8327 (28.7%) | Phys: 0.00 | Val: 1.9440 (50.18%) | Best: 50.97%\n",
            "Epoch 57 | T: 25s | Train: 2.8051 (29.3%) | Phys: 0.00 | Val: 1.8879 (51.93%) | Best: 51.93%\n",
            "Epoch 58 | T: 25s | Train: 2.8269 (29.1%) | Phys: 0.00 | Val: 1.9203 (51.70%) | Best: 51.93%\n",
            "Epoch 59 | T: 25s | Train: 2.8432 (28.7%) | Phys: 0.00 | Val: 1.9264 (51.10%) | Best: 51.93%\n",
            "Epoch 60 | T: 25s | Train: 2.8111 (29.5%) | Phys: 0.00 | Val: 1.9205 (51.79%) | Best: 51.93%\n",
            "Epoch 61 | T: 25s | Train: 2.8026 (29.5%) | Phys: 0.00 | Val: 1.8807 (51.85%) | Best: 51.93%\n",
            "Epoch 62 | T: 25s | Train: 2.7874 (30.1%) | Phys: 0.00 | Val: 1.8985 (51.09%) | Best: 51.93%\n",
            "Epoch 63 | T: 25s | Train: 2.7842 (30.0%) | Phys: 0.00 | Val: 1.8483 (52.11%) | Best: 52.11%\n",
            "Epoch 64 | T: 25s | Train: 2.8112 (29.5%) | Phys: 0.00 | Val: 1.8222 (53.23%) | Best: 53.23%\n",
            "Epoch 65 | T: 25s | Train: 2.7635 (30.5%) | Phys: 0.00 | Val: 1.8396 (52.61%) | Best: 53.23%\n",
            "Epoch 66 | T: 25s | Train: 2.7505 (31.0%) | Phys: 0.00 | Val: 1.8451 (52.50%) | Best: 53.23%\n",
            "Epoch 67 | T: 25s | Train: 2.7533 (30.8%) | Phys: 0.00 | Val: 1.8490 (52.78%) | Best: 53.23%\n",
            "Epoch 68 | T: 25s | Train: 2.7450 (31.4%) | Phys: 0.00 | Val: 1.8342 (52.75%) | Best: 53.23%\n",
            "Epoch 69 | T: 25s | Train: 2.7381 (31.1%) | Phys: 0.00 | Val: 1.8026 (53.79%) | Best: 53.79%\n",
            "Epoch 70 | T: 25s | Train: 2.7535 (31.1%) | Phys: 0.00 | Val: 1.7847 (54.16%) | Best: 54.16%\n",
            "Epoch 71 | T: 25s | Train: 2.7576 (31.0%) | Phys: 0.00 | Val: 1.8359 (52.45%) | Best: 54.16%\n",
            "Epoch 72 | T: 25s | Train: 2.7429 (31.5%) | Phys: 0.00 | Val: 1.7942 (53.86%) | Best: 54.16%\n",
            "Epoch 73 | T: 25s | Train: 2.7511 (31.0%) | Phys: 0.00 | Val: 1.8348 (53.11%) | Best: 54.16%\n",
            "Epoch 74 | T: 25s | Train: 2.7286 (31.5%) | Phys: 0.00 | Val: 1.8176 (53.32%) | Best: 54.16%\n",
            "Epoch 75 | T: 25s | Train: 2.7356 (31.5%) | Phys: 0.00 | Val: 1.7866 (54.22%) | Best: 54.22%\n",
            "Epoch 76 | T: 25s | Train: 2.7251 (31.9%) | Phys: 0.00 | Val: 1.8050 (53.50%) | Best: 54.22%\n",
            "Epoch 77 | T: 25s | Train: 2.6920 (32.9%) | Phys: 0.00 | Val: 1.7804 (54.44%) | Best: 54.44%\n",
            "Epoch 78 | T: 25s | Train: 2.7403 (31.6%) | Phys: 0.00 | Val: 1.7502 (55.16%) | Best: 55.16%\n",
            "Epoch 79 | T: 25s | Train: 2.6999 (32.7%) | Phys: 0.00 | Val: 1.7558 (54.79%) | Best: 55.16%\n",
            "Epoch 80 | T: 25s | Train: 2.6822 (32.8%) | Phys: 0.00 | Val: 1.7317 (55.45%) | Best: 55.45%\n",
            "Epoch 81 | T: 25s | Train: 2.6630 (33.4%) | Phys: 0.00 | Val: 1.7583 (54.77%) | Best: 55.45%\n",
            "Epoch 82 | T: 25s | Train: 2.6327 (34.1%) | Phys: 0.00 | Val: 1.7618 (54.99%) | Best: 55.45%\n",
            "Epoch 83 | T: 25s | Train: 2.6651 (33.4%) | Phys: 0.00 | Val: 1.7372 (55.68%) | Best: 55.68%\n",
            "Epoch 84 | T: 25s | Train: 2.6828 (32.9%) | Phys: 0.00 | Val: 1.7565 (55.00%) | Best: 55.68%\n",
            "Epoch 85 | T: 25s | Train: 2.6136 (34.8%) | Phys: 0.00 | Val: 1.7390 (55.57%) | Best: 55.68%\n",
            "Epoch 86 | T: 25s | Train: 2.5917 (35.4%) | Phys: 0.00 | Val: 1.7216 (55.77%) | Best: 55.77%\n",
            "Epoch 87 | T: 25s | Train: 2.6673 (33.4%) | Phys: 0.00 | Val: 1.7045 (56.14%) | Best: 56.14%\n",
            "Epoch 88 | T: 25s | Train: 2.6190 (34.6%) | Phys: 0.00 | Val: 1.7165 (55.88%) | Best: 56.14%\n",
            "Epoch 89 | T: 25s | Train: 2.6261 (34.3%) | Phys: 0.00 | Val: 1.7521 (54.85%) | Best: 56.14%\n",
            "Epoch 90 | T: 25s | Train: 2.5825 (35.6%) | Phys: 0.00 | Val: 1.6719 (56.04%) | Best: 56.14%\n",
            "Epoch 91 | T: 25s | Train: 2.5952 (35.2%) | Phys: 0.00 | Val: 1.7174 (56.04%) | Best: 56.14%\n",
            "Epoch 92 | T: 25s | Train: 2.5702 (35.9%) | Phys: 0.00 | Val: 1.6953 (55.99%) | Best: 56.14%\n",
            "Epoch 93 | T: 25s | Train: 2.6137 (35.0%) | Phys: 0.00 | Val: 1.6855 (56.65%) | Best: 56.65%\n",
            "Epoch 94 | T: 25s | Train: 2.5208 (36.9%) | Phys: 0.00 | Val: 1.6415 (57.78%) | Best: 57.78%\n",
            "Epoch 95 | T: 25s | Train: 2.6019 (35.0%) | Phys: 0.00 | Val: 1.6724 (56.90%) | Best: 57.78%\n",
            "Epoch 96 | T: 25s | Train: 2.5757 (35.8%) | Phys: 0.00 | Val: 1.6549 (57.15%) | Best: 57.78%\n",
            "Epoch 97 | T: 25s | Train: 2.6120 (35.0%) | Phys: 0.00 | Val: 1.7086 (56.65%) | Best: 57.78%\n",
            "Epoch 98 | T: 25s | Train: 2.5963 (35.3%) | Phys: 0.00 | Val: 1.6757 (56.97%) | Best: 57.78%\n",
            "Epoch 99 | T: 25s | Train: 2.5786 (35.8%) | Phys: 0.00 | Val: 1.7067 (56.41%) | Best: 57.78%\n",
            "Epoch 100 | T: 25s | Train: 2.5239 (37.5%) | Phys: 0.00 | Val: 1.6670 (57.09%) | Best: 57.78%\n",
            "Epoch 101 | T: 25s | Train: 2.5758 (35.7%) | Phys: 0.00 | Val: 1.6736 (57.18%) | Best: 57.78%\n",
            "Epoch 102 | T: 25s | Train: 2.5421 (36.6%) | Phys: 0.00 | Val: 1.6487 (57.59%) | Best: 57.78%\n",
            "Epoch 103 | T: 25s | Train: 2.5707 (35.9%) | Phys: 0.00 | Val: 1.6577 (57.10%) | Best: 57.78%\n",
            "Epoch 104 | T: 25s | Train: 2.5430 (36.5%) | Phys: 0.00 | Val: 1.6062 (57.70%) | Best: 57.78%\n",
            "Epoch 105 | T: 25s | Train: 2.5348 (37.1%) | Phys: 0.00 | Val: 1.6327 (57.47%) | Best: 57.78%\n",
            "Epoch 106 | T: 25s | Train: 2.5531 (36.7%) | Phys: 0.00 | Val: 1.6510 (57.84%) | Best: 57.84%\n",
            "Epoch 107 | T: 25s | Train: 2.5149 (37.4%) | Phys: 0.00 | Val: 1.5884 (58.38%) | Best: 58.38%\n",
            "Epoch 108 | T: 25s | Train: 2.5468 (36.6%) | Phys: 0.00 | Val: 1.6694 (57.63%) | Best: 58.38%\n",
            "Epoch 109 | T: 25s | Train: 2.5521 (36.5%) | Phys: 0.00 | Val: 1.6136 (57.88%) | Best: 58.38%\n",
            "Epoch 110 | T: 25s | Train: 2.5002 (37.9%) | Phys: 0.00 | Val: 1.5807 (58.45%) | Best: 58.45%\n",
            "Epoch 111 | T: 25s | Train: 2.5196 (37.4%) | Phys: 0.00 | Val: 1.6087 (58.10%) | Best: 58.45%\n",
            "Epoch 112 | T: 25s | Train: 2.4613 (39.1%) | Phys: 0.00 | Val: 1.5765 (58.48%) | Best: 58.48%\n",
            "Epoch 113 | T: 25s | Train: 2.4990 (37.9%) | Phys: 0.00 | Val: 1.5974 (59.15%) | Best: 59.15%\n",
            "Epoch 114 | T: 25s | Train: 2.4711 (38.5%) | Phys: 0.00 | Val: 1.5929 (58.17%) | Best: 59.15%\n",
            "Epoch 115 | T: 25s | Train: 2.4302 (39.7%) | Phys: 0.00 | Val: 1.5719 (59.04%) | Best: 59.15%\n",
            "Epoch 116 | T: 25s | Train: 2.5024 (37.9%) | Phys: 0.00 | Val: 1.5816 (58.57%) | Best: 59.15%\n",
            "Epoch 117 | T: 25s | Train: 2.4765 (38.7%) | Phys: 0.00 | Val: 1.5939 (58.24%) | Best: 59.15%\n",
            "Epoch 118 | T: 25s | Train: 2.4483 (39.6%) | Phys: 0.00 | Val: 1.5512 (59.91%) | Best: 59.91%\n",
            "Epoch 119 | T: 25s | Train: 2.5187 (37.6%) | Phys: 0.00 | Val: 1.5999 (58.97%) | Best: 59.91%\n",
            "Epoch 120 | T: 25s | Train: 2.4691 (38.9%) | Phys: 0.00 | Val: 1.5413 (59.78%) | Best: 59.91%\n",
            "Epoch 121 | T: 25s | Train: 2.4336 (39.7%) | Phys: 0.00 | Val: 1.5547 (59.54%) | Best: 59.91%\n",
            "Epoch 122 | T: 25s | Train: 2.4553 (39.2%) | Phys: 0.00 | Val: 1.5799 (59.23%) | Best: 59.91%\n",
            "Epoch 123 | T: 25s | Train: 2.4572 (39.5%) | Phys: 0.00 | Val: 1.5658 (58.89%) | Best: 59.91%\n",
            "Epoch 124 | T: 25s | Train: 2.4467 (39.4%) | Phys: 0.00 | Val: 1.5510 (59.73%) | Best: 59.91%\n",
            "Epoch 125 | T: 25s | Train: 2.4385 (39.8%) | Phys: 0.00 | Val: 1.5699 (59.40%) | Best: 59.91%\n",
            "Epoch 126 | T: 25s | Train: 2.3978 (40.7%) | Phys: 0.00 | Val: 1.5392 (60.04%) | Best: 60.04%\n",
            "Epoch 127 | T: 25s | Train: 2.4557 (39.2%) | Phys: 0.00 | Val: 1.5966 (58.63%) | Best: 60.04%\n",
            "Epoch 128 | T: 25s | Train: 2.4468 (39.4%) | Phys: 0.00 | Val: 1.5584 (59.92%) | Best: 60.04%\n",
            "Epoch 129 | T: 25s | Train: 2.4780 (38.7%) | Phys: 0.00 | Val: 1.5749 (59.16%) | Best: 60.04%\n",
            "Epoch 130 | T: 25s | Train: 2.4230 (40.0%) | Phys: 0.00 | Val: 1.5347 (59.21%) | Best: 60.04%\n",
            "Epoch 131 | T: 25s | Train: 2.4154 (40.4%) | Phys: 0.00 | Val: 1.5322 (60.11%) | Best: 60.11%\n",
            "Epoch 132 | T: 25s | Train: 2.5017 (38.3%) | Phys: 0.00 | Val: 1.5514 (59.84%) | Best: 60.11%\n",
            "Epoch 133 | T: 25s | Train: 2.4064 (40.9%) | Phys: 0.00 | Val: 1.5753 (59.81%) | Best: 60.11%\n",
            "Epoch 134 | T: 25s | Train: 2.4512 (39.5%) | Phys: 0.00 | Val: 1.5123 (60.23%) | Best: 60.23%\n",
            "Epoch 135 | T: 25s | Train: 2.4424 (39.9%) | Phys: 0.00 | Val: 1.5682 (59.70%) | Best: 60.23%\n",
            "Epoch 136 | T: 25s | Train: 2.4342 (40.2%) | Phys: 0.00 | Val: 1.5539 (59.87%) | Best: 60.23%\n",
            "Epoch 137 | T: 25s | Train: 2.4052 (40.8%) | Phys: 0.00 | Val: 1.5623 (59.82%) | Best: 60.23%\n",
            "Epoch 138 | T: 25s | Train: 2.3484 (42.2%) | Phys: 0.00 | Val: 1.5323 (59.94%) | Best: 60.23%\n",
            "Epoch 139 | T: 25s | Train: 2.3323 (42.7%) | Phys: 0.00 | Val: 1.5181 (60.40%) | Best: 60.40%\n",
            "Epoch 140 | T: 25s | Train: 2.3479 (42.3%) | Phys: 0.00 | Val: 1.5124 (60.58%) | Best: 60.58%\n",
            "Epoch 141 | T: 25s | Train: 2.3533 (42.3%) | Phys: 0.00 | Val: 1.5364 (60.20%) | Best: 60.58%\n",
            "Epoch 142 | T: 25s | Train: 2.3911 (41.1%) | Phys: 0.00 | Val: 1.5559 (58.99%) | Best: 60.58%\n",
            "Epoch 143 | T: 25s | Train: 2.4119 (40.6%) | Phys: 0.00 | Val: 1.5106 (61.11%) | Best: 61.11%\n",
            "Epoch 144 | T: 25s | Train: 2.4232 (40.6%) | Phys: 0.00 | Val: 1.5395 (60.00%) | Best: 61.11%\n",
            "Epoch 145 | T: 25s | Train: 2.3612 (42.0%) | Phys: 0.00 | Val: 1.5274 (60.82%) | Best: 61.11%\n",
            "Epoch 146 | T: 25s | Train: 2.3114 (43.2%) | Phys: 0.00 | Val: 1.5051 (60.44%) | Best: 61.11%\n",
            "Epoch 147 | T: 25s | Train: 2.3364 (42.5%) | Phys: 0.00 | Val: 1.4829 (61.22%) | Best: 61.22%\n",
            "Epoch 148 | T: 25s | Train: 2.2899 (43.9%) | Phys: 0.00 | Val: 1.5161 (60.64%) | Best: 61.22%\n",
            "Epoch 149 | T: 25s | Train: 2.3417 (42.5%) | Phys: 0.00 | Val: 1.4976 (60.80%) | Best: 61.22%\n",
            "Epoch 150 | T: 25s | Train: 2.3059 (43.5%) | Phys: 0.00 | Val: 1.4573 (61.91%) | Best: 61.91%\n",
            "Epoch 151 | T: 25s | Train: 2.3606 (42.0%) | Phys: 0.00 | Val: 1.4759 (61.18%) | Best: 61.91%\n",
            "Epoch 152 | T: 25s | Train: 2.2798 (44.3%) | Phys: 0.00 | Val: 1.4764 (61.96%) | Best: 61.96%\n",
            "Epoch 153 | T: 25s | Train: 2.2932 (43.9%) | Phys: 0.00 | Val: 1.4761 (61.45%) | Best: 61.96%\n",
            "Epoch 154 | T: 25s | Train: 2.3671 (42.4%) | Phys: 0.00 | Val: 1.5206 (60.46%) | Best: 61.96%\n",
            "Epoch 155 | T: 25s | Train: 2.2619 (44.8%) | Phys: 0.00 | Val: 1.4435 (62.05%) | Best: 62.05%\n",
            "Epoch 156 | T: 25s | Train: 2.3052 (43.9%) | Phys: 0.00 | Val: 1.5278 (61.12%) | Best: 62.05%\n",
            "Epoch 157 | T: 25s | Train: 2.2499 (45.0%) | Phys: 0.00 | Val: 1.4588 (62.09%) | Best: 62.09%\n",
            "Epoch 158 | T: 25s | Train: 2.3131 (43.3%) | Phys: 0.00 | Val: 1.4460 (62.11%) | Best: 62.11%\n",
            "Epoch 159 | T: 25s | Train: 2.3376 (42.8%) | Phys: 0.00 | Val: 1.4532 (61.71%) | Best: 62.11%\n",
            "Epoch 160 | T: 25s | Train: 2.3000 (43.6%) | Phys: 0.00 | Val: 1.4511 (61.90%) | Best: 62.11%\n",
            "Epoch 161 | T: 25s | Train: 2.3303 (43.1%) | Phys: 0.00 | Val: 1.4750 (61.74%) | Best: 62.11%\n",
            "Epoch 162 | T: 25s | Train: 2.2965 (44.1%) | Phys: 0.00 | Val: 1.4954 (60.81%) | Best: 62.11%\n",
            "Epoch 163 | T: 25s | Train: 2.2816 (44.6%) | Phys: 0.00 | Val: 1.4637 (61.56%) | Best: 62.11%\n",
            "Epoch 164 | T: 25s | Train: 2.3162 (43.6%) | Phys: 0.00 | Val: 1.4435 (62.02%) | Best: 62.11%\n",
            "Epoch 165 | T: 25s | Train: 2.2605 (44.9%) | Phys: 0.00 | Val: 1.4413 (61.95%) | Best: 62.11%\n",
            "Epoch 166 | T: 25s | Train: 2.2745 (44.7%) | Phys: 0.00 | Val: 1.4122 (63.27%) | Best: 63.27%\n",
            "Epoch 167 | T: 25s | Train: 2.2339 (45.4%) | Phys: 0.00 | Val: 1.4440 (61.71%) | Best: 63.27%\n",
            "Epoch 168 | T: 25s | Train: 2.2890 (44.5%) | Phys: 0.00 | Val: 1.4326 (62.02%) | Best: 63.27%\n",
            "Epoch 169 | T: 25s | Train: 2.2187 (46.1%) | Phys: 0.00 | Val: 1.4612 (62.38%) | Best: 63.27%\n",
            "Epoch 170 | T: 25s | Train: 2.2258 (45.8%) | Phys: 0.00 | Val: 1.4394 (62.41%) | Best: 63.27%\n",
            "Epoch 171 | T: 25s | Train: 2.2768 (44.6%) | Phys: 0.00 | Val: 1.4624 (61.80%) | Best: 63.27%\n",
            "Epoch 172 | T: 26s | Train: 2.2930 (43.8%) | Phys: 0.00 | Val: 1.4515 (62.35%) | Best: 63.27%\n",
            "Epoch 173 | T: 26s | Train: 2.2568 (45.1%) | Phys: 0.00 | Val: 1.4263 (63.28%) | Best: 63.28%\n",
            "Epoch 174 | T: 25s | Train: 2.2671 (44.7%) | Phys: 0.00 | Val: 1.4001 (63.35%) | Best: 63.35%\n",
            "Epoch 175 | T: 25s | Train: 2.2177 (46.3%) | Phys: 0.00 | Val: 1.4362 (62.51%) | Best: 63.35%\n",
            "Epoch 176 | T: 25s | Train: 2.2110 (46.4%) | Phys: 0.00 | Val: 1.3963 (63.49%) | Best: 63.49%\n",
            "Epoch 177 | T: 25s | Train: 2.2707 (44.7%) | Phys: 0.00 | Val: 1.4525 (62.68%) | Best: 63.49%\n",
            "Epoch 178 | T: 25s | Train: 2.1955 (46.9%) | Phys: 0.00 | Val: 1.4092 (63.53%) | Best: 63.53%\n",
            "Epoch 179 | T: 25s | Train: 2.2379 (46.0%) | Phys: 0.00 | Val: 1.3825 (63.65%) | Best: 63.65%\n",
            "Epoch 180 | T: 25s | Train: 2.1625 (47.8%) | Phys: 0.00 | Val: 1.4113 (63.21%) | Best: 63.65%\n",
            "Epoch 181 | T: 25s | Train: 2.2222 (46.4%) | Phys: 0.00 | Val: 1.4255 (62.98%) | Best: 63.65%\n",
            "Epoch 182 | T: 25s | Train: 2.2260 (46.2%) | Phys: 0.00 | Val: 1.4184 (63.14%) | Best: 63.65%\n",
            "Epoch 183 | T: 25s | Train: 2.1773 (47.7%) | Phys: 0.00 | Val: 1.4065 (63.29%) | Best: 63.65%\n",
            "Epoch 184 | T: 25s | Train: 2.1391 (48.3%) | Phys: 0.00 | Val: 1.4156 (63.03%) | Best: 63.65%\n",
            "Epoch 185 | T: 25s | Train: 2.2099 (46.9%) | Phys: 0.00 | Val: 1.4389 (63.08%) | Best: 63.65%\n",
            "Epoch 186 | T: 25s | Train: 2.1691 (47.7%) | Phys: 0.00 | Val: 1.4382 (63.12%) | Best: 63.65%\n",
            "Epoch 187 | T: 25s | Train: 2.2340 (45.7%) | Phys: 0.00 | Val: 1.4009 (63.12%) | Best: 63.65%\n",
            "Epoch 188 | T: 25s | Train: 2.2674 (44.8%) | Phys: 0.00 | Val: 1.3964 (63.74%) | Best: 63.74%\n",
            "Epoch 189 | T: 25s | Train: 2.2257 (46.2%) | Phys: 0.00 | Val: 1.3787 (64.34%) | Best: 64.34%\n",
            "Epoch 190 | T: 25s | Train: 2.1750 (47.3%) | Phys: 0.00 | Val: 1.4059 (63.51%) | Best: 64.34%\n",
            "Epoch 191 | T: 25s | Train: 2.1613 (48.1%) | Phys: 0.00 | Val: 1.4044 (63.59%) | Best: 64.34%\n",
            "Epoch 192 | T: 25s | Train: 2.0976 (49.5%) | Phys: 0.00 | Val: 1.3975 (63.42%) | Best: 64.34%\n",
            "Epoch 193 | T: 25s | Train: 2.1995 (46.9%) | Phys: 0.00 | Val: 1.3919 (63.61%) | Best: 64.34%\n",
            "Epoch 194 | T: 25s | Train: 2.1483 (48.1%) | Phys: 0.00 | Val: 1.4027 (63.63%) | Best: 64.34%\n",
            "Epoch 195 | T: 25s | Train: 2.1017 (49.6%) | Phys: 0.00 | Val: 1.4004 (63.32%) | Best: 64.34%\n",
            "Epoch 196 | T: 25s | Train: 2.1878 (47.2%) | Phys: 0.00 | Val: 1.3986 (63.47%) | Best: 64.34%\n",
            "Epoch 197 | T: 25s | Train: 2.1378 (48.4%) | Phys: 0.00 | Val: 1.3733 (63.75%) | Best: 64.34%\n",
            "Epoch 198 | T: 25s | Train: 2.1322 (48.6%) | Phys: 0.00 | Val: 1.3839 (64.18%) | Best: 64.34%\n",
            "Epoch 199 | T: 25s | Train: 2.1520 (48.3%) | Phys: 0.00 | Val: 1.3761 (63.81%) | Best: 64.34%\n",
            "Epoch 200 | T: 25s | Train: 2.1316 (49.2%) | Phys: 0.00 | Val: 1.3799 (63.96%) | Best: 64.34%\n",
            "Epoch 201 | T: 25s | Train: 2.1223 (48.9%) | Phys: 0.00 | Val: 1.3824 (63.98%) | Best: 64.34%\n",
            "Epoch 202 | T: 25s | Train: 2.1535 (48.2%) | Phys: 0.00 | Val: 1.3597 (64.28%) | Best: 64.34%\n",
            "Epoch 203 | T: 25s | Train: 2.2053 (47.1%) | Phys: 0.00 | Val: 1.3981 (63.61%) | Best: 64.34%\n",
            "Epoch 204 | T: 25s | Train: 2.1771 (47.7%) | Phys: 0.00 | Val: 1.3835 (64.00%) | Best: 64.34%\n",
            "Epoch 205 | T: 25s | Train: 2.0930 (50.2%) | Phys: 0.00 | Val: 1.3876 (63.78%) | Best: 64.34%\n",
            "Epoch 206 | T: 25s | Train: 2.1367 (48.6%) | Phys: 0.00 | Val: 1.3747 (64.66%) | Best: 64.66%\n",
            "Epoch 207 | T: 25s | Train: 2.1240 (48.8%) | Phys: 0.00 | Val: 1.3940 (63.99%) | Best: 64.66%\n",
            "Epoch 208 | T: 25s | Train: 2.1502 (48.5%) | Phys: 0.00 | Val: 1.3600 (64.99%) | Best: 64.99%\n",
            "Epoch 209 | T: 25s | Train: 2.2017 (47.1%) | Phys: 0.00 | Val: 1.3515 (65.47%) | Best: 65.47%\n",
            "Epoch 210 | T: 25s | Train: 2.0703 (50.8%) | Phys: 0.00 | Val: 1.3568 (65.22%) | Best: 65.47%\n",
            "Epoch 211 | T: 25s | Train: 2.1113 (49.6%) | Phys: 0.00 | Val: 1.3562 (64.83%) | Best: 65.47%\n",
            "Epoch 212 | T: 25s | Train: 2.1044 (49.6%) | Phys: 0.00 | Val: 1.3967 (63.95%) | Best: 65.47%\n",
            "Epoch 213 | T: 25s | Train: 2.0428 (51.4%) | Phys: 0.00 | Val: 1.3655 (65.05%) | Best: 65.47%\n",
            "Epoch 214 | T: 25s | Train: 2.1239 (49.4%) | Phys: 0.00 | Val: 1.3624 (65.09%) | Best: 65.47%\n",
            "Epoch 215 | T: 25s | Train: 2.1309 (49.1%) | Phys: 0.00 | Val: 1.3616 (65.24%) | Best: 65.47%\n",
            "Epoch 216 | T: 25s | Train: 2.1457 (48.2%) | Phys: 0.00 | Val: 1.3455 (64.78%) | Best: 65.47%\n",
            "Epoch 217 | T: 25s | Train: 2.1374 (48.9%) | Phys: 0.00 | Val: 1.3548 (65.03%) | Best: 65.47%\n",
            "Epoch 218 | T: 25s | Train: 2.1006 (49.7%) | Phys: 0.00 | Val: 1.3306 (65.75%) | Best: 65.75%\n",
            "Epoch 219 | T: 25s | Train: 2.0098 (52.6%) | Phys: 0.00 | Val: 1.3553 (65.18%) | Best: 65.75%\n",
            "Epoch 220 | T: 25s | Train: 2.0808 (50.3%) | Phys: 0.00 | Val: 1.3457 (64.85%) | Best: 65.75%\n",
            "Epoch 221 | T: 25s | Train: 2.1039 (49.6%) | Phys: 0.00 | Val: 1.3530 (64.24%) | Best: 65.75%\n",
            "Epoch 222 | T: 25s | Train: 2.0204 (52.0%) | Phys: 0.00 | Val: 1.3507 (65.08%) | Best: 65.75%\n",
            "Epoch 223 | T: 25s | Train: 2.0393 (51.8%) | Phys: 0.00 | Val: 1.3409 (65.52%) | Best: 65.75%\n",
            "Epoch 224 | T: 25s | Train: 2.0480 (51.3%) | Phys: 0.00 | Val: 1.3229 (65.73%) | Best: 65.75%\n",
            "Epoch 225 | T: 25s | Train: 2.0768 (50.6%) | Phys: 0.00 | Val: 1.3193 (65.44%) | Best: 65.75%\n",
            "Epoch 226 | T: 25s | Train: 2.1334 (48.9%) | Phys: 0.00 | Val: 1.3302 (65.15%) | Best: 65.75%\n",
            "Epoch 227 | T: 25s | Train: 2.0373 (51.7%) | Phys: 0.00 | Val: 1.3413 (65.41%) | Best: 65.75%\n",
            "Epoch 228 | T: 25s | Train: 2.0437 (51.7%) | Phys: 0.00 | Val: 1.3234 (65.37%) | Best: 65.75%\n",
            "Epoch 229 | T: 25s | Train: 1.9934 (52.6%) | Phys: 0.00 | Val: 1.3146 (65.91%) | Best: 65.91%\n",
            "Epoch 230 | T: 25s | Train: 2.0494 (51.3%) | Phys: 0.00 | Val: 1.3240 (65.90%) | Best: 65.91%\n",
            "Epoch 231 | T: 25s | Train: 2.0377 (51.8%) | Phys: 0.00 | Val: 1.3095 (66.30%) | Best: 66.30%\n",
            "Epoch 232 | T: 25s | Train: 2.0955 (49.9%) | Phys: 0.00 | Val: 1.3223 (65.57%) | Best: 66.30%\n",
            "Epoch 233 | T: 25s | Train: 2.0626 (51.0%) | Phys: 0.00 | Val: 1.3508 (65.17%) | Best: 66.30%\n",
            "Epoch 234 | T: 25s | Train: 2.0137 (52.6%) | Phys: 0.00 | Val: 1.3187 (65.65%) | Best: 66.30%\n",
            "Epoch 235 | T: 25s | Train: 2.0212 (52.2%) | Phys: 0.00 | Val: 1.3233 (66.30%) | Best: 66.30%\n",
            "Epoch 236 | T: 25s | Train: 2.0209 (52.0%) | Phys: 0.00 | Val: 1.3054 (65.90%) | Best: 66.30%\n",
            "Epoch 237 | T: 25s | Train: 2.0589 (51.4%) | Phys: 0.00 | Val: 1.3241 (65.79%) | Best: 66.30%\n",
            "Epoch 238 | T: 25s | Train: 2.0192 (52.1%) | Phys: 0.00 | Val: 1.3044 (66.27%) | Best: 66.30%\n",
            "Epoch 239 | T: 25s | Train: 2.0258 (52.0%) | Phys: 0.00 | Val: 1.3001 (66.03%) | Best: 66.30%\n",
            "Epoch 240 | T: 25s | Train: 2.0673 (51.0%) | Phys: 0.00 | Val: 1.3227 (66.12%) | Best: 66.30%\n",
            "Epoch 241 | T: 25s | Train: 1.9316 (54.8%) | Phys: 0.00 | Val: 1.3124 (65.59%) | Best: 66.30%\n",
            "Epoch 242 | T: 25s | Train: 1.9598 (53.7%) | Phys: 0.00 | Val: 1.3226 (65.74%) | Best: 66.30%\n",
            "Epoch 243 | T: 25s | Train: 2.0049 (52.5%) | Phys: 0.00 | Val: 1.2945 (66.19%) | Best: 66.30%\n",
            "Epoch 244 | T: 25s | Train: 2.0121 (52.5%) | Phys: 0.00 | Val: 1.3231 (66.01%) | Best: 66.30%\n",
            "Epoch 245 | T: 25s | Train: 1.9958 (53.0%) | Phys: 0.00 | Val: 1.2963 (66.43%) | Best: 66.43%\n",
            "Epoch 246 | T: 25s | Train: 1.9620 (54.0%) | Phys: 0.00 | Val: 1.3289 (65.84%) | Best: 66.43%\n",
            "Epoch 247 | T: 25s | Train: 1.9889 (53.1%) | Phys: 0.00 | Val: 1.2803 (66.52%) | Best: 66.52%\n",
            "Epoch 248 | T: 25s | Train: 2.0487 (51.3%) | Phys: 0.00 | Val: 1.2960 (66.45%) | Best: 66.52%\n",
            "Epoch 249 | T: 25s | Train: 2.0232 (52.3%) | Phys: 0.00 | Val: 1.3000 (66.60%) | Best: 66.60%\n",
            "Epoch 250 | T: 25s | Train: 1.9732 (53.5%) | Phys: 0.00 | Val: 1.3187 (66.34%) | Best: 66.60%\n",
            "Epoch 251 | T: 25s | Train: 1.9781 (53.6%) | Phys: 0.00 | Val: 1.3123 (66.43%) | Best: 66.60%\n",
            "Epoch 252 | T: 25s | Train: 1.9326 (54.5%) | Phys: 0.00 | Val: 1.2945 (67.17%) | Best: 67.17%\n",
            "Epoch 253 | T: 25s | Train: 2.0022 (52.8%) | Phys: 0.00 | Val: 1.3192 (66.72%) | Best: 67.17%\n",
            "Epoch 254 | T: 25s | Train: 2.0199 (52.5%) | Phys: 0.00 | Val: 1.3113 (66.41%) | Best: 67.17%\n",
            "Epoch 255 | T: 25s | Train: 1.9756 (53.3%) | Phys: 0.00 | Val: 1.2936 (66.84%) | Best: 67.17%\n",
            "Epoch 256 | T: 25s | Train: 1.9293 (54.6%) | Phys: 0.00 | Val: 1.2657 (67.68%) | Best: 67.68%\n",
            "Epoch 257 | T: 25s | Train: 1.8949 (56.2%) | Phys: 0.00 | Val: 1.2801 (66.98%) | Best: 67.68%\n",
            "Epoch 258 | T: 25s | Train: 1.8737 (56.3%) | Phys: 0.00 | Val: 1.2980 (66.56%) | Best: 67.68%\n",
            "Epoch 259 | T: 25s | Train: 1.8336 (57.6%) | Phys: 0.00 | Val: 1.2896 (66.75%) | Best: 67.68%\n",
            "Epoch 260 | T: 25s | Train: 1.9608 (54.1%) | Phys: 0.00 | Val: 1.2870 (66.75%) | Best: 67.68%\n",
            "Epoch 261 | T: 25s | Train: 1.9712 (53.9%) | Phys: 0.00 | Val: 1.3087 (66.42%) | Best: 67.68%\n",
            "Epoch 262 | T: 25s | Train: 1.8233 (57.8%) | Phys: 0.00 | Val: 1.3035 (66.67%) | Best: 67.68%\n",
            "Epoch 263 | T: 25s | Train: 2.0321 (51.8%) | Phys: 0.00 | Val: 1.2799 (66.94%) | Best: 67.68%\n",
            "Epoch 264 | T: 25s | Train: 1.9322 (54.8%) | Phys: 0.00 | Val: 1.2678 (67.38%) | Best: 67.68%\n",
            "Epoch 265 | T: 25s | Train: 1.8781 (55.9%) | Phys: 0.00 | Val: 1.2574 (67.46%) | Best: 67.68%\n",
            "Epoch 266 | T: 25s | Train: 1.8220 (57.6%) | Phys: 0.00 | Val: 1.2521 (68.04%) | Best: 68.04%\n",
            "Epoch 267 | T: 25s | Train: 1.8361 (57.5%) | Phys: 0.00 | Val: 1.2807 (67.09%) | Best: 68.04%\n",
            "Epoch 268 | T: 25s | Train: 1.9554 (54.4%) | Phys: 0.00 | Val: 1.3190 (66.30%) | Best: 68.04%\n",
            "Epoch 269 | T: 25s | Train: 1.9412 (54.4%) | Phys: 0.00 | Val: 1.2859 (66.83%) | Best: 68.04%\n",
            "Epoch 270 | T: 25s | Train: 1.9406 (54.4%) | Phys: 0.00 | Val: 1.2620 (67.57%) | Best: 68.04%\n",
            "Epoch 271 | T: 25s | Train: 1.9073 (55.3%) | Phys: 0.00 | Val: 1.2758 (66.98%) | Best: 68.04%\n",
            "Epoch 272 | T: 25s | Train: 1.8623 (56.7%) | Phys: 0.00 | Val: 1.2657 (67.67%) | Best: 68.04%\n",
            "Epoch 273 | T: 25s | Train: 1.9082 (55.4%) | Phys: 0.00 | Val: 1.2570 (67.48%) | Best: 68.04%\n",
            "Epoch 274 | T: 25s | Train: 1.8927 (55.9%) | Phys: 0.00 | Val: 1.2726 (67.54%) | Best: 68.04%\n",
            "Epoch 275 | T: 25s | Train: 1.8248 (58.1%) | Phys: 0.00 | Val: 1.2684 (67.46%) | Best: 68.04%\n",
            "Epoch 276 | T: 25s | Train: 1.9163 (55.1%) | Phys: 0.00 | Val: 1.2753 (67.53%) | Best: 68.04%\n",
            "Epoch 277 | T: 25s | Train: 1.8626 (56.9%) | Phys: 0.00 | Val: 1.2739 (67.47%) | Best: 68.04%\n",
            "Epoch 278 | T: 25s | Train: 1.8874 (55.8%) | Phys: 0.00 | Val: 1.2910 (67.31%) | Best: 68.04%\n",
            "Epoch 279 | T: 25s | Train: 1.8470 (57.3%) | Phys: 0.00 | Val: 1.2430 (67.43%) | Best: 68.04%\n",
            "Epoch 280 | T: 25s | Train: 1.9056 (55.1%) | Phys: 0.00 | Val: 1.2810 (67.37%) | Best: 68.04%\n",
            "Epoch 281 | T: 25s | Train: 1.9155 (55.2%) | Phys: 0.00 | Val: 1.2817 (67.64%) | Best: 68.04%\n",
            "Epoch 282 | T: 25s | Train: 1.8589 (57.1%) | Phys: 0.00 | Val: 1.2590 (67.68%) | Best: 68.04%\n",
            "Epoch 283 | T: 25s | Train: 1.9019 (55.4%) | Phys: 0.00 | Val: 1.2611 (67.78%) | Best: 68.04%\n",
            "Epoch 284 | T: 25s | Train: 1.9158 (55.1%) | Phys: 0.00 | Val: 1.2467 (68.24%) | Best: 68.24%\n",
            "Epoch 285 | T: 25s | Train: 1.8501 (57.1%) | Phys: 0.00 | Val: 1.2397 (67.90%) | Best: 68.24%\n",
            "Epoch 286 | T: 25s | Train: 1.9440 (54.4%) | Phys: 0.00 | Val: 1.2524 (68.20%) | Best: 68.24%\n",
            "Epoch 287 | T: 25s | Train: 1.7803 (59.0%) | Phys: 0.00 | Val: 1.2605 (67.81%) | Best: 68.24%\n",
            "Epoch 288 | T: 25s | Train: 1.8601 (56.8%) | Phys: 0.00 | Val: 1.2465 (67.96%) | Best: 68.24%\n",
            "Epoch 289 | T: 25s | Train: 1.8269 (57.4%) | Phys: 0.00 | Val: 1.2577 (68.40%) | Best: 68.40%\n",
            "Epoch 290 | T: 25s | Train: 1.8489 (56.9%) | Phys: 0.00 | Val: 1.2515 (67.82%) | Best: 68.40%\n",
            "Epoch 291 | T: 25s | Train: 1.8961 (55.7%) | Phys: 0.00 | Val: 1.2531 (68.11%) | Best: 68.40%\n",
            "Epoch 292 | T: 25s | Train: 1.7240 (60.2%) | Phys: 0.00 | Val: 1.2392 (68.20%) | Best: 68.40%\n",
            "Epoch 293 | T: 25s | Train: 1.8648 (56.3%) | Phys: 0.00 | Val: 1.2633 (68.11%) | Best: 68.40%\n",
            "Epoch 294 | T: 25s | Train: 1.8511 (57.3%) | Phys: 0.00 | Val: 1.2424 (68.58%) | Best: 68.58%\n",
            "Epoch 295 | T: 25s | Train: 1.7406 (60.1%) | Phys: 0.00 | Val: 1.2552 (68.11%) | Best: 68.58%\n",
            "Epoch 296 | T: 25s | Train: 1.8557 (57.0%) | Phys: 0.00 | Val: 1.2543 (67.96%) | Best: 68.58%\n",
            "Epoch 297 | T: 25s | Train: 1.8091 (58.3%) | Phys: 0.00 | Val: 1.2560 (68.05%) | Best: 68.58%\n",
            "Epoch 298 | T: 25s | Train: 1.7456 (60.0%) | Phys: 0.00 | Val: 1.2653 (68.07%) | Best: 68.58%\n",
            "Epoch 299 | T: 25s | Train: 1.8256 (57.6%) | Phys: 0.00 | Val: 1.2280 (68.70%) | Best: 68.70%\n",
            "Epoch 300 | T: 25s | Train: 1.8124 (58.0%) | Phys: 0.00 | Val: 1.2131 (69.01%) | Best: 69.01%\n",
            "Epoch 301 | T: 25s | Train: 1.8304 (57.2%) | Phys: 0.00 | Val: 1.2566 (68.35%) | Best: 69.01%\n",
            "Epoch 302 | T: 25s | Train: 1.9311 (54.7%) | Phys: 0.00 | Val: 1.2277 (68.20%) | Best: 69.01%\n",
            "Epoch 303 | T: 25s | Train: 1.8448 (57.2%) | Phys: 0.00 | Val: 1.2369 (68.73%) | Best: 69.01%\n",
            "Epoch 304 | T: 25s | Train: 1.8253 (57.8%) | Phys: 0.00 | Val: 1.2411 (68.74%) | Best: 69.01%\n",
            "Epoch 305 | T: 25s | Train: 1.7766 (58.8%) | Phys: 0.00 | Val: 1.2073 (69.06%) | Best: 69.06%\n",
            "Epoch 306 | T: 25s | Train: 1.8350 (58.0%) | Phys: 0.00 | Val: 1.2457 (68.12%) | Best: 69.06%\n",
            "Epoch 307 | T: 25s | Train: 1.7396 (60.1%) | Phys: 0.00 | Val: 1.2080 (69.09%) | Best: 69.09%\n",
            "Epoch 308 | T: 25s | Train: 1.7619 (59.5%) | Phys: 0.00 | Val: 1.2290 (68.86%) | Best: 69.09%\n",
            "Epoch 309 | T: 25s | Train: 1.7419 (60.0%) | Phys: 0.00 | Val: 1.2350 (69.29%) | Best: 69.29%\n",
            "Epoch 310 | T: 25s | Train: 1.8201 (57.8%) | Phys: 0.00 | Val: 1.2221 (69.31%) | Best: 69.31%\n",
            "Epoch 311 | T: 25s | Train: 1.7993 (58.2%) | Phys: 0.00 | Val: 1.2182 (69.28%) | Best: 69.31%\n",
            "Epoch 312 | T: 25s | Train: 1.7994 (58.2%) | Phys: 0.00 | Val: 1.2381 (68.93%) | Best: 69.31%\n",
            "Epoch 313 | T: 25s | Train: 1.7940 (58.6%) | Phys: 0.00 | Val: 1.2050 (69.68%) | Best: 69.68%\n",
            "Epoch 314 | T: 25s | Train: 1.7523 (59.8%) | Phys: 0.00 | Val: 1.2162 (69.21%) | Best: 69.68%\n",
            "Epoch 315 | T: 25s | Train: 1.7251 (60.8%) | Phys: 0.00 | Val: 1.1964 (69.89%) | Best: 69.89%\n",
            "Epoch 316 | T: 25s | Train: 1.8260 (57.9%) | Phys: 0.00 | Val: 1.2272 (69.20%) | Best: 69.89%\n",
            "Epoch 317 | T: 25s | Train: 1.7863 (58.7%) | Phys: 0.00 | Val: 1.2024 (69.50%) | Best: 69.89%\n",
            "Epoch 318 | T: 25s | Train: 1.7791 (58.7%) | Phys: 0.00 | Val: 1.2190 (69.13%) | Best: 69.89%\n",
            "Epoch 319 | T: 25s | Train: 1.7248 (60.6%) | Phys: 0.00 | Val: 1.2270 (68.98%) | Best: 69.89%\n",
            "Epoch 320 | T: 25s | Train: 1.7326 (60.4%) | Phys: 0.00 | Val: 1.2196 (69.36%) | Best: 69.89%\n",
            "Epoch 321 | T: 25s | Train: 1.7933 (58.5%) | Phys: 0.00 | Val: 1.2387 (69.01%) | Best: 69.89%\n",
            "Epoch 322 | T: 25s | Train: 1.6948 (61.4%) | Phys: 0.00 | Val: 1.2189 (69.45%) | Best: 69.89%\n",
            "Epoch 323 | T: 25s | Train: 1.8073 (58.2%) | Phys: 0.00 | Val: 1.2073 (69.91%) | Best: 69.91%\n",
            "Epoch 324 | T: 25s | Train: 1.7405 (59.7%) | Phys: 0.00 | Val: 1.2048 (69.40%) | Best: 69.91%\n",
            "Epoch 325 | T: 25s | Train: 1.7731 (58.9%) | Phys: 0.00 | Val: 1.2244 (69.32%) | Best: 69.91%\n",
            "Epoch 326 | T: 25s | Train: 1.7547 (59.4%) | Phys: 0.00 | Val: 1.2117 (69.47%) | Best: 69.91%\n",
            "Epoch 327 | T: 25s | Train: 1.7559 (59.5%) | Phys: 0.00 | Val: 1.2094 (69.36%) | Best: 69.91%\n",
            "Epoch 328 | T: 25s | Train: 1.6710 (62.0%) | Phys: 0.00 | Val: 1.2116 (69.18%) | Best: 69.91%\n",
            "Epoch 329 | T: 25s | Train: 1.7664 (59.4%) | Phys: 0.00 | Val: 1.2142 (69.61%) | Best: 69.91%\n",
            "Epoch 330 | T: 25s | Train: 1.6662 (62.1%) | Phys: 0.00 | Val: 1.2126 (69.79%) | Best: 69.91%\n",
            "Epoch 331 | T: 25s | Train: 1.7418 (60.1%) | Phys: 0.00 | Val: 1.2252 (69.53%) | Best: 69.91%\n",
            "Epoch 332 | T: 25s | Train: 1.7217 (60.7%) | Phys: 0.00 | Val: 1.2115 (69.54%) | Best: 69.91%\n",
            "Epoch 333 | T: 25s | Train: 1.7204 (60.6%) | Phys: 0.00 | Val: 1.2009 (69.96%) | Best: 69.96%\n",
            "Epoch 334 | T: 25s | Train: 1.7136 (61.0%) | Phys: 0.00 | Val: 1.2013 (69.92%) | Best: 69.96%\n",
            "Epoch 335 | T: 25s | Train: 1.7297 (60.7%) | Phys: 0.00 | Val: 1.1999 (70.24%) | Best: 70.24%\n",
            "Epoch 336 | T: 25s | Train: 1.6907 (61.3%) | Phys: 0.00 | Val: 1.2027 (69.81%) | Best: 70.24%\n",
            "Epoch 337 | T: 25s | Train: 1.7872 (58.9%) | Phys: 0.00 | Val: 1.1914 (69.98%) | Best: 70.24%\n",
            "Epoch 338 | T: 25s | Train: 1.7825 (58.6%) | Phys: 0.00 | Val: 1.1866 (69.88%) | Best: 70.24%\n",
            "Epoch 339 | T: 25s | Train: 1.7323 (59.9%) | Phys: 0.00 | Val: 1.2017 (69.89%) | Best: 70.24%\n",
            "Epoch 340 | T: 25s | Train: 1.6891 (61.5%) | Phys: 0.00 | Val: 1.1917 (69.75%) | Best: 70.24%\n",
            "Epoch 341 | T: 25s | Train: 1.7352 (60.1%) | Phys: 0.00 | Val: 1.2017 (70.17%) | Best: 70.24%\n",
            "Epoch 342 | T: 25s | Train: 1.7131 (60.7%) | Phys: 0.00 | Val: 1.2000 (70.26%) | Best: 70.26%\n",
            "Epoch 343 | T: 25s | Train: 1.7252 (60.5%) | Phys: 0.00 | Val: 1.1926 (70.13%) | Best: 70.26%\n",
            "Epoch 344 | T: 25s | Train: 1.7768 (58.8%) | Phys: 0.00 | Val: 1.1963 (70.21%) | Best: 70.26%\n",
            "Epoch 345 | T: 25s | Train: 1.6691 (62.2%) | Phys: 0.00 | Val: 1.1898 (70.18%) | Best: 70.26%\n",
            "Epoch 346 | T: 25s | Train: 1.7008 (61.1%) | Phys: 0.00 | Val: 1.1974 (70.10%) | Best: 70.26%\n",
            "Epoch 347 | T: 25s | Train: 1.7033 (61.1%) | Phys: 0.00 | Val: 1.1887 (70.34%) | Best: 70.34%\n",
            "Epoch 348 | T: 25s | Train: 1.7408 (60.0%) | Phys: 0.00 | Val: 1.1863 (70.39%) | Best: 70.39%\n",
            "Epoch 349 | T: 25s | Train: 1.6787 (61.5%) | Phys: 0.00 | Val: 1.1958 (70.26%) | Best: 70.39%\n",
            "Epoch 350 | T: 25s | Train: 1.6318 (63.0%) | Phys: 0.00 | Val: 1.1842 (70.38%) | Best: 70.39%\n",
            "Epoch 351 | T: 25s | Train: 1.7295 (60.0%) | Phys: 0.00 | Val: 1.1918 (70.34%) | Best: 70.39%\n",
            "Epoch 352 | T: 25s | Train: 1.7456 (59.9%) | Phys: 0.00 | Val: 1.2019 (69.92%) | Best: 70.39%\n",
            "Epoch 353 | T: 25s | Train: 1.6920 (61.4%) | Phys: 0.00 | Val: 1.1791 (70.66%) | Best: 70.66%\n",
            "Epoch 354 | T: 25s | Train: 1.6897 (61.5%) | Phys: 0.00 | Val: 1.1773 (70.76%) | Best: 70.76%\n",
            "Epoch 355 | T: 25s | Train: 1.7116 (60.6%) | Phys: 0.00 | Val: 1.1877 (70.32%) | Best: 70.76%\n",
            "Epoch 356 | T: 25s | Train: 1.6863 (61.5%) | Phys: 0.00 | Val: 1.1860 (70.32%) | Best: 70.76%\n",
            "Epoch 357 | T: 25s | Train: 1.7264 (60.6%) | Phys: 0.00 | Val: 1.1792 (70.67%) | Best: 70.76%\n",
            "Epoch 358 | T: 25s | Train: 1.6078 (63.6%) | Phys: 0.00 | Val: 1.1918 (70.57%) | Best: 70.76%\n",
            "Epoch 359 | T: 25s | Train: 1.7118 (60.6%) | Phys: 0.00 | Val: 1.1751 (70.75%) | Best: 70.76%\n",
            "Epoch 360 | T: 25s | Train: 1.7802 (58.6%) | Phys: 0.00 | Val: 1.1784 (70.65%) | Best: 70.76%\n",
            "Epoch 361 | T: 25s | Train: 1.7399 (59.9%) | Phys: 0.00 | Val: 1.1884 (70.33%) | Best: 70.76%\n",
            "Epoch 362 | T: 25s | Train: 1.6284 (62.8%) | Phys: 0.00 | Val: 1.1920 (70.17%) | Best: 70.76%\n",
            "Epoch 363 | T: 25s | Train: 1.6946 (61.6%) | Phys: 0.00 | Val: 1.1865 (70.27%) | Best: 70.76%\n",
            "Epoch 364 | T: 25s | Train: 1.6781 (61.6%) | Phys: 0.00 | Val: 1.1869 (70.40%) | Best: 70.76%\n",
            "Epoch 365 | T: 25s | Train: 1.7093 (61.3%) | Phys: 0.00 | Val: 1.1896 (70.31%) | Best: 70.76%\n",
            "Epoch 366 | T: 25s | Train: 1.6592 (62.1%) | Phys: 0.00 | Val: 1.1834 (70.41%) | Best: 70.76%\n",
            "Epoch 367 | T: 25s | Train: 1.5963 (64.2%) | Phys: 0.00 | Val: 1.1988 (70.51%) | Best: 70.76%\n",
            "Epoch 368 | T: 25s | Train: 1.6718 (61.8%) | Phys: 0.00 | Val: 1.1847 (70.09%) | Best: 70.76%\n",
            "Epoch 369 | T: 25s | Train: 1.7120 (60.6%) | Phys: 0.00 | Val: 1.1790 (70.40%) | Best: 70.76%\n",
            "Epoch 370 | T: 25s | Train: 1.6817 (61.5%) | Phys: 0.00 | Val: 1.1735 (70.47%) | Best: 70.76%\n",
            "Epoch 371 | T: 25s | Train: 1.6928 (61.1%) | Phys: 0.00 | Val: 1.1888 (69.99%) | Best: 70.76%\n",
            "Epoch 372 | T: 25s | Train: 1.6978 (61.1%) | Phys: 0.00 | Val: 1.1800 (70.18%) | Best: 70.76%\n",
            "Epoch 373 | T: 25s | Train: 1.7418 (59.4%) | Phys: 0.00 | Val: 1.1739 (70.55%) | Best: 70.76%\n",
            "Epoch 374 | T: 25s | Train: 1.5913 (63.9%) | Phys: 0.00 | Val: 1.1736 (70.55%) | Best: 70.76%\n",
            "Epoch 375 | T: 25s | Train: 1.7084 (60.3%) | Phys: 0.00 | Val: 1.1732 (70.53%) | Best: 70.76%\n",
            "Epoch 376 | T: 25s | Train: 1.6246 (63.1%) | Phys: 0.00 | Val: 1.1703 (70.53%) | Best: 70.76%\n",
            "Epoch 377 | T: 25s | Train: 1.7394 (59.9%) | Phys: 0.00 | Val: 1.1846 (70.34%) | Best: 70.76%\n",
            "Epoch 378 | T: 25s | Train: 1.5867 (64.5%) | Phys: 0.00 | Val: 1.1867 (70.29%) | Best: 70.76%\n",
            "Epoch 379 | T: 25s | Train: 1.6514 (62.7%) | Phys: 0.00 | Val: 1.1779 (70.50%) | Best: 70.76%\n",
            "Epoch 380 | T: 25s | Train: 1.6603 (62.2%) | Phys: 0.00 | Val: 1.1741 (70.52%) | Best: 70.76%\n",
            "Epoch 381 | T: 25s | Train: 1.6856 (61.4%) | Phys: 0.00 | Val: 1.1779 (70.66%) | Best: 70.76%\n",
            "Epoch 382 | T: 25s | Train: 1.7163 (60.5%) | Phys: 0.00 | Val: 1.1812 (70.60%) | Best: 70.76%\n",
            "Epoch 383 | T: 25s | Train: 1.6450 (62.7%) | Phys: 0.00 | Val: 1.1756 (70.67%) | Best: 70.76%\n",
            "Epoch 384 | T: 25s | Train: 1.6917 (61.1%) | Phys: 0.00 | Val: 1.1819 (70.53%) | Best: 70.76%\n",
            "Epoch 385 | T: 25s | Train: 1.6811 (61.5%) | Phys: 0.00 | Val: 1.1729 (70.55%) | Best: 70.76%\n",
            "Epoch 386 | T: 25s | Train: 1.6602 (61.8%) | Phys: 0.00 | Val: 1.1845 (70.47%) | Best: 70.76%\n",
            "Epoch 387 | T: 25s | Train: 1.6539 (62.5%) | Phys: 0.00 | Val: 1.1805 (70.49%) | Best: 70.76%\n",
            "Epoch 388 | T: 25s | Train: 1.6576 (62.2%) | Phys: 0.00 | Val: 1.1772 (70.46%) | Best: 70.76%\n",
            "Epoch 389 | T: 25s | Train: 1.6461 (62.7%) | Phys: 0.00 | Val: 1.1766 (70.68%) | Best: 70.76%\n",
            "Epoch 390 | T: 25s | Train: 1.6509 (62.5%) | Phys: 0.00 | Val: 1.1737 (70.47%) | Best: 70.76%\n",
            "Epoch 391 | T: 25s | Train: 1.6398 (63.0%) | Phys: 0.00 | Val: 1.1802 (70.70%) | Best: 70.76%\n",
            "Epoch 392 | T: 25s | Train: 1.6995 (61.3%) | Phys: 0.00 | Val: 1.1829 (70.74%) | Best: 70.76%\n",
            "Epoch 393 | T: 25s | Train: 1.6934 (61.0%) | Phys: 0.00 | Val: 1.1700 (70.57%) | Best: 70.76%\n",
            "Epoch 394 | T: 25s | Train: 1.6693 (62.0%) | Phys: 0.00 | Val: 1.1721 (70.54%) | Best: 70.76%\n",
            "Epoch 395 | T: 25s | Train: 1.6590 (62.2%) | Phys: 0.00 | Val: 1.1735 (70.49%) | Best: 70.76%\n",
            "Epoch 396 | T: 25s | Train: 1.6597 (61.9%) | Phys: 0.00 | Val: 1.1748 (70.76%) | Best: 70.76%\n",
            "Epoch 397 | T: 25s | Train: 1.6834 (61.8%) | Phys: 0.00 | Val: 1.1725 (70.60%) | Best: 70.76%\n",
            "Epoch 398 | T: 25s | Train: 1.6554 (62.4%) | Phys: 0.00 | Val: 1.1656 (70.71%) | Best: 70.76%\n",
            "Epoch 399 | T: 25s | Train: 1.6722 (61.8%) | Phys: 0.00 | Val: 1.1746 (70.68%) | Best: 70.76%\n",
            "Epoch 400 | T: 25s | Train: 1.7219 (60.2%) | Phys: 0.00 | Val: 1.1721 (70.58%) | Best: 70.76%\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": [],
      "name": "CIFAR-100 ViT",
      "gpuType": "L4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}