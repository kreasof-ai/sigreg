{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/KellerJordan/Muon"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DDIx2dVH4PHw",
        "outputId": "61d27ac9-ff68-4910-dfd6-7395725b759f"
      },
      "id": "DDIx2dVH4PHw",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/KellerJordan/Muon\n",
            "  Cloning https://github.com/KellerJordan/Muon to /tmp/pip-req-build-r_h12u95\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/KellerJordan/Muon /tmp/pip-req-build-r_h12u95\n",
            "  Resolved https://github.com/KellerJordan/Muon to commit 6399c658d3c4a3356ba823fa6664b10e23871068\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: muon-optimizer\n",
            "  Building wheel for muon-optimizer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for muon-optimizer: filename=muon_optimizer-0.1.0-py3-none-any.whl size=7141 sha256=24796dac90623b10bc0296c7f51a086b82633d8cb0868c4e120fe0828e75570f\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-3129q_m4/wheels/6e/33/94/64d18603ba0f39064aab523d6edf493c388cfb7419bb5c9043\n",
            "Successfully built muon-optimizer\n",
            "Installing collected packages: muon-optimizer\n",
            "Successfully installed muon-optimizer-0.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "id": "pBxX3I1kYwojaamntSxiNZTK",
      "metadata": {
        "tags": [],
        "id": "pBxX3I1kYwojaamntSxiNZTK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35e3af13-ed42-4372-9d4f-23e71f2b3233"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "import math\n",
        "import time\n",
        "\n",
        "from muon import SingleDeviceMuonWithAuxAdam\n",
        "\n",
        "# ==========================================\n",
        "# 1. Configuration (Tuned for ViT on CIFAR)\n",
        "# ==========================================\n",
        "REG_MODE = 'strong'\n",
        "SIGR_ALPHA = 0.01   # Strength of the physics constraint\n",
        "SKETCH_DIM = 64    # Dimension of the random observer\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "LEARNING_RATE = 1e-2  # Slightly higher initial LR for AdamW with cosine schedule\n",
        "EPOCHS = 400\n",
        "WEIGHT_DECAY = 0.05\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "if torch.backends.mps.is_available(): DEVICE = 'mps'\n",
        "\n",
        "# Regularization Config\n",
        "MIXUP_ALPHA = 0.8\n",
        "CUTMIX_ALPHA = 1.0\n",
        "DROP_PATH_RATE = 0.1\n",
        "\n",
        "print(f\"Training on device: {DEVICE}\")\n",
        "\n",
        "# ==========================================\n",
        "# 2. Data Preparation (THE FIX: Strong Augmentation)\n",
        "# ==========================================\n",
        "def get_data_loaders():\n",
        "    print('==> Preparing data with Strong Augmentation...')\n",
        "\n",
        "    mean = (0.5071, 0.4867, 0.4408)\n",
        "    std = (0.2675, 0.2565, 0.2761)\n",
        "\n",
        "    # FIX 1: Add RandAugment\n",
        "    transform_train = transforms.Compose([\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandAugment(num_ops=2, magnitude=9), # <--- CRITICAL FOR ViT\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean, std),\n",
        "    ])\n",
        "\n",
        "    transform_test = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean, std),\n",
        "    ])\n",
        "\n",
        "    trainset = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=transform_train)\n",
        "    # Increase workers to handle augmentation load\n",
        "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
        "\n",
        "    testset = torchvision.datasets.CIFAR100(root='./data', train=False, download=True, transform=transform_test)\n",
        "    testloader = torch.utils.data.DataLoader(testset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
        "\n",
        "    return trainloader, testloader\n",
        "\n",
        "\n",
        "# ------------------------------------------\n",
        "# Physics Engine: The Regularizers\n",
        "# ------------------------------------------\n",
        "\n",
        "def sigreg_weak_loss(x, sketch_dim=64):\n",
        "    \"\"\"\n",
        "    Forces Covariance(x) ~ Identity.\n",
        "    Matches the 2nd Moment (Spherical Cloud).\n",
        "    \"\"\"\n",
        "    N, C = x.size()\n",
        "    # 1. Sketching (Optional for C=512, but good for consistency)\n",
        "    if C > sketch_dim:\n",
        "        S = torch.randn(sketch_dim, C, device=x.device) / (C ** 0.5)\n",
        "        x = x @ S.T  # [N, sketch_dim]\n",
        "    else:\n",
        "        sketch_dim = C\n",
        "\n",
        "    # 2. Centering & Covariance\n",
        "    x = x - x.mean(dim=0, keepdim=True)\n",
        "    cov = (x.T @ x) / (N - 1 + 1e-6)\n",
        "\n",
        "    # 3. Target Identity\n",
        "    target = torch.eye(sketch_dim, device=x.device)\n",
        "\n",
        "    # 4. Off-diagonal suppression + Diagonal maintenance\n",
        "    return torch.norm(cov - target, p='fro')\n",
        "\n",
        "def sigreg_strong_loss(x, sketch_dim=64):\n",
        "    \"\"\"\n",
        "    Forces ECF(x) ~ ECF(Gaussian).\n",
        "    Matches ALL Moments (Maximum Entropy Cloud).\n",
        "    Exact implementation of LeJEPA Algorithm 1.\n",
        "    \"\"\"\n",
        "    N, C = x.size()\n",
        "\n",
        "    # 1. Projection (The Observer)\n",
        "    # Project channels down to sketch_dim\n",
        "    A = torch.randn(C, sketch_dim, device=x.device)\n",
        "    A = A / (A.norm(p=2, dim=0, keepdim=True) + 1e-6)\n",
        "\n",
        "    # 2. Integration Points\n",
        "    t = torch.linspace(-5, 5, 17, device=x.device)\n",
        "\n",
        "    # 3. Theoretical Gaussian CF\n",
        "    exp_f = torch.exp(-0.5 * t**2)\n",
        "\n",
        "    # 4. Empirical CF\n",
        "    # proj: [N, sketch_dim]\n",
        "    proj = x @ A\n",
        "\n",
        "    # args: [N, sketch_dim, T]\n",
        "    args = proj.unsqueeze(2) * t.view(1, 1, -1)\n",
        "\n",
        "    # ecf: [sketch_dim, T] (Mean over batch)\n",
        "    ecf = torch.exp(1j * args).mean(dim=0)\n",
        "\n",
        "    # 5. Weighted L2 Distance\n",
        "    # |ecf - gauss|^2 * gauss_weight\n",
        "    diff_sq = (ecf - exp_f.unsqueeze(0)).abs().square()\n",
        "    err = diff_sq * exp_f.unsqueeze(0)\n",
        "\n",
        "    # 6. Integrate\n",
        "    loss = torch.trapz(err, t, dim=1) * N\n",
        "\n",
        "    return loss.mean()\n",
        "\n",
        "# ==========================================\n",
        "# 3. Mixup / CutMix Utilities\n",
        "# ==========================================\n",
        "def rand_bbox(size, lam):\n",
        "    W = size[2]\n",
        "    H = size[3]\n",
        "    cut_rat = np.sqrt(1. - lam)\n",
        "    cut_w = int(W * cut_rat)\n",
        "    cut_h = int(H * cut_rat)\n",
        "\n",
        "    cx = np.random.randint(W)\n",
        "    cy = np.random.randint(H)\n",
        "\n",
        "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
        "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
        "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
        "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
        "\n",
        "    return bbx1, bby1, bbx2, bby2\n",
        "\n",
        "def drop_path(x, drop_prob: float = 0., training: bool = False):\n",
        "    if drop_prob == 0. or not training: return x\n",
        "    keep_prob = 1 - drop_prob\n",
        "    shape = (x.shape[0],) + (1,) * (x.ndim - 1)\n",
        "    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
        "    random_tensor.floor_()\n",
        "    return x.div(keep_prob) * random_tensor\n",
        "\n",
        "class DropPath(nn.Module):\n",
        "    def __init__(self, drop_prob=None):\n",
        "        super(DropPath, self).__init__()\n",
        "        self.drop_prob = drop_prob\n",
        "    def forward(self, x): return drop_path(x, self.drop_prob, self.training)\n",
        "\n",
        "class PatchEmbedding(nn.Module):\n",
        "    def __init__(self, img_size=32, patch_size=4, in_chans=3, embed_dim=192):\n",
        "        super().__init__()\n",
        "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "    def forward(self, x):\n",
        "        x = self.proj(x).flatten(2).transpose(1, 2)\n",
        "        return x\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, dim, num_heads=8, qkv_bias=False, attn_drop=0., proj_drop=0.):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        head_dim = dim // num_heads\n",
        "        self.scale = head_dim ** -0.5\n",
        "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
        "        self.attn_drop = nn.Dropout(attn_drop)\n",
        "        self.proj = nn.Linear(dim, dim)\n",
        "        self.proj_drop = nn.Dropout(proj_drop)\n",
        "    def forward(self, x):\n",
        "        B, N, C = x.shape\n",
        "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        attn = self.attn_drop(attn)\n",
        "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
        "        x = self.proj(x)\n",
        "        x = self.proj_drop(x)\n",
        "        return x\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, dim, hidden_dim, drop=0.):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(dim, hidden_dim)\n",
        "        self.act = nn.GELU()\n",
        "        self.fc2 = nn.Linear(hidden_dim, dim)\n",
        "        self.drop = nn.Dropout(drop)\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.drop(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.drop(x)\n",
        "        return x\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, dim, num_heads, mlp_ratio=4., drop=0., attn_drop=0., drop_path=0., reg_mode='strong', sketch_dim=64):\n",
        "        super().__init__()\n",
        "        self.norm1 = nn.LayerNorm(dim)\n",
        "        self.attn = MultiHeadAttention(dim, num_heads=num_heads, qkv_bias=True, attn_drop=attn_drop, proj_drop=drop)\n",
        "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
        "        self.norm2 = nn.LayerNorm(dim)\n",
        "        self.mlp = MLP(dim, int(dim * mlp_ratio), drop=drop)\n",
        "\n",
        "        self.reg_mode = reg_mode\n",
        "        self.sketch_dim = sketch_dim\n",
        "    def forward(self, x):\n",
        "        x = x + self.drop_path(self.attn(self.norm1(x)))\n",
        "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
        "\n",
        "\n",
        "        # --- PHYSICS INJECTION ---\n",
        "        reg_loss = torch.tensor(0.0, device=x.device)\n",
        "        if self.reg_mode != 'baseline':\n",
        "            # Global Average Pool of the tokens [B, N, C] -> [B, C]\n",
        "            # This represents the \"Image Vector\" at this depth\n",
        "            flat_rep = x.mean(dim=1)\n",
        "\n",
        "            # Crucial: Pre-Norm vs Post-Norm context.\n",
        "            # LayerNorm forces variance=1. SIGReg forces Distribution=Gaussian.\n",
        "            # They are compatible.\n",
        "            if self.reg_mode == 'weak':\n",
        "                reg_loss = sigreg_weak_loss(flat_rep, self.sketch_dim)\n",
        "            elif self.reg_mode == 'strong':\n",
        "                reg_loss = sigreg_strong_loss(flat_rep, self.sketch_dim)\n",
        "\n",
        "        return x, reg_loss\n",
        "\n",
        "class VisionTransformer(nn.Module):\n",
        "    def __init__(self, img_size=32, patch_size=4, in_chans=3, num_classes=100, embed_dim=192, depth=9, num_heads=3, mlp_ratio=4., drop_rate=0., attn_drop_rate=0., drop_path_rate=0.1, reg_mode='strong', sketch_dim=64):\n",
        "        super().__init__()\n",
        "        self.patch_embed = PatchEmbedding(img_size, patch_size, in_chans, embed_dim)\n",
        "        num_patches = self.patch_embed.proj.weight.shape[2] # Just a hack to get patch count logic\n",
        "        num_patches = (img_size // patch_size) ** 2\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n",
        "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]\n",
        "        self.blocks = nn.ModuleList([\n",
        "            TransformerBlock(embed_dim, num_heads, mlp_ratio, drop_rate, attn_drop_rate, dpr[i], reg_mode, sketch_dim)\n",
        "            for i in range(depth)])\n",
        "        self.norm = nn.LayerNorm(embed_dim)\n",
        "        self.head = nn.Linear(embed_dim, num_classes)\n",
        "        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n",
        "        nn.init.trunc_normal_(self.cls_token, std=0.02)\n",
        "        nn.init.xavier_uniform_(self.head.weight)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B = x.shape[0]\n",
        "        x = self.patch_embed(x)\n",
        "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
        "        x = torch.cat((cls_tokens, x), dim=1)\n",
        "        x = x + self.pos_embed\n",
        "\n",
        "        total_phys_loss = 0.0\n",
        "\n",
        "        # Pass through blocks\n",
        "        for blk in self.blocks:\n",
        "            x, l_loss = blk(x, )\n",
        "            total_phys_loss += l_loss\n",
        "\n",
        "        x = self.norm(x)\n",
        "        out = self.head(x[:, 0])\n",
        "        return out, (total_phys_loss / len(self.blocks))\n",
        "\n",
        "# ==========================================\n",
        "# 5. Training Engine (Updated for Mixup/CutMix)\n",
        "# ==========================================\n",
        "def train(epoch, net, trainloader, optimizer, criterion):\n",
        "    net.train()\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    phys_loss_meter = 0\n",
        "\n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
        "\n",
        "        # Apply Mixup/CutMix\n",
        "        r = np.random.rand(1)\n",
        "        if r < 0.5: # Mixup\n",
        "            lam = np.random.beta(MIXUP_ALPHA, MIXUP_ALPHA)\n",
        "            index = torch.randperm(inputs.size(0)).to(DEVICE)\n",
        "            inputs = lam * inputs + (1 - lam) * inputs[index, :]\n",
        "            targets_a, targets_b = targets, targets[index]\n",
        "        else: # CutMix\n",
        "            lam = np.random.beta(CUTMIX_ALPHA, CUTMIX_ALPHA)\n",
        "            rand_index = torch.randperm(inputs.size(0)).to(DEVICE)\n",
        "            target_a = targets\n",
        "            target_b = targets[rand_index]\n",
        "            bbx1, bby1, bbx2, bby2 = rand_bbox(inputs.size(), lam)\n",
        "            inputs[:, :, bbx1:bbx2, bby1:bby2] = inputs[rand_index, :, bbx1:bbx2, bby1:bby2]\n",
        "            lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (inputs.size()[-1] * inputs.size()[-2]))\n",
        "            targets_a, targets_b = target_a, target_b\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward\n",
        "        outputs, p_loss = net(inputs)\n",
        "\n",
        "        # Task Loss\n",
        "        c_loss = criterion(outputs, targets_a) * lam + criterion(outputs, targets_b) * (1. - lam)\n",
        "\n",
        "        # Total Loss\n",
        "        loss = (1 - SIGR_ALPHA) * c_loss + (SIGR_ALPHA * p_loss)\n",
        "\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(net.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += ((1 - SIGR_ALPHA) * c_loss).item() # Log only task loss for comparison\n",
        "        phys_loss_meter += (SIGR_ALPHA * p_loss).item()\n",
        "\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += (lam * predicted.eq(targets_a).float() + (1 - lam) * predicted.eq(targets_b).float()).sum().item()\n",
        "\n",
        "    acc = 100. * correct / total\n",
        "    return train_loss / (batch_idx + 1), acc, phys_loss_meter / (batch_idx + 1)\n",
        "\n",
        "def test(epoch, net, testloader, criterion):\n",
        "    net.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "            inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
        "            outputs, _ = net(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "    acc = 100. * correct / total\n",
        "    return test_loss / (batch_idx + 1), acc\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    trainloader, testloader = get_data_loaders()\n",
        "\n",
        "    net = VisionTransformer(img_size=32, patch_size=4, embed_dim=192, depth=9, num_heads=3, drop_path_rate=0.1, reg_mode=REG_MODE, sketch_dim=SKETCH_DIM)\n",
        "    net = net.to(DEVICE)\n",
        "\n",
        "    hidden_weights = [p for p in net.parameters() if p.ndim == 2]\n",
        "    non_hidden_params = [p for p in net.parameters() if p.ndim != 2]\n",
        "\n",
        "    param_groups = [\n",
        "      dict(params=hidden_weights, use_muon=True, lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY),\n",
        "      dict(params=non_hidden_params, use_muon=False, lr=1e-3, betas=(0.9, 0.95), weight_decay=WEIGHT_DECAY)\n",
        "    ]\n",
        "\n",
        "    # Standard CrossEntropy for final eval, SoftLabel for training is handled by Mixup logic\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    optimizer = SingleDeviceMuonWithAuxAdam(param_groups)\n",
        "\n",
        "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS, eta_min=1e-5)\n",
        "\n",
        "    print(f\"Starting training for {EPOCHS} epochs with RandAugment + Mixup/CutMix...\")\n",
        "\n",
        "    best_acc = 0\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        start_time = time.time()\n",
        "\n",
        "        train_loss, train_acc, physics_loss = train(epoch, net, trainloader, optimizer, criterion)\n",
        "        test_loss, test_acc = test(epoch, net, testloader, criterion)\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        if test_acc > best_acc:\n",
        "            best_acc = test_acc\n",
        "            # torch.save(net.state_dict(), f'thermo_resnet_{REG_MODE}.pth')\n",
        "\n",
        "        epoch_time = time.time() - start_time\n",
        "\n",
        "        print(f\"Epoch {epoch+1} | T: {epoch_time:.0f}s | \"\n",
        "              f\"Train: {train_loss:.4f} ({train_acc:.1f}%) | \"\n",
        "              f\"Phys: {physics_loss:.2f} | \"\n",
        "              f\"Val: {test_loss:.4f} ({test_acc:.2f}%) | \"\n",
        "              f\"Best: {best_acc:.2f}%\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on device: cuda\n",
            "==> Preparing data with Strong Augmentation...\n",
            "Starting training for 400 epochs with RandAugment + Mixup/CutMix...\n",
            "Epoch 1 | T: 42s | Train: 4.2440 (6.1%) | Phys: 0.09 | Val: 3.5622 (14.47%) | Best: 14.47%\n",
            "Epoch 2 | T: 42s | Train: 4.0014 (9.7%) | Phys: 0.05 | Val: 3.3119 (20.44%) | Best: 20.44%\n",
            "Epoch 3 | T: 41s | Train: 3.9219 (11.3%) | Phys: 0.05 | Val: 3.2528 (21.78%) | Best: 21.78%\n",
            "Epoch 4 | T: 42s | Train: 3.8472 (12.6%) | Phys: 0.05 | Val: 3.0269 (25.84%) | Best: 25.84%\n",
            "Epoch 5 | T: 41s | Train: 3.7913 (13.6%) | Phys: 0.05 | Val: 2.9801 (27.60%) | Best: 27.60%\n",
            "Epoch 6 | T: 41s | Train: 3.7790 (14.0%) | Phys: 0.05 | Val: 2.8950 (29.24%) | Best: 29.24%\n",
            "Epoch 7 | T: 41s | Train: 3.6932 (15.7%) | Phys: 0.05 | Val: 2.6469 (33.82%) | Best: 33.82%\n",
            "Epoch 8 | T: 41s | Train: 3.5743 (18.2%) | Phys: 0.05 | Val: 2.5389 (34.94%) | Best: 34.94%\n",
            "Epoch 9 | T: 42s | Train: 3.5419 (19.0%) | Phys: 0.05 | Val: 2.4757 (37.23%) | Best: 37.23%\n",
            "Epoch 10 | T: 42s | Train: 3.5398 (18.9%) | Phys: 0.05 | Val: 2.4986 (37.08%) | Best: 37.23%\n",
            "Epoch 11 | T: 42s | Train: 3.4726 (20.3%) | Phys: 0.05 | Val: 2.4066 (39.53%) | Best: 39.53%\n",
            "Epoch 12 | T: 42s | Train: 3.3819 (22.1%) | Phys: 0.05 | Val: 2.3367 (39.64%) | Best: 39.64%\n",
            "Epoch 13 | T: 41s | Train: 3.3747 (22.5%) | Phys: 0.05 | Val: 2.3370 (40.98%) | Best: 40.98%\n",
            "Epoch 14 | T: 41s | Train: 3.3463 (23.3%) | Phys: 0.05 | Val: 2.1827 (44.32%) | Best: 44.32%\n",
            "Epoch 15 | T: 41s | Train: 3.3304 (23.5%) | Phys: 0.05 | Val: 2.1922 (43.95%) | Best: 44.32%\n",
            "Epoch 16 | T: 41s | Train: 3.3049 (24.0%) | Phys: 0.05 | Val: 2.1635 (45.53%) | Best: 45.53%\n",
            "Epoch 17 | T: 42s | Train: 3.2512 (25.4%) | Phys: 0.05 | Val: 2.1349 (45.58%) | Best: 45.58%\n",
            "Epoch 18 | T: 41s | Train: 3.2616 (25.2%) | Phys: 0.05 | Val: 2.0774 (46.28%) | Best: 46.28%\n",
            "Epoch 19 | T: 41s | Train: 3.2585 (25.0%) | Phys: 0.05 | Val: 2.0252 (47.95%) | Best: 47.95%\n",
            "Epoch 20 | T: 41s | Train: 3.2070 (26.3%) | Phys: 0.05 | Val: 2.0726 (47.41%) | Best: 47.95%\n",
            "Epoch 21 | T: 41s | Train: 3.1949 (26.8%) | Phys: 0.05 | Val: 2.0163 (48.13%) | Best: 48.13%\n",
            "Epoch 22 | T: 41s | Train: 3.1619 (27.2%) | Phys: 0.06 | Val: 2.0229 (47.95%) | Best: 48.13%\n",
            "Epoch 23 | T: 42s | Train: 3.1936 (26.5%) | Phys: 0.06 | Val: 1.9081 (50.49%) | Best: 50.49%\n",
            "Epoch 24 | T: 41s | Train: 3.1312 (28.0%) | Phys: 0.06 | Val: 1.9171 (50.06%) | Best: 50.49%\n",
            "Epoch 25 | T: 41s | Train: 3.1436 (27.8%) | Phys: 0.06 | Val: 1.9608 (49.71%) | Best: 50.49%\n",
            "Epoch 26 | T: 41s | Train: 3.1421 (27.9%) | Phys: 0.06 | Val: 1.9975 (50.19%) | Best: 50.49%\n",
            "Epoch 27 | T: 42s | Train: 3.1645 (27.5%) | Phys: 0.06 | Val: 1.9477 (49.60%) | Best: 50.49%\n",
            "Epoch 28 | T: 42s | Train: 3.1748 (27.3%) | Phys: 0.06 | Val: 1.9336 (50.24%) | Best: 50.49%\n",
            "Epoch 29 | T: 42s | Train: 3.1360 (27.9%) | Phys: 0.06 | Val: 1.8613 (51.74%) | Best: 51.74%\n",
            "Epoch 30 | T: 41s | Train: 3.1252 (28.5%) | Phys: 0.06 | Val: 1.9192 (51.12%) | Best: 51.74%\n",
            "Epoch 31 | T: 40s | Train: 3.1085 (28.5%) | Phys: 0.06 | Val: 1.8243 (52.64%) | Best: 52.64%\n",
            "Epoch 32 | T: 42s | Train: 3.1304 (28.2%) | Phys: 0.06 | Val: 1.9185 (50.83%) | Best: 52.64%\n",
            "Epoch 33 | T: 42s | Train: 3.0664 (29.6%) | Phys: 0.06 | Val: 1.8656 (51.97%) | Best: 52.64%\n",
            "Epoch 34 | T: 42s | Train: 3.1150 (28.7%) | Phys: 0.06 | Val: 1.9027 (50.77%) | Best: 52.64%\n",
            "Epoch 35 | T: 41s | Train: 3.0619 (29.7%) | Phys: 0.06 | Val: 1.8480 (53.27%) | Best: 53.27%\n",
            "Epoch 36 | T: 41s | Train: 3.0989 (28.9%) | Phys: 0.06 | Val: 1.8899 (51.32%) | Best: 53.27%\n",
            "Epoch 37 | T: 41s | Train: 3.1195 (28.5%) | Phys: 0.06 | Val: 1.8966 (50.65%) | Best: 53.27%\n",
            "Epoch 38 | T: 41s | Train: 3.1012 (28.8%) | Phys: 0.06 | Val: 1.8348 (52.38%) | Best: 53.27%\n",
            "Epoch 39 | T: 42s | Train: 3.1158 (28.8%) | Phys: 0.06 | Val: 1.8080 (52.88%) | Best: 53.27%\n",
            "Epoch 40 | T: 41s | Train: 3.1037 (28.8%) | Phys: 0.06 | Val: 1.7887 (53.55%) | Best: 53.55%\n",
            "Epoch 41 | T: 41s | Train: 3.0924 (29.3%) | Phys: 0.06 | Val: 1.8192 (52.53%) | Best: 53.55%\n",
            "Epoch 42 | T: 41s | Train: 3.0423 (30.2%) | Phys: 0.06 | Val: 1.8102 (53.08%) | Best: 53.55%\n",
            "Epoch 43 | T: 41s | Train: 3.0251 (30.6%) | Phys: 0.06 | Val: 1.8594 (52.78%) | Best: 53.55%\n",
            "Epoch 44 | T: 41s | Train: 3.0919 (29.4%) | Phys: 0.06 | Val: 1.8335 (52.52%) | Best: 53.55%\n",
            "Epoch 45 | T: 41s | Train: 3.0182 (30.7%) | Phys: 0.06 | Val: 1.7422 (54.11%) | Best: 54.11%\n",
            "Epoch 46 | T: 41s | Train: 3.0683 (29.8%) | Phys: 0.06 | Val: 1.8248 (53.20%) | Best: 54.11%\n",
            "Epoch 47 | T: 41s | Train: 3.0572 (30.0%) | Phys: 0.06 | Val: 1.7750 (54.11%) | Best: 54.11%\n",
            "Epoch 48 | T: 41s | Train: 3.0676 (29.8%) | Phys: 0.06 | Val: 1.8646 (52.82%) | Best: 54.11%\n",
            "Epoch 49 | T: 42s | Train: 3.0574 (30.0%) | Phys: 0.06 | Val: 1.8235 (53.37%) | Best: 54.11%\n",
            "Epoch 50 | T: 41s | Train: 3.0723 (29.8%) | Phys: 0.06 | Val: 1.8257 (52.81%) | Best: 54.11%\n",
            "Epoch 51 | T: 41s | Train: 3.0125 (31.2%) | Phys: 0.06 | Val: 1.8340 (53.30%) | Best: 54.11%\n",
            "Epoch 52 | T: 41s | Train: 3.0437 (30.1%) | Phys: 0.06 | Val: 1.7562 (54.68%) | Best: 54.68%\n",
            "Epoch 53 | T: 42s | Train: 2.9883 (31.5%) | Phys: 0.06 | Val: 1.8033 (53.50%) | Best: 54.68%\n",
            "Epoch 54 | T: 41s | Train: 3.0896 (29.3%) | Phys: 0.06 | Val: 1.8505 (52.18%) | Best: 54.68%\n",
            "Epoch 55 | T: 41s | Train: 3.0090 (31.2%) | Phys: 0.06 | Val: 1.7615 (54.61%) | Best: 54.68%\n",
            "Epoch 56 | T: 41s | Train: 3.0628 (29.8%) | Phys: 0.06 | Val: 1.8372 (53.33%) | Best: 54.68%\n",
            "Epoch 57 | T: 42s | Train: 3.0788 (29.5%) | Phys: 0.06 | Val: 1.7901 (54.05%) | Best: 54.68%\n",
            "Epoch 58 | T: 41s | Train: 3.0197 (30.7%) | Phys: 0.06 | Val: 1.7589 (54.22%) | Best: 54.68%\n",
            "Epoch 59 | T: 41s | Train: 3.0291 (30.7%) | Phys: 0.06 | Val: 1.7772 (54.61%) | Best: 54.68%\n",
            "Epoch 60 | T: 41s | Train: 3.0111 (31.0%) | Phys: 0.06 | Val: 1.7864 (53.89%) | Best: 54.68%\n",
            "Epoch 61 | T: 42s | Train: 3.0517 (30.4%) | Phys: 0.06 | Val: 1.7800 (54.59%) | Best: 54.68%\n",
            "Epoch 62 | T: 41s | Train: 3.0395 (30.3%) | Phys: 0.06 | Val: 1.7901 (54.23%) | Best: 54.68%\n",
            "Epoch 63 | T: 42s | Train: 3.0660 (29.9%) | Phys: 0.06 | Val: 1.7860 (53.72%) | Best: 54.68%\n",
            "Epoch 64 | T: 41s | Train: 2.9684 (32.0%) | Phys: 0.06 | Val: 1.7858 (54.01%) | Best: 54.68%\n",
            "Epoch 65 | T: 41s | Train: 2.9939 (31.6%) | Phys: 0.06 | Val: 1.7050 (55.44%) | Best: 55.44%\n",
            "Epoch 66 | T: 42s | Train: 3.0502 (30.4%) | Phys: 0.06 | Val: 1.7492 (54.67%) | Best: 55.44%\n",
            "Epoch 67 | T: 41s | Train: 2.9935 (31.5%) | Phys: 0.06 | Val: 1.7326 (55.47%) | Best: 55.47%\n",
            "Epoch 68 | T: 42s | Train: 3.0618 (30.0%) | Phys: 0.06 | Val: 1.7697 (54.16%) | Best: 55.47%\n",
            "Epoch 69 | T: 42s | Train: 3.0612 (30.1%) | Phys: 0.06 | Val: 1.7479 (54.78%) | Best: 55.47%\n",
            "Epoch 70 | T: 41s | Train: 3.0311 (30.8%) | Phys: 0.06 | Val: 1.7237 (55.81%) | Best: 55.81%\n",
            "Epoch 71 | T: 41s | Train: 3.0152 (30.8%) | Phys: 0.06 | Val: 1.7315 (55.34%) | Best: 55.81%\n",
            "Epoch 72 | T: 42s | Train: 3.0441 (30.1%) | Phys: 0.06 | Val: 1.7785 (54.56%) | Best: 55.81%\n",
            "Epoch 73 | T: 42s | Train: 3.0465 (30.6%) | Phys: 0.06 | Val: 1.7664 (54.06%) | Best: 55.81%\n",
            "Epoch 74 | T: 40s | Train: 3.0077 (30.9%) | Phys: 0.06 | Val: 1.7493 (54.32%) | Best: 55.81%\n",
            "Epoch 75 | T: 42s | Train: 3.0354 (30.5%) | Phys: 0.06 | Val: 1.7552 (54.61%) | Best: 55.81%\n",
            "Epoch 76 | T: 41s | Train: 2.9653 (32.0%) | Phys: 0.06 | Val: 1.7123 (55.45%) | Best: 55.81%\n",
            "Epoch 77 | T: 42s | Train: 2.9847 (31.8%) | Phys: 0.06 | Val: 1.7285 (54.07%) | Best: 55.81%\n",
            "Epoch 78 | T: 42s | Train: 2.9942 (31.5%) | Phys: 0.06 | Val: 1.7429 (55.57%) | Best: 55.81%\n",
            "Epoch 79 | T: 42s | Train: 3.0241 (30.8%) | Phys: 0.06 | Val: 1.7743 (54.48%) | Best: 55.81%\n",
            "Epoch 80 | T: 41s | Train: 3.0089 (31.3%) | Phys: 0.06 | Val: 1.7432 (55.40%) | Best: 55.81%\n",
            "Epoch 81 | T: 42s | Train: 3.0093 (31.4%) | Phys: 0.06 | Val: 1.7229 (55.51%) | Best: 55.81%\n",
            "Epoch 82 | T: 42s | Train: 3.0094 (31.2%) | Phys: 0.06 | Val: 1.7205 (55.11%) | Best: 55.81%\n",
            "Epoch 83 | T: 42s | Train: 3.0178 (30.9%) | Phys: 0.06 | Val: 1.7368 (55.08%) | Best: 55.81%\n",
            "Epoch 84 | T: 41s | Train: 3.0777 (29.5%) | Phys: 0.06 | Val: 1.7792 (54.79%) | Best: 55.81%\n",
            "Epoch 85 | T: 42s | Train: 2.9849 (31.5%) | Phys: 0.06 | Val: 1.6519 (57.13%) | Best: 57.13%\n",
            "Epoch 86 | T: 41s | Train: 3.0062 (31.1%) | Phys: 0.06 | Val: 1.8165 (54.16%) | Best: 57.13%\n",
            "Epoch 87 | T: 42s | Train: 2.9498 (32.4%) | Phys: 0.06 | Val: 1.6937 (56.33%) | Best: 57.13%\n",
            "Epoch 88 | T: 42s | Train: 2.9944 (31.8%) | Phys: 0.06 | Val: 1.7027 (55.73%) | Best: 57.13%\n",
            "Epoch 89 | T: 41s | Train: 2.9809 (31.9%) | Phys: 0.06 | Val: 1.7169 (55.79%) | Best: 57.13%\n",
            "Epoch 90 | T: 41s | Train: 2.9678 (32.1%) | Phys: 0.06 | Val: 1.6697 (56.63%) | Best: 57.13%\n",
            "Epoch 91 | T: 41s | Train: 2.9471 (32.7%) | Phys: 0.06 | Val: 1.7744 (54.77%) | Best: 57.13%\n",
            "Epoch 92 | T: 42s | Train: 2.9975 (31.6%) | Phys: 0.06 | Val: 1.6684 (56.36%) | Best: 57.13%\n",
            "Epoch 93 | T: 41s | Train: 3.0047 (31.4%) | Phys: 0.06 | Val: 1.7383 (55.42%) | Best: 57.13%\n",
            "Epoch 94 | T: 41s | Train: 3.0002 (31.6%) | Phys: 0.06 | Val: 1.7586 (55.61%) | Best: 57.13%\n",
            "Epoch 95 | T: 41s | Train: 2.9708 (32.1%) | Phys: 0.06 | Val: 1.6860 (55.80%) | Best: 57.13%\n",
            "Epoch 96 | T: 42s | Train: 2.9687 (31.9%) | Phys: 0.06 | Val: 1.6410 (56.73%) | Best: 57.13%\n",
            "Epoch 97 | T: 42s | Train: 3.0460 (30.5%) | Phys: 0.06 | Val: 1.6741 (56.28%) | Best: 57.13%\n",
            "Epoch 98 | T: 41s | Train: 3.0072 (31.2%) | Phys: 0.06 | Val: 1.7135 (56.06%) | Best: 57.13%\n",
            "Epoch 99 | T: 41s | Train: 2.9702 (32.1%) | Phys: 0.06 | Val: 1.7176 (55.27%) | Best: 57.13%\n",
            "Epoch 100 | T: 41s | Train: 2.9778 (32.0%) | Phys: 0.06 | Val: 1.7196 (55.30%) | Best: 57.13%\n",
            "Epoch 101 | T: 41s | Train: 2.9544 (32.1%) | Phys: 0.06 | Val: 1.6919 (56.00%) | Best: 57.13%\n",
            "Epoch 102 | T: 41s | Train: 2.9318 (32.9%) | Phys: 0.06 | Val: 1.6885 (56.32%) | Best: 57.13%\n",
            "Epoch 103 | T: 41s | Train: 2.9457 (32.7%) | Phys: 0.06 | Val: 1.6376 (56.77%) | Best: 57.13%\n",
            "Epoch 104 | T: 41s | Train: 2.9799 (31.9%) | Phys: 0.07 | Val: 1.6726 (56.58%) | Best: 57.13%\n",
            "Epoch 105 | T: 40s | Train: 3.0056 (31.4%) | Phys: 0.06 | Val: 1.6718 (56.58%) | Best: 57.13%\n",
            "Epoch 106 | T: 41s | Train: 2.9331 (32.9%) | Phys: 0.06 | Val: 1.6457 (56.69%) | Best: 57.13%\n",
            "Epoch 107 | T: 41s | Train: 2.9712 (32.1%) | Phys: 0.06 | Val: 1.6445 (57.47%) | Best: 57.47%\n",
            "Epoch 108 | T: 41s | Train: 2.9233 (33.4%) | Phys: 0.06 | Val: 1.6472 (56.94%) | Best: 57.47%\n",
            "Epoch 109 | T: 42s | Train: 2.9328 (33.2%) | Phys: 0.06 | Val: 1.6869 (56.41%) | Best: 57.47%\n",
            "Epoch 110 | T: 42s | Train: 2.9577 (32.4%) | Phys: 0.06 | Val: 1.6272 (57.28%) | Best: 57.47%\n",
            "Epoch 111 | T: 41s | Train: 2.9448 (32.7%) | Phys: 0.07 | Val: 1.6615 (56.77%) | Best: 57.47%\n",
            "Epoch 112 | T: 41s | Train: 2.9505 (32.6%) | Phys: 0.07 | Val: 1.6351 (57.20%) | Best: 57.47%\n",
            "Epoch 113 | T: 41s | Train: 2.9527 (32.5%) | Phys: 0.07 | Val: 1.6847 (56.76%) | Best: 57.47%\n",
            "Epoch 114 | T: 41s | Train: 2.9305 (33.1%) | Phys: 0.06 | Val: 1.6753 (57.43%) | Best: 57.47%\n",
            "Epoch 115 | T: 41s | Train: 2.9297 (33.2%) | Phys: 0.06 | Val: 1.6482 (57.09%) | Best: 57.47%\n",
            "Epoch 116 | T: 42s | Train: 2.9180 (33.4%) | Phys: 0.07 | Val: 1.6839 (56.63%) | Best: 57.47%\n",
            "Epoch 117 | T: 41s | Train: 2.9036 (33.5%) | Phys: 0.06 | Val: 1.7180 (55.26%) | Best: 57.47%\n",
            "Epoch 118 | T: 42s | Train: 2.9627 (32.3%) | Phys: 0.07 | Val: 1.6755 (57.16%) | Best: 57.47%\n",
            "Epoch 119 | T: 41s | Train: 2.9159 (33.1%) | Phys: 0.07 | Val: 1.6803 (56.29%) | Best: 57.47%\n",
            "Epoch 120 | T: 41s | Train: 2.9431 (32.9%) | Phys: 0.07 | Val: 1.6395 (57.84%) | Best: 57.84%\n",
            "Epoch 121 | T: 42s | Train: 2.8704 (34.3%) | Phys: 0.07 | Val: 1.6014 (58.49%) | Best: 58.49%\n",
            "Epoch 122 | T: 41s | Train: 2.9472 (32.7%) | Phys: 0.07 | Val: 1.6795 (56.41%) | Best: 58.49%\n",
            "Epoch 123 | T: 41s | Train: 2.8672 (34.5%) | Phys: 0.07 | Val: 1.6606 (57.17%) | Best: 58.49%\n",
            "Epoch 124 | T: 42s | Train: 2.8978 (33.9%) | Phys: 0.07 | Val: 1.6125 (58.18%) | Best: 58.49%\n",
            "Epoch 125 | T: 41s | Train: 2.8500 (34.9%) | Phys: 0.07 | Val: 1.6065 (57.89%) | Best: 58.49%\n",
            "Epoch 126 | T: 42s | Train: 2.9283 (33.2%) | Phys: 0.07 | Val: 1.6439 (57.74%) | Best: 58.49%\n",
            "Epoch 127 | T: 41s | Train: 2.9364 (32.9%) | Phys: 0.07 | Val: 1.7183 (55.85%) | Best: 58.49%\n",
            "Epoch 128 | T: 41s | Train: 2.8894 (34.3%) | Phys: 0.07 | Val: 1.6224 (57.47%) | Best: 58.49%\n",
            "Epoch 129 | T: 41s | Train: 2.9073 (33.7%) | Phys: 0.07 | Val: 1.6359 (57.82%) | Best: 58.49%\n",
            "Epoch 130 | T: 41s | Train: 2.9392 (32.8%) | Phys: 0.07 | Val: 1.6320 (57.84%) | Best: 58.49%\n",
            "Epoch 131 | T: 41s | Train: 2.9284 (33.1%) | Phys: 0.07 | Val: 1.6788 (56.79%) | Best: 58.49%\n",
            "Epoch 132 | T: 41s | Train: 2.9155 (33.3%) | Phys: 0.07 | Val: 1.5881 (58.17%) | Best: 58.49%\n",
            "Epoch 133 | T: 41s | Train: 2.8902 (34.0%) | Phys: 0.07 | Val: 1.6327 (57.99%) | Best: 58.49%\n",
            "Epoch 134 | T: 41s | Train: 2.8805 (34.2%) | Phys: 0.07 | Val: 1.6357 (57.58%) | Best: 58.49%\n",
            "Epoch 135 | T: 41s | Train: 2.8980 (34.1%) | Phys: 0.07 | Val: 1.6090 (58.30%) | Best: 58.49%\n",
            "Epoch 136 | T: 42s | Train: 2.9746 (31.9%) | Phys: 0.07 | Val: 1.6363 (57.95%) | Best: 58.49%\n",
            "Epoch 137 | T: 41s | Train: 2.9585 (32.7%) | Phys: 0.07 | Val: 1.6486 (57.80%) | Best: 58.49%\n",
            "Epoch 138 | T: 41s | Train: 2.8591 (34.8%) | Phys: 0.07 | Val: 1.6243 (58.11%) | Best: 58.49%\n",
            "Epoch 139 | T: 41s | Train: 2.9136 (33.7%) | Phys: 0.07 | Val: 1.5730 (58.86%) | Best: 58.86%\n",
            "Epoch 140 | T: 41s | Train: 2.9374 (32.9%) | Phys: 0.07 | Val: 1.5599 (59.34%) | Best: 59.34%\n",
            "Epoch 141 | T: 41s | Train: 2.8926 (33.9%) | Phys: 0.07 | Val: 1.6038 (57.65%) | Best: 59.34%\n",
            "Epoch 142 | T: 42s | Train: 2.9103 (33.7%) | Phys: 0.07 | Val: 1.5975 (59.20%) | Best: 59.34%\n",
            "Epoch 143 | T: 41s | Train: 2.8568 (34.7%) | Phys: 0.07 | Val: 1.5923 (58.61%) | Best: 59.34%\n",
            "Epoch 144 | T: 41s | Train: 2.8735 (34.2%) | Phys: 0.07 | Val: 1.5611 (59.04%) | Best: 59.34%\n",
            "Epoch 145 | T: 41s | Train: 2.8916 (34.0%) | Phys: 0.07 | Val: 1.5715 (58.93%) | Best: 59.34%\n",
            "Epoch 146 | T: 41s | Train: 2.9182 (33.4%) | Phys: 0.07 | Val: 1.5642 (59.78%) | Best: 59.78%\n",
            "Epoch 147 | T: 41s | Train: 2.8804 (34.3%) | Phys: 0.07 | Val: 1.6405 (58.10%) | Best: 59.78%\n",
            "Epoch 148 | T: 42s | Train: 2.8453 (34.9%) | Phys: 0.07 | Val: 1.5605 (58.86%) | Best: 59.78%\n",
            "Epoch 149 | T: 41s | Train: 2.8676 (34.7%) | Phys: 0.07 | Val: 1.5433 (59.87%) | Best: 59.87%\n",
            "Epoch 150 | T: 42s | Train: 2.8633 (34.6%) | Phys: 0.07 | Val: 1.6165 (57.68%) | Best: 59.87%\n",
            "Epoch 151 | T: 42s | Train: 2.9391 (33.0%) | Phys: 0.07 | Val: 1.5825 (59.31%) | Best: 59.87%\n",
            "Epoch 152 | T: 41s | Train: 2.8938 (34.2%) | Phys: 0.07 | Val: 1.6296 (57.80%) | Best: 59.87%\n",
            "Epoch 153 | T: 41s | Train: 2.8941 (34.3%) | Phys: 0.07 | Val: 1.5631 (59.58%) | Best: 59.87%\n",
            "Epoch 154 | T: 41s | Train: 2.8633 (34.7%) | Phys: 0.07 | Val: 1.5296 (59.74%) | Best: 59.87%\n",
            "Epoch 155 | T: 42s | Train: 2.8852 (34.4%) | Phys: 0.07 | Val: 1.5758 (59.30%) | Best: 59.87%\n",
            "Epoch 156 | T: 42s | Train: 2.8686 (34.6%) | Phys: 0.07 | Val: 1.5737 (59.75%) | Best: 59.87%\n",
            "Epoch 157 | T: 41s | Train: 2.8528 (35.2%) | Phys: 0.07 | Val: 1.5362 (60.43%) | Best: 60.43%\n",
            "Epoch 158 | T: 42s | Train: 2.8395 (35.4%) | Phys: 0.07 | Val: 1.5300 (59.64%) | Best: 60.43%\n",
            "Epoch 159 | T: 41s | Train: 2.8450 (35.5%) | Phys: 0.07 | Val: 1.6085 (58.23%) | Best: 60.43%\n",
            "Epoch 160 | T: 42s | Train: 2.8298 (35.7%) | Phys: 0.07 | Val: 1.5280 (60.05%) | Best: 60.43%\n",
            "Epoch 161 | T: 41s | Train: 2.8166 (35.7%) | Phys: 0.07 | Val: 1.4931 (60.69%) | Best: 60.69%\n",
            "Epoch 162 | T: 41s | Train: 2.8204 (35.9%) | Phys: 0.07 | Val: 1.4979 (61.30%) | Best: 61.30%\n",
            "Epoch 163 | T: 42s | Train: 2.8613 (34.6%) | Phys: 0.07 | Val: 1.5332 (59.68%) | Best: 61.30%\n",
            "Epoch 164 | T: 41s | Train: 2.7789 (36.7%) | Phys: 0.07 | Val: 1.5267 (60.18%) | Best: 61.30%\n",
            "Epoch 165 | T: 42s | Train: 2.8891 (34.6%) | Phys: 0.07 | Val: 1.5526 (59.54%) | Best: 61.30%\n",
            "Epoch 166 | T: 41s | Train: 2.8500 (35.3%) | Phys: 0.07 | Val: 1.5771 (58.79%) | Best: 61.30%\n",
            "Epoch 167 | T: 41s | Train: 2.8148 (36.1%) | Phys: 0.07 | Val: 1.5321 (60.72%) | Best: 61.30%\n",
            "Epoch 168 | T: 41s | Train: 2.8414 (35.2%) | Phys: 0.07 | Val: 1.5105 (60.36%) | Best: 61.30%\n",
            "Epoch 169 | T: 41s | Train: 2.7930 (36.6%) | Phys: 0.07 | Val: 1.5013 (61.13%) | Best: 61.30%\n",
            "Epoch 170 | T: 42s | Train: 2.8392 (35.3%) | Phys: 0.07 | Val: 1.5120 (60.07%) | Best: 61.30%\n",
            "Epoch 171 | T: 41s | Train: 2.8164 (35.8%) | Phys: 0.07 | Val: 1.5028 (60.90%) | Best: 61.30%\n",
            "Epoch 172 | T: 42s | Train: 2.8548 (34.9%) | Phys: 0.07 | Val: 1.5003 (60.69%) | Best: 61.30%\n",
            "Epoch 173 | T: 42s | Train: 2.7849 (36.5%) | Phys: 0.07 | Val: 1.5181 (60.46%) | Best: 61.30%\n",
            "Epoch 174 | T: 42s | Train: 2.8315 (35.3%) | Phys: 0.07 | Val: 1.5136 (60.44%) | Best: 61.30%\n",
            "Epoch 175 | T: 41s | Train: 2.7818 (36.7%) | Phys: 0.07 | Val: 1.4397 (62.34%) | Best: 62.34%\n",
            "Epoch 176 | T: 41s | Train: 2.8231 (35.9%) | Phys: 0.07 | Val: 1.4831 (61.27%) | Best: 62.34%\n",
            "Epoch 177 | T: 42s | Train: 2.8092 (36.0%) | Phys: 0.07 | Val: 1.4677 (61.42%) | Best: 62.34%\n",
            "Epoch 178 | T: 41s | Train: 2.7649 (37.1%) | Phys: 0.07 | Val: 1.4980 (60.65%) | Best: 62.34%\n",
            "Epoch 179 | T: 41s | Train: 2.7933 (36.5%) | Phys: 0.07 | Val: 1.4969 (60.99%) | Best: 62.34%\n",
            "Epoch 180 | T: 42s | Train: 2.7587 (37.3%) | Phys: 0.07 | Val: 1.4516 (61.85%) | Best: 62.34%\n",
            "Epoch 181 | T: 41s | Train: 2.7588 (37.3%) | Phys: 0.07 | Val: 1.5091 (61.10%) | Best: 62.34%\n",
            "Epoch 182 | T: 41s | Train: 2.8216 (35.8%) | Phys: 0.07 | Val: 1.4723 (61.23%) | Best: 62.34%\n",
            "Epoch 183 | T: 42s | Train: 2.7994 (36.6%) | Phys: 0.07 | Val: 1.4588 (62.09%) | Best: 62.34%\n",
            "Epoch 184 | T: 42s | Train: 2.7284 (37.9%) | Phys: 0.07 | Val: 1.4774 (61.65%) | Best: 62.34%\n",
            "Epoch 185 | T: 41s | Train: 2.7845 (36.6%) | Phys: 0.07 | Val: 1.4619 (61.79%) | Best: 62.34%\n",
            "Epoch 186 | T: 41s | Train: 2.7346 (37.9%) | Phys: 0.07 | Val: 1.4654 (61.39%) | Best: 62.34%\n",
            "Epoch 187 | T: 41s | Train: 2.7619 (37.3%) | Phys: 0.07 | Val: 1.4759 (61.77%) | Best: 62.34%\n",
            "Epoch 188 | T: 42s | Train: 2.8279 (35.9%) | Phys: 0.07 | Val: 1.4691 (61.62%) | Best: 62.34%\n",
            "Epoch 189 | T: 42s | Train: 2.7112 (38.3%) | Phys: 0.07 | Val: 1.4445 (61.53%) | Best: 62.34%\n",
            "Epoch 190 | T: 41s | Train: 2.7367 (37.8%) | Phys: 0.07 | Val: 1.4362 (61.28%) | Best: 62.34%\n",
            "Epoch 191 | T: 41s | Train: 2.8109 (36.4%) | Phys: 0.07 | Val: 1.4864 (61.67%) | Best: 62.34%\n",
            "Epoch 192 | T: 41s | Train: 2.7341 (38.1%) | Phys: 0.07 | Val: 1.4448 (62.86%) | Best: 62.86%\n",
            "Epoch 193 | T: 41s | Train: 2.7270 (38.2%) | Phys: 0.07 | Val: 1.4801 (61.47%) | Best: 62.86%\n",
            "Epoch 194 | T: 41s | Train: 2.7449 (37.9%) | Phys: 0.07 | Val: 1.4116 (62.98%) | Best: 62.98%\n",
            "Epoch 195 | T: 41s | Train: 2.7335 (37.8%) | Phys: 0.07 | Val: 1.4437 (62.11%) | Best: 62.98%\n",
            "Epoch 196 | T: 42s | Train: 2.6838 (39.0%) | Phys: 0.07 | Val: 1.4300 (62.82%) | Best: 62.98%\n",
            "Epoch 197 | T: 41s | Train: 2.7155 (38.3%) | Phys: 0.07 | Val: 1.4182 (62.96%) | Best: 62.98%\n",
            "Epoch 198 | T: 41s | Train: 2.7117 (38.4%) | Phys: 0.07 | Val: 1.3817 (62.75%) | Best: 62.98%\n",
            "Epoch 199 | T: 41s | Train: 2.6644 (39.2%) | Phys: 0.07 | Val: 1.4626 (62.27%) | Best: 62.98%\n",
            "Epoch 200 | T: 41s | Train: 2.7565 (37.5%) | Phys: 0.07 | Val: 1.4257 (63.30%) | Best: 63.30%\n",
            "Epoch 201 | T: 42s | Train: 2.6966 (38.8%) | Phys: 0.07 | Val: 1.4023 (63.05%) | Best: 63.30%\n",
            "Epoch 202 | T: 41s | Train: 2.7360 (38.2%) | Phys: 0.07 | Val: 1.3992 (63.23%) | Best: 63.30%\n",
            "Epoch 203 | T: 42s | Train: 2.6964 (38.5%) | Phys: 0.07 | Val: 1.3966 (63.89%) | Best: 63.89%\n",
            "Epoch 204 | T: 41s | Train: 2.7307 (37.9%) | Phys: 0.07 | Val: 1.3645 (63.76%) | Best: 63.89%\n",
            "Epoch 205 | T: 41s | Train: 2.6928 (38.8%) | Phys: 0.07 | Val: 1.4013 (63.43%) | Best: 63.89%\n",
            "Epoch 206 | T: 42s | Train: 2.6823 (39.3%) | Phys: 0.07 | Val: 1.3614 (64.05%) | Best: 64.05%\n",
            "Epoch 207 | T: 41s | Train: 2.6737 (39.2%) | Phys: 0.07 | Val: 1.3814 (63.70%) | Best: 64.05%\n",
            "Epoch 208 | T: 41s | Train: 2.7111 (38.4%) | Phys: 0.07 | Val: 1.3833 (63.42%) | Best: 64.05%\n",
            "Epoch 209 | T: 42s | Train: 2.6931 (38.9%) | Phys: 0.07 | Val: 1.4320 (63.08%) | Best: 64.05%\n",
            "Epoch 210 | T: 41s | Train: 2.6410 (40.4%) | Phys: 0.07 | Val: 1.3902 (63.97%) | Best: 64.05%\n",
            "Epoch 211 | T: 41s | Train: 2.6437 (40.0%) | Phys: 0.07 | Val: 1.3779 (63.64%) | Best: 64.05%\n",
            "Epoch 212 | T: 42s | Train: 2.6378 (40.2%) | Phys: 0.07 | Val: 1.3968 (63.99%) | Best: 64.05%\n",
            "Epoch 213 | T: 42s | Train: 2.7319 (38.1%) | Phys: 0.08 | Val: 1.3442 (64.76%) | Best: 64.76%\n",
            "Epoch 214 | T: 41s | Train: 2.6703 (39.7%) | Phys: 0.07 | Val: 1.3630 (64.38%) | Best: 64.76%\n",
            "Epoch 215 | T: 41s | Train: 2.7073 (38.8%) | Phys: 0.07 | Val: 1.3549 (64.25%) | Best: 64.76%\n",
            "Epoch 216 | T: 43s | Train: 2.6390 (40.2%) | Phys: 0.07 | Val: 1.3273 (65.04%) | Best: 65.04%\n",
            "Epoch 217 | T: 42s | Train: 2.6222 (40.6%) | Phys: 0.07 | Val: 1.3270 (65.27%) | Best: 65.27%\n",
            "Epoch 218 | T: 41s | Train: 2.6600 (39.9%) | Phys: 0.07 | Val: 1.3136 (65.14%) | Best: 65.27%\n",
            "Epoch 219 | T: 41s | Train: 2.6300 (40.4%) | Phys: 0.07 | Val: 1.3229 (65.46%) | Best: 65.46%\n",
            "Epoch 220 | T: 42s | Train: 2.6739 (39.5%) | Phys: 0.07 | Val: 1.3207 (65.18%) | Best: 65.46%\n",
            "Epoch 221 | T: 42s | Train: 2.5758 (41.9%) | Phys: 0.07 | Val: 1.2847 (65.83%) | Best: 65.83%\n",
            "Epoch 222 | T: 42s | Train: 2.6273 (40.5%) | Phys: 0.08 | Val: 1.2778 (66.15%) | Best: 66.15%\n",
            "Epoch 223 | T: 42s | Train: 2.6505 (40.2%) | Phys: 0.08 | Val: 1.3395 (65.09%) | Best: 66.15%\n",
            "Epoch 224 | T: 41s | Train: 2.6033 (41.3%) | Phys: 0.07 | Val: 1.3114 (65.18%) | Best: 66.15%\n",
            "Epoch 225 | T: 40s | Train: 2.7111 (38.8%) | Phys: 0.08 | Val: 1.3281 (65.21%) | Best: 66.15%\n",
            "Epoch 226 | T: 41s | Train: 2.5724 (42.0%) | Phys: 0.08 | Val: 1.2906 (65.61%) | Best: 66.15%\n",
            "Epoch 227 | T: 41s | Train: 2.5500 (42.5%) | Phys: 0.08 | Val: 1.2744 (66.43%) | Best: 66.43%\n",
            "Epoch 228 | T: 42s | Train: 2.5735 (42.1%) | Phys: 0.08 | Val: 1.2965 (65.80%) | Best: 66.43%\n",
            "Epoch 229 | T: 41s | Train: 2.5583 (42.3%) | Phys: 0.08 | Val: 1.2745 (66.26%) | Best: 66.43%\n",
            "Epoch 230 | T: 41s | Train: 2.5596 (42.3%) | Phys: 0.08 | Val: 1.2902 (65.96%) | Best: 66.43%\n",
            "Epoch 231 | T: 40s | Train: 2.5958 (41.4%) | Phys: 0.08 | Val: 1.2749 (66.23%) | Best: 66.43%\n",
            "Epoch 232 | T: 42s | Train: 2.6215 (41.1%) | Phys: 0.08 | Val: 1.3543 (64.47%) | Best: 66.43%\n",
            "Epoch 233 | T: 42s | Train: 2.6432 (40.4%) | Phys: 0.08 | Val: 1.2913 (66.40%) | Best: 66.43%\n",
            "Epoch 234 | T: 41s | Train: 2.6176 (41.3%) | Phys: 0.08 | Val: 1.3013 (66.47%) | Best: 66.47%\n",
            "Epoch 235 | T: 42s | Train: 2.5756 (41.9%) | Phys: 0.08 | Val: 1.2676 (66.29%) | Best: 66.47%\n",
            "Epoch 236 | T: 42s | Train: 2.5809 (42.0%) | Phys: 0.08 | Val: 1.2597 (67.18%) | Best: 67.18%\n",
            "Epoch 237 | T: 42s | Train: 2.5779 (41.6%) | Phys: 0.08 | Val: 1.2719 (66.80%) | Best: 67.18%\n",
            "Epoch 238 | T: 42s | Train: 2.5697 (42.4%) | Phys: 0.08 | Val: 1.2563 (66.53%) | Best: 67.18%\n",
            "Epoch 239 | T: 41s | Train: 2.5359 (42.6%) | Phys: 0.08 | Val: 1.2767 (66.71%) | Best: 67.18%\n",
            "Epoch 240 | T: 41s | Train: 2.5988 (41.6%) | Phys: 0.08 | Val: 1.3065 (66.18%) | Best: 67.18%\n",
            "Epoch 241 | T: 42s | Train: 2.5977 (41.6%) | Phys: 0.08 | Val: 1.2739 (66.94%) | Best: 67.18%\n",
            "Epoch 242 | T: 41s | Train: 2.5486 (43.1%) | Phys: 0.08 | Val: 1.2515 (66.62%) | Best: 67.18%\n",
            "Epoch 243 | T: 41s | Train: 2.5456 (42.3%) | Phys: 0.08 | Val: 1.2349 (67.12%) | Best: 67.18%\n",
            "Epoch 244 | T: 42s | Train: 2.5366 (43.0%) | Phys: 0.08 | Val: 1.2584 (66.41%) | Best: 67.18%\n",
            "Epoch 245 | T: 41s | Train: 2.4855 (44.1%) | Phys: 0.08 | Val: 1.2499 (67.14%) | Best: 67.18%\n",
            "Epoch 246 | T: 42s | Train: 2.5526 (42.6%) | Phys: 0.08 | Val: 1.2189 (66.76%) | Best: 67.18%\n",
            "Epoch 247 | T: 41s | Train: 2.5289 (43.2%) | Phys: 0.08 | Val: 1.2757 (65.64%) | Best: 67.18%\n",
            "Epoch 248 | T: 41s | Train: 2.5311 (43.1%) | Phys: 0.08 | Val: 1.2622 (66.61%) | Best: 67.18%\n",
            "Epoch 249 | T: 42s | Train: 2.5081 (43.8%) | Phys: 0.08 | Val: 1.2020 (67.44%) | Best: 67.44%\n",
            "Epoch 250 | T: 41s | Train: 2.5567 (42.7%) | Phys: 0.08 | Val: 1.2666 (67.37%) | Best: 67.44%\n",
            "Epoch 251 | T: 41s | Train: 2.5012 (43.4%) | Phys: 0.08 | Val: 1.2144 (67.46%) | Best: 67.46%\n",
            "Epoch 252 | T: 41s | Train: 2.4669 (44.9%) | Phys: 0.08 | Val: 1.2535 (66.57%) | Best: 67.46%\n",
            "Epoch 253 | T: 41s | Train: 2.5713 (42.1%) | Phys: 0.08 | Val: 1.2355 (67.83%) | Best: 67.83%\n",
            "Epoch 254 | T: 42s | Train: 2.4851 (44.4%) | Phys: 0.08 | Val: 1.1987 (67.86%) | Best: 67.86%\n",
            "Epoch 255 | T: 42s | Train: 2.4754 (44.5%) | Phys: 0.08 | Val: 1.1893 (68.33%) | Best: 68.33%\n",
            "Epoch 256 | T: 41s | Train: 2.4153 (46.0%) | Phys: 0.08 | Val: 1.2266 (68.49%) | Best: 68.49%\n",
            "Epoch 257 | T: 41s | Train: 2.4395 (45.4%) | Phys: 0.08 | Val: 1.2333 (67.33%) | Best: 68.49%\n",
            "Epoch 258 | T: 41s | Train: 2.4532 (45.1%) | Phys: 0.08 | Val: 1.1547 (69.53%) | Best: 69.53%\n",
            "Epoch 259 | T: 42s | Train: 2.4101 (45.9%) | Phys: 0.08 | Val: 1.1750 (69.11%) | Best: 69.53%\n",
            "Epoch 260 | T: 41s | Train: 2.4196 (45.9%) | Phys: 0.08 | Val: 1.1511 (69.42%) | Best: 69.53%\n",
            "Epoch 261 | T: 41s | Train: 2.4505 (45.2%) | Phys: 0.08 | Val: 1.1908 (68.67%) | Best: 69.53%\n",
            "Epoch 262 | T: 41s | Train: 2.4935 (44.4%) | Phys: 0.08 | Val: 1.2282 (68.83%) | Best: 69.53%\n",
            "Epoch 263 | T: 42s | Train: 2.4561 (45.2%) | Phys: 0.08 | Val: 1.1990 (68.20%) | Best: 69.53%\n",
            "Epoch 264 | T: 41s | Train: 2.4112 (46.3%) | Phys: 0.08 | Val: 1.2159 (67.96%) | Best: 69.53%\n",
            "Epoch 265 | T: 42s | Train: 2.4341 (45.6%) | Phys: 0.08 | Val: 1.2002 (68.29%) | Best: 69.53%\n",
            "Epoch 266 | T: 42s | Train: 2.4922 (44.4%) | Phys: 0.08 | Val: 1.1955 (68.87%) | Best: 69.53%\n",
            "Epoch 267 | T: 40s | Train: 2.4719 (45.0%) | Phys: 0.08 | Val: 1.2068 (68.35%) | Best: 69.53%\n",
            "Epoch 268 | T: 41s | Train: 2.4375 (45.3%) | Phys: 0.08 | Val: 1.1915 (69.31%) | Best: 69.53%\n",
            "Epoch 269 | T: 42s | Train: 2.4795 (44.5%) | Phys: 0.08 | Val: 1.1567 (69.64%) | Best: 69.64%\n",
            "Epoch 270 | T: 41s | Train: 2.4320 (45.7%) | Phys: 0.08 | Val: 1.1508 (70.41%) | Best: 70.41%\n",
            "Epoch 271 | T: 42s | Train: 2.2983 (49.0%) | Phys: 0.08 | Val: 1.1664 (68.62%) | Best: 70.41%\n",
            "Epoch 272 | T: 42s | Train: 2.3861 (46.8%) | Phys: 0.08 | Val: 1.1250 (69.85%) | Best: 70.41%\n",
            "Epoch 273 | T: 42s | Train: 2.3609 (47.6%) | Phys: 0.08 | Val: 1.1872 (68.65%) | Best: 70.41%\n",
            "Epoch 274 | T: 41s | Train: 2.4397 (45.5%) | Phys: 0.08 | Val: 1.1639 (69.11%) | Best: 70.41%\n",
            "Epoch 275 | T: 41s | Train: 2.4190 (46.2%) | Phys: 0.08 | Val: 1.1673 (70.02%) | Best: 70.41%\n",
            "Epoch 276 | T: 42s | Train: 2.3636 (47.3%) | Phys: 0.08 | Val: 1.1424 (69.82%) | Best: 70.41%\n",
            "Epoch 277 | T: 41s | Train: 2.3395 (48.2%) | Phys: 0.08 | Val: 1.1280 (70.01%) | Best: 70.41%\n",
            "Epoch 278 | T: 41s | Train: 2.3455 (47.9%) | Phys: 0.08 | Val: 1.1463 (69.83%) | Best: 70.41%\n",
            "Epoch 279 | T: 41s | Train: 2.3939 (46.6%) | Phys: 0.08 | Val: 1.1178 (70.38%) | Best: 70.41%\n",
            "Epoch 280 | T: 41s | Train: 2.2946 (49.0%) | Phys: 0.08 | Val: 1.1216 (70.27%) | Best: 70.41%\n",
            "Epoch 281 | T: 41s | Train: 2.3706 (47.2%) | Phys: 0.08 | Val: 1.1239 (70.69%) | Best: 70.69%\n",
            "Epoch 282 | T: 42s | Train: 2.4026 (46.7%) | Phys: 0.08 | Val: 1.1630 (69.45%) | Best: 70.69%\n",
            "Epoch 283 | T: 41s | Train: 2.3025 (48.9%) | Phys: 0.08 | Val: 1.1209 (70.10%) | Best: 70.69%\n",
            "Epoch 284 | T: 40s | Train: 2.2829 (49.3%) | Phys: 0.08 | Val: 1.1261 (70.29%) | Best: 70.69%\n",
            "Epoch 285 | T: 41s | Train: 2.2930 (49.6%) | Phys: 0.08 | Val: 1.1309 (70.65%) | Best: 70.69%\n",
            "Epoch 286 | T: 41s | Train: 2.3533 (47.9%) | Phys: 0.08 | Val: 1.1219 (70.44%) | Best: 70.69%\n",
            "Epoch 287 | T: 41s | Train: 2.2860 (49.1%) | Phys: 0.08 | Val: 1.1236 (70.25%) | Best: 70.69%\n",
            "Epoch 288 | T: 42s | Train: 2.2982 (49.2%) | Phys: 0.08 | Val: 1.0886 (70.94%) | Best: 70.94%\n",
            "Epoch 289 | T: 41s | Train: 2.2984 (48.9%) | Phys: 0.08 | Val: 1.0903 (71.41%) | Best: 71.41%\n",
            "Epoch 290 | T: 41s | Train: 2.3401 (48.6%) | Phys: 0.08 | Val: 1.1185 (71.15%) | Best: 71.41%\n",
            "Epoch 291 | T: 41s | Train: 2.2363 (50.8%) | Phys: 0.08 | Val: 1.1480 (70.65%) | Best: 71.41%\n",
            "Epoch 292 | T: 41s | Train: 2.2626 (50.3%) | Phys: 0.08 | Val: 1.1164 (70.85%) | Best: 71.41%\n",
            "Epoch 293 | T: 42s | Train: 2.2290 (51.0%) | Phys: 0.08 | Val: 1.1026 (71.25%) | Best: 71.41%\n",
            "Epoch 294 | T: 41s | Train: 2.2165 (51.5%) | Phys: 0.08 | Val: 1.0971 (70.75%) | Best: 71.41%\n",
            "Epoch 295 | T: 41s | Train: 2.2717 (49.9%) | Phys: 0.08 | Val: 1.1214 (70.73%) | Best: 71.41%\n",
            "Epoch 296 | T: 41s | Train: 2.2882 (49.3%) | Phys: 0.08 | Val: 1.1149 (70.47%) | Best: 71.41%\n",
            "Epoch 297 | T: 41s | Train: 2.2496 (50.4%) | Phys: 0.08 | Val: 1.1021 (71.59%) | Best: 71.59%\n",
            "Epoch 298 | T: 41s | Train: 2.1608 (53.0%) | Phys: 0.08 | Val: 1.0949 (71.72%) | Best: 71.72%\n",
            "Epoch 299 | T: 41s | Train: 2.2563 (50.3%) | Phys: 0.08 | Val: 1.0864 (72.14%) | Best: 72.14%\n",
            "Epoch 300 | T: 42s | Train: 2.2924 (49.6%) | Phys: 0.08 | Val: 1.0899 (71.23%) | Best: 72.14%\n",
            "Epoch 301 | T: 41s | Train: 2.1794 (52.5%) | Phys: 0.08 | Val: 1.0872 (71.70%) | Best: 72.14%\n",
            "Epoch 302 | T: 41s | Train: 2.2376 (50.5%) | Phys: 0.08 | Val: 1.0640 (72.22%) | Best: 72.22%\n",
            "Epoch 303 | T: 43s | Train: 2.1622 (52.4%) | Phys: 0.08 | Val: 1.0599 (71.87%) | Best: 72.22%\n",
            "Epoch 304 | T: 41s | Train: 2.2444 (50.9%) | Phys: 0.08 | Val: 1.0783 (71.51%) | Best: 72.22%\n",
            "Epoch 305 | T: 41s | Train: 2.2025 (51.5%) | Phys: 0.08 | Val: 1.0672 (71.56%) | Best: 72.22%\n",
            "Epoch 306 | T: 42s | Train: 2.2279 (51.1%) | Phys: 0.08 | Val: 1.0680 (72.16%) | Best: 72.22%\n",
            "Epoch 307 | T: 41s | Train: 2.1575 (52.9%) | Phys: 0.08 | Val: 1.0671 (72.20%) | Best: 72.22%\n",
            "Epoch 308 | T: 41s | Train: 2.2202 (51.1%) | Phys: 0.08 | Val: 1.0545 (72.31%) | Best: 72.31%\n",
            "Epoch 309 | T: 41s | Train: 2.2245 (51.4%) | Phys: 0.08 | Val: 1.0594 (72.02%) | Best: 72.31%\n",
            "Epoch 310 | T: 41s | Train: 2.0459 (55.9%) | Phys: 0.08 | Val: 1.0380 (72.96%) | Best: 72.96%\n",
            "Epoch 311 | T: 41s | Train: 2.2196 (51.4%) | Phys: 0.08 | Val: 1.0261 (72.69%) | Best: 72.96%\n",
            "Epoch 312 | T: 40s | Train: 2.1241 (54.0%) | Phys: 0.08 | Val: 1.0615 (72.57%) | Best: 72.96%\n",
            "Epoch 313 | T: 41s | Train: 2.1310 (53.5%) | Phys: 0.08 | Val: 1.0517 (72.70%) | Best: 72.96%\n",
            "Epoch 314 | T: 41s | Train: 2.1351 (53.4%) | Phys: 0.08 | Val: 1.0563 (72.27%) | Best: 72.96%\n",
            "Epoch 315 | T: 42s | Train: 2.0948 (54.6%) | Phys: 0.08 | Val: 1.0422 (73.00%) | Best: 73.00%\n",
            "Epoch 316 | T: 41s | Train: 2.1294 (53.6%) | Phys: 0.08 | Val: 1.0542 (72.62%) | Best: 73.00%\n",
            "Epoch 317 | T: 42s | Train: 2.1873 (52.3%) | Phys: 0.08 | Val: 1.0538 (73.21%) | Best: 73.21%\n",
            "Epoch 318 | T: 41s | Train: 2.1274 (53.7%) | Phys: 0.08 | Val: 1.0592 (72.29%) | Best: 73.21%\n",
            "Epoch 319 | T: 41s | Train: 2.0044 (56.8%) | Phys: 0.08 | Val: 1.0208 (73.35%) | Best: 73.35%\n",
            "Epoch 320 | T: 42s | Train: 2.0421 (55.9%) | Phys: 0.08 | Val: 1.0453 (73.14%) | Best: 73.35%\n",
            "Epoch 321 | T: 41s | Train: 2.1382 (53.8%) | Phys: 0.08 | Val: 1.0490 (73.08%) | Best: 73.35%\n",
            "Epoch 322 | T: 41s | Train: 2.0635 (56.0%) | Phys: 0.08 | Val: 1.0558 (73.34%) | Best: 73.35%\n",
            "Epoch 323 | T: 42s | Train: 2.0765 (55.2%) | Phys: 0.08 | Val: 1.0350 (73.06%) | Best: 73.35%\n",
            "Epoch 324 | T: 41s | Train: 2.1008 (54.3%) | Phys: 0.08 | Val: 1.0208 (73.64%) | Best: 73.64%\n",
            "Epoch 325 | T: 42s | Train: 2.0837 (55.2%) | Phys: 0.08 | Val: 1.0220 (73.65%) | Best: 73.65%\n",
            "Epoch 326 | T: 41s | Train: 2.1077 (54.4%) | Phys: 0.08 | Val: 1.0307 (74.26%) | Best: 74.26%\n",
            "Epoch 327 | T: 41s | Train: 2.0270 (56.5%) | Phys: 0.08 | Val: 1.0327 (73.77%) | Best: 74.26%\n",
            "Epoch 328 | T: 40s | Train: 2.0119 (56.8%) | Phys: 0.08 | Val: 1.0283 (73.99%) | Best: 74.26%\n",
            "Epoch 329 | T: 42s | Train: 1.9858 (57.4%) | Phys: 0.08 | Val: 1.0217 (73.57%) | Best: 74.26%\n",
            "Epoch 330 | T: 42s | Train: 2.1061 (54.5%) | Phys: 0.08 | Val: 1.0122 (73.75%) | Best: 74.26%\n",
            "Epoch 331 | T: 41s | Train: 2.0209 (57.1%) | Phys: 0.08 | Val: 1.0212 (73.62%) | Best: 74.26%\n",
            "Epoch 332 | T: 40s | Train: 2.0109 (56.9%) | Phys: 0.08 | Val: 1.0094 (73.86%) | Best: 74.26%\n",
            "Epoch 333 | T: 42s | Train: 2.0347 (56.3%) | Phys: 0.08 | Val: 1.0128 (74.02%) | Best: 74.26%\n",
            "Epoch 334 | T: 41s | Train: 2.0019 (57.4%) | Phys: 0.08 | Val: 1.0175 (74.26%) | Best: 74.26%\n",
            "Epoch 335 | T: 41s | Train: 1.9559 (58.1%) | Phys: 0.08 | Val: 1.0130 (73.84%) | Best: 74.26%\n",
            "Epoch 336 | T: 41s | Train: 2.0030 (56.9%) | Phys: 0.08 | Val: 1.0037 (74.03%) | Best: 74.26%\n",
            "Epoch 337 | T: 40s | Train: 2.0266 (56.7%) | Phys: 0.08 | Val: 1.0024 (74.33%) | Best: 74.33%\n",
            "Epoch 338 | T: 41s | Train: 2.0005 (57.0%) | Phys: 0.08 | Val: 1.0045 (74.45%) | Best: 74.45%\n",
            "Epoch 339 | T: 42s | Train: 2.0971 (54.8%) | Phys: 0.08 | Val: 0.9924 (74.08%) | Best: 74.45%\n",
            "Epoch 340 | T: 41s | Train: 1.9520 (58.2%) | Phys: 0.08 | Val: 0.9917 (74.41%) | Best: 74.45%\n",
            "Epoch 341 | T: 41s | Train: 1.9656 (58.2%) | Phys: 0.08 | Val: 1.0011 (74.50%) | Best: 74.50%\n",
            "Epoch 342 | T: 41s | Train: 2.0149 (57.1%) | Phys: 0.08 | Val: 0.9935 (74.25%) | Best: 74.50%\n",
            "Epoch 343 | T: 41s | Train: 1.9255 (59.0%) | Phys: 0.08 | Val: 0.9786 (74.77%) | Best: 74.77%\n",
            "Epoch 344 | T: 41s | Train: 1.9191 (59.2%) | Phys: 0.09 | Val: 0.9853 (75.17%) | Best: 75.17%\n",
            "Epoch 345 | T: 41s | Train: 1.9263 (59.2%) | Phys: 0.08 | Val: 0.9824 (74.58%) | Best: 75.17%\n",
            "Epoch 346 | T: 42s | Train: 1.9426 (58.6%) | Phys: 0.08 | Val: 0.9850 (74.61%) | Best: 75.17%\n",
            "Epoch 347 | T: 41s | Train: 1.9448 (58.3%) | Phys: 0.08 | Val: 1.0062 (74.72%) | Best: 75.17%\n",
            "Epoch 348 | T: 40s | Train: 1.9087 (60.0%) | Phys: 0.08 | Val: 0.9771 (75.20%) | Best: 75.20%\n",
            "Epoch 349 | T: 42s | Train: 1.8806 (60.4%) | Phys: 0.09 | Val: 0.9843 (74.89%) | Best: 75.20%\n",
            "Epoch 350 | T: 41s | Train: 1.9942 (57.2%) | Phys: 0.09 | Val: 0.9643 (75.32%) | Best: 75.32%\n",
            "Epoch 351 | T: 42s | Train: 1.8616 (61.2%) | Phys: 0.08 | Val: 0.9720 (75.36%) | Best: 75.36%\n",
            "Epoch 352 | T: 42s | Train: 1.8395 (61.4%) | Phys: 0.08 | Val: 0.9614 (76.00%) | Best: 76.00%\n",
            "Epoch 353 | T: 41s | Train: 1.9302 (59.3%) | Phys: 0.09 | Val: 0.9723 (75.31%) | Best: 76.00%\n",
            "Epoch 354 | T: 41s | Train: 1.9734 (57.9%) | Phys: 0.08 | Val: 0.9689 (75.50%) | Best: 76.00%\n",
            "Epoch 355 | T: 41s | Train: 1.8204 (61.8%) | Phys: 0.08 | Val: 0.9704 (75.30%) | Best: 76.00%\n",
            "Epoch 356 | T: 42s | Train: 1.9100 (59.3%) | Phys: 0.08 | Val: 0.9597 (75.56%) | Best: 76.00%\n",
            "Epoch 357 | T: 41s | Train: 1.9616 (58.1%) | Phys: 0.08 | Val: 0.9684 (75.54%) | Best: 76.00%\n",
            "Epoch 358 | T: 41s | Train: 1.8461 (60.7%) | Phys: 0.08 | Val: 0.9631 (75.38%) | Best: 76.00%\n",
            "Epoch 359 | T: 42s | Train: 1.8963 (59.8%) | Phys: 0.08 | Val: 0.9766 (75.72%) | Best: 76.00%\n",
            "Epoch 360 | T: 41s | Train: 1.8257 (61.2%) | Phys: 0.08 | Val: 0.9566 (75.98%) | Best: 76.00%\n",
            "Epoch 361 | T: 41s | Train: 1.8157 (62.2%) | Phys: 0.08 | Val: 0.9775 (75.37%) | Best: 76.00%\n",
            "Epoch 362 | T: 41s | Train: 1.8520 (61.3%) | Phys: 0.08 | Val: 0.9746 (75.68%) | Best: 76.00%\n",
            "Epoch 363 | T: 42s | Train: 1.8786 (60.2%) | Phys: 0.08 | Val: 0.9731 (75.78%) | Best: 76.00%\n",
            "Epoch 364 | T: 42s | Train: 1.7693 (63.3%) | Phys: 0.08 | Val: 0.9609 (76.10%) | Best: 76.10%\n",
            "Epoch 365 | T: 41s | Train: 1.7698 (63.2%) | Phys: 0.08 | Val: 0.9613 (75.83%) | Best: 76.10%\n",
            "Epoch 366 | T: 41s | Train: 1.7760 (62.8%) | Phys: 0.08 | Val: 0.9578 (75.79%) | Best: 76.10%\n",
            "Epoch 367 | T: 41s | Train: 1.8591 (61.0%) | Phys: 0.08 | Val: 0.9633 (75.89%) | Best: 76.10%\n",
            "Epoch 368 | T: 41s | Train: 1.7807 (62.4%) | Phys: 0.08 | Val: 0.9653 (76.07%) | Best: 76.10%\n",
            "Epoch 369 | T: 42s | Train: 1.8068 (61.8%) | Phys: 0.08 | Val: 0.9501 (76.10%) | Best: 76.10%\n",
            "Epoch 370 | T: 41s | Train: 1.7759 (62.9%) | Phys: 0.08 | Val: 0.9593 (76.22%) | Best: 76.22%\n",
            "Epoch 371 | T: 42s | Train: 1.8043 (62.0%) | Phys: 0.08 | Val: 0.9547 (75.86%) | Best: 76.22%\n",
            "Epoch 372 | T: 41s | Train: 1.7486 (63.6%) | Phys: 0.08 | Val: 0.9421 (76.29%) | Best: 76.29%\n",
            "Epoch 373 | T: 42s | Train: 1.7371 (63.8%) | Phys: 0.08 | Val: 0.9497 (76.25%) | Best: 76.29%\n",
            "Epoch 374 | T: 41s | Train: 1.8494 (61.2%) | Phys: 0.08 | Val: 0.9478 (76.52%) | Best: 76.52%\n",
            "Epoch 375 | T: 41s | Train: 1.8414 (60.9%) | Phys: 0.08 | Val: 0.9351 (76.42%) | Best: 76.52%\n",
            "Epoch 376 | T: 41s | Train: 1.7742 (62.8%) | Phys: 0.08 | Val: 0.9497 (76.54%) | Best: 76.54%\n",
            "Epoch 377 | T: 41s | Train: 1.8238 (61.9%) | Phys: 0.08 | Val: 0.9462 (76.46%) | Best: 76.54%\n",
            "Epoch 378 | T: 41s | Train: 1.8036 (61.8%) | Phys: 0.08 | Val: 0.9390 (76.77%) | Best: 76.77%\n",
            "Epoch 379 | T: 41s | Train: 1.7454 (64.1%) | Phys: 0.08 | Val: 0.9464 (76.40%) | Best: 76.77%\n",
            "Epoch 380 | T: 42s | Train: 1.6894 (64.8%) | Phys: 0.08 | Val: 0.9356 (76.59%) | Best: 76.77%\n",
            "Epoch 381 | T: 41s | Train: 1.7471 (64.0%) | Phys: 0.08 | Val: 0.9455 (76.47%) | Best: 76.77%\n",
            "Epoch 382 | T: 41s | Train: 1.7311 (63.8%) | Phys: 0.08 | Val: 0.9380 (76.37%) | Best: 76.77%\n",
            "Epoch 383 | T: 41s | Train: 1.7319 (64.2%) | Phys: 0.08 | Val: 0.9405 (76.52%) | Best: 76.77%\n",
            "Epoch 384 | T: 41s | Train: 1.7052 (64.5%) | Phys: 0.08 | Val: 0.9380 (76.60%) | Best: 76.77%\n",
            "Epoch 385 | T: 41s | Train: 1.8118 (61.9%) | Phys: 0.08 | Val: 0.9352 (76.81%) | Best: 76.81%\n",
            "Epoch 386 | T: 43s | Train: 1.6525 (65.9%) | Phys: 0.08 | Val: 0.9211 (76.79%) | Best: 76.81%\n",
            "Epoch 387 | T: 42s | Train: 1.8423 (60.8%) | Phys: 0.08 | Val: 0.9319 (76.81%) | Best: 76.81%\n",
            "Epoch 388 | T: 41s | Train: 1.7977 (61.9%) | Phys: 0.08 | Val: 0.9292 (76.86%) | Best: 76.86%\n",
            "Epoch 389 | T: 41s | Train: 1.7526 (63.2%) | Phys: 0.08 | Val: 0.9291 (76.98%) | Best: 76.98%\n",
            "Epoch 390 | T: 41s | Train: 1.7040 (64.7%) | Phys: 0.08 | Val: 0.9347 (76.69%) | Best: 76.98%\n",
            "Epoch 391 | T: 42s | Train: 1.7348 (64.0%) | Phys: 0.08 | Val: 0.9361 (76.74%) | Best: 76.98%\n",
            "Epoch 392 | T: 41s | Train: 1.7501 (63.5%) | Phys: 0.08 | Val: 0.9300 (76.76%) | Best: 76.98%\n",
            "Epoch 393 | T: 41s | Train: 1.7864 (62.4%) | Phys: 0.08 | Val: 0.9322 (76.84%) | Best: 76.98%\n",
            "Epoch 394 | T: 41s | Train: 1.7169 (64.3%) | Phys: 0.08 | Val: 0.9260 (76.83%) | Best: 76.98%\n",
            "Epoch 395 | T: 41s | Train: 1.7182 (64.4%) | Phys: 0.08 | Val: 0.9262 (76.68%) | Best: 76.98%\n",
            "Epoch 396 | T: 41s | Train: 1.7806 (62.7%) | Phys: 0.08 | Val: 0.9296 (76.60%) | Best: 76.98%\n",
            "Epoch 397 | T: 41s | Train: 1.7897 (62.4%) | Phys: 0.08 | Val: 0.9357 (76.73%) | Best: 76.98%\n",
            "Epoch 398 | T: 42s | Train: 1.7274 (64.0%) | Phys: 0.08 | Val: 0.9327 (76.68%) | Best: 76.98%\n",
            "Epoch 399 | T: 41s | Train: 1.7622 (62.7%) | Phys: 0.08 | Val: 0.9281 (76.78%) | Best: 76.98%\n",
            "Epoch 400 | T: 41s | Train: 1.7315 (63.9%) | Phys: 0.08 | Val: 0.9259 (76.82%) | Best: 76.98%\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": [],
      "name": "CIFAR-100 ViT SIGReg Strong",
      "gpuType": "L4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}