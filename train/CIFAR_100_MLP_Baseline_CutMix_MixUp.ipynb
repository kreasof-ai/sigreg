{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pBxX3I1kYwojaamntSxiNZTK",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pBxX3I1kYwojaamntSxiNZTK",
        "outputId": "0aec7ff4-f8be-431b-ea56-eb3d426cb98c",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training on device: cuda\n",
            "==> Preparing data with Strong Augmentation...\n",
            "Starting training for 400 epochs with RandAugment + Mixup/CutMix...\n",
            "Epoch 1 | T: 18s | Train: 4.1441 (1.2%) | Phys: 0.00 | Val: 4.5991 (2.35%) | Best: 2.35%\n",
            "Epoch 2 | T: 17s | Train: 4.1154 (2.2%) | Phys: 0.00 | Val: 4.3972 (3.51%) | Best: 3.51%\n",
            "Epoch 3 | T: 18s | Train: 3.9977 (3.4%) | Phys: 0.00 | Val: 4.1849 (5.72%) | Best: 5.72%\n",
            "Epoch 4 | T: 17s | Train: 3.9431 (4.3%) | Phys: 0.00 | Val: 4.0917 (7.39%) | Best: 7.39%\n",
            "Epoch 5 | T: 17s | Train: 3.9198 (4.7%) | Phys: 0.00 | Val: 3.9998 (8.38%) | Best: 8.38%\n",
            "Epoch 6 | T: 17s | Train: 3.8885 (5.5%) | Phys: 0.00 | Val: 3.9636 (9.37%) | Best: 9.37%\n",
            "Epoch 7 | T: 17s | Train: 3.8802 (5.5%) | Phys: 0.00 | Val: 3.9422 (9.89%) | Best: 9.89%\n",
            "Epoch 8 | T: 17s | Train: 3.8548 (5.9%) | Phys: 0.00 | Val: 3.8787 (11.02%) | Best: 11.02%\n",
            "Epoch 9 | T: 17s | Train: 3.8308 (6.6%) | Phys: 0.00 | Val: 3.8704 (10.45%) | Best: 11.02%\n",
            "Epoch 10 | T: 17s | Train: 3.8293 (6.6%) | Phys: 0.00 | Val: 3.8017 (12.26%) | Best: 12.26%\n",
            "Epoch 11 | T: 17s | Train: 3.8115 (6.9%) | Phys: 0.00 | Val: 3.7937 (12.35%) | Best: 12.35%\n",
            "Epoch 12 | T: 17s | Train: 3.7993 (7.1%) | Phys: 0.00 | Val: 3.7499 (13.18%) | Best: 13.18%\n",
            "Epoch 13 | T: 17s | Train: 3.7808 (7.5%) | Phys: 0.00 | Val: 3.7353 (13.99%) | Best: 13.99%\n",
            "Epoch 14 | T: 17s | Train: 3.7700 (7.8%) | Phys: 0.00 | Val: 3.7201 (13.70%) | Best: 13.99%\n",
            "Epoch 15 | T: 17s | Train: 3.7643 (7.9%) | Phys: 0.00 | Val: 3.7241 (14.37%) | Best: 14.37%\n",
            "Epoch 16 | T: 17s | Train: 3.7575 (8.0%) | Phys: 0.00 | Val: 3.6907 (14.29%) | Best: 14.37%\n",
            "Epoch 17 | T: 17s | Train: 3.7439 (8.3%) | Phys: 0.00 | Val: 3.6766 (15.01%) | Best: 15.01%\n",
            "Epoch 18 | T: 17s | Train: 3.7284 (8.6%) | Phys: 0.00 | Val: 3.6519 (15.30%) | Best: 15.30%\n",
            "Epoch 19 | T: 17s | Train: 3.7085 (8.8%) | Phys: 0.00 | Val: 3.6680 (15.37%) | Best: 15.37%\n",
            "Epoch 20 | T: 17s | Train: 3.7088 (9.0%) | Phys: 0.00 | Val: 3.6631 (15.55%) | Best: 15.55%\n",
            "Epoch 21 | T: 17s | Train: 3.7061 (9.2%) | Phys: 0.00 | Val: 3.6660 (15.68%) | Best: 15.68%\n",
            "Epoch 22 | T: 17s | Train: 3.7038 (9.1%) | Phys: 0.00 | Val: 3.6452 (16.41%) | Best: 16.41%\n",
            "Epoch 23 | T: 17s | Train: 3.6900 (9.4%) | Phys: 0.00 | Val: 3.6492 (15.77%) | Best: 16.41%\n",
            "Epoch 24 | T: 17s | Train: 3.7006 (9.2%) | Phys: 0.00 | Val: 3.5965 (16.63%) | Best: 16.63%\n",
            "Epoch 25 | T: 17s | Train: 3.6754 (9.5%) | Phys: 0.00 | Val: 3.5661 (16.66%) | Best: 16.66%\n",
            "Epoch 26 | T: 17s | Train: 3.6867 (9.5%) | Phys: 0.00 | Val: 3.6090 (16.08%) | Best: 16.66%\n",
            "Epoch 27 | T: 17s | Train: 3.6664 (9.7%) | Phys: 0.00 | Val: 3.5299 (17.46%) | Best: 17.46%\n",
            "Epoch 28 | T: 17s | Train: 3.6894 (9.5%) | Phys: 0.00 | Val: 3.5788 (16.95%) | Best: 17.46%\n",
            "Epoch 29 | T: 17s | Train: 3.6640 (9.8%) | Phys: 0.00 | Val: 3.5727 (17.88%) | Best: 17.88%\n",
            "Epoch 30 | T: 17s | Train: 3.6563 (10.0%) | Phys: 0.00 | Val: 3.5365 (18.20%) | Best: 18.20%\n",
            "Epoch 31 | T: 17s | Train: 3.6586 (10.3%) | Phys: 0.00 | Val: 3.5355 (18.46%) | Best: 18.46%\n",
            "Epoch 32 | T: 17s | Train: 3.6359 (10.5%) | Phys: 0.00 | Val: 3.5168 (19.03%) | Best: 19.03%\n",
            "Epoch 33 | T: 17s | Train: 3.6423 (10.4%) | Phys: 0.00 | Val: 3.5034 (17.82%) | Best: 19.03%\n",
            "Epoch 34 | T: 17s | Train: 3.6460 (10.3%) | Phys: 0.00 | Val: 3.5263 (18.41%) | Best: 19.03%\n",
            "Epoch 35 | T: 17s | Train: 3.6279 (10.7%) | Phys: 0.00 | Val: 3.4878 (18.92%) | Best: 19.03%\n",
            "Epoch 36 | T: 17s | Train: 3.6075 (10.9%) | Phys: 0.00 | Val: 3.4282 (19.51%) | Best: 19.51%\n",
            "Epoch 37 | T: 17s | Train: 3.6276 (10.8%) | Phys: 0.00 | Val: 3.5149 (19.01%) | Best: 19.51%\n",
            "Epoch 38 | T: 17s | Train: 3.6302 (10.7%) | Phys: 0.00 | Val: 3.4647 (18.96%) | Best: 19.51%\n",
            "Epoch 39 | T: 17s | Train: 3.6118 (11.1%) | Phys: 0.00 | Val: 3.5200 (18.44%) | Best: 19.51%\n",
            "Epoch 40 | T: 17s | Train: 3.6335 (10.5%) | Phys: 0.00 | Val: 3.4792 (19.58%) | Best: 19.58%\n",
            "Epoch 41 | T: 17s | Train: 3.6164 (10.8%) | Phys: 0.00 | Val: 3.5130 (18.98%) | Best: 19.58%\n",
            "Epoch 42 | T: 17s | Train: 3.6064 (11.1%) | Phys: 0.00 | Val: 3.4558 (19.39%) | Best: 19.58%\n",
            "Epoch 43 | T: 17s | Train: 3.6164 (10.9%) | Phys: 0.00 | Val: 3.4268 (20.67%) | Best: 20.67%\n",
            "Epoch 44 | T: 17s | Train: 3.6178 (11.1%) | Phys: 0.00 | Val: 3.4723 (19.31%) | Best: 20.67%\n",
            "Epoch 45 | T: 17s | Train: 3.6091 (11.2%) | Phys: 0.00 | Val: 3.4869 (19.62%) | Best: 20.67%\n",
            "Epoch 46 | T: 17s | Train: 3.6040 (11.1%) | Phys: 0.00 | Val: 3.4187 (20.72%) | Best: 20.72%\n",
            "Epoch 47 | T: 17s | Train: 3.6046 (11.4%) | Phys: 0.00 | Val: 3.4017 (21.87%) | Best: 21.87%\n",
            "Epoch 48 | T: 17s | Train: 3.5836 (11.5%) | Phys: 0.00 | Val: 3.3576 (21.60%) | Best: 21.87%\n",
            "Epoch 49 | T: 17s | Train: 3.5882 (11.6%) | Phys: 0.00 | Val: 3.4280 (20.06%) | Best: 21.87%\n",
            "Epoch 50 | T: 17s | Train: 3.5668 (12.0%) | Phys: 0.00 | Val: 3.3507 (21.89%) | Best: 21.89%\n",
            "Epoch 51 | T: 17s | Train: 3.5915 (11.5%) | Phys: 0.00 | Val: 3.3830 (21.03%) | Best: 21.89%\n",
            "Epoch 52 | T: 17s | Train: 3.5821 (11.7%) | Phys: 0.00 | Val: 3.4001 (21.06%) | Best: 21.89%\n",
            "Epoch 53 | T: 17s | Train: 3.5860 (11.5%) | Phys: 0.00 | Val: 3.4187 (20.74%) | Best: 21.89%\n",
            "Epoch 54 | T: 17s | Train: 3.5740 (11.8%) | Phys: 0.00 | Val: 3.3852 (20.98%) | Best: 21.89%\n",
            "Epoch 55 | T: 17s | Train: 3.5649 (11.8%) | Phys: 0.00 | Val: 3.4211 (20.93%) | Best: 21.89%\n",
            "Epoch 56 | T: 17s | Train: 3.5875 (11.5%) | Phys: 0.00 | Val: 3.3711 (22.05%) | Best: 22.05%\n",
            "Epoch 57 | T: 17s | Train: 3.5712 (11.6%) | Phys: 0.00 | Val: 3.3363 (21.72%) | Best: 22.05%\n",
            "Epoch 58 | T: 17s | Train: 3.5641 (12.1%) | Phys: 0.00 | Val: 3.3530 (22.23%) | Best: 22.23%\n",
            "Epoch 59 | T: 17s | Train: 3.5566 (12.2%) | Phys: 0.00 | Val: 3.3657 (22.55%) | Best: 22.55%\n",
            "Epoch 60 | T: 17s | Train: 3.5532 (12.3%) | Phys: 0.00 | Val: 3.3741 (22.35%) | Best: 22.55%\n",
            "Epoch 61 | T: 17s | Train: 3.5226 (12.8%) | Phys: 0.00 | Val: 3.3238 (22.95%) | Best: 22.95%\n",
            "Epoch 62 | T: 17s | Train: 3.5527 (12.2%) | Phys: 0.00 | Val: 3.3881 (21.62%) | Best: 22.95%\n",
            "Epoch 63 | T: 17s | Train: 3.5658 (12.1%) | Phys: 0.00 | Val: 3.3111 (22.83%) | Best: 22.95%\n",
            "Epoch 64 | T: 17s | Train: 3.5510 (12.2%) | Phys: 0.00 | Val: 3.2956 (23.25%) | Best: 23.25%\n",
            "Epoch 65 | T: 17s | Train: 3.5619 (12.0%) | Phys: 0.00 | Val: 3.3113 (22.65%) | Best: 23.25%\n",
            "Epoch 66 | T: 17s | Train: 3.5319 (12.7%) | Phys: 0.00 | Val: 3.3380 (22.25%) | Best: 23.25%\n",
            "Epoch 67 | T: 17s | Train: 3.5377 (12.7%) | Phys: 0.00 | Val: 3.2809 (23.04%) | Best: 23.25%\n",
            "Epoch 68 | T: 17s | Train: 3.5488 (12.4%) | Phys: 0.00 | Val: 3.3299 (23.06%) | Best: 23.25%\n",
            "Epoch 69 | T: 17s | Train: 3.5200 (12.8%) | Phys: 0.00 | Val: 3.3159 (22.57%) | Best: 23.25%\n",
            "Epoch 70 | T: 17s | Train: 3.5375 (12.7%) | Phys: 0.00 | Val: 3.2723 (23.78%) | Best: 23.78%\n",
            "Epoch 71 | T: 17s | Train: 3.5332 (12.9%) | Phys: 0.00 | Val: 3.2803 (24.34%) | Best: 24.34%\n",
            "Epoch 72 | T: 17s | Train: 3.5260 (12.8%) | Phys: 0.00 | Val: 3.2934 (23.83%) | Best: 24.34%\n",
            "Epoch 73 | T: 17s | Train: 3.5268 (12.9%) | Phys: 0.00 | Val: 3.3056 (23.08%) | Best: 24.34%\n",
            "Epoch 74 | T: 17s | Train: 3.5193 (13.1%) | Phys: 0.00 | Val: 3.2684 (23.81%) | Best: 24.34%\n",
            "Epoch 75 | T: 17s | Train: 3.4837 (13.7%) | Phys: 0.00 | Val: 3.2682 (24.38%) | Best: 24.38%\n",
            "Epoch 76 | T: 17s | Train: 3.4999 (13.4%) | Phys: 0.00 | Val: 3.2813 (24.35%) | Best: 24.38%\n",
            "Epoch 77 | T: 17s | Train: 3.5178 (13.2%) | Phys: 0.00 | Val: 3.2719 (24.02%) | Best: 24.38%\n",
            "Epoch 78 | T: 17s | Train: 3.5045 (13.4%) | Phys: 0.00 | Val: 3.2831 (24.22%) | Best: 24.38%\n",
            "Epoch 79 | T: 17s | Train: 3.4954 (13.5%) | Phys: 0.00 | Val: 3.2440 (24.91%) | Best: 24.91%\n",
            "Epoch 80 | T: 17s | Train: 3.5164 (13.1%) | Phys: 0.00 | Val: 3.2375 (24.83%) | Best: 24.91%\n",
            "Epoch 81 | T: 17s | Train: 3.4926 (13.4%) | Phys: 0.00 | Val: 3.2392 (24.65%) | Best: 24.91%\n",
            "Epoch 82 | T: 17s | Train: 3.5067 (13.4%) | Phys: 0.00 | Val: 3.1968 (24.51%) | Best: 24.91%\n",
            "Epoch 83 | T: 17s | Train: 3.4823 (13.8%) | Phys: 0.00 | Val: 3.2675 (24.74%) | Best: 24.91%\n",
            "Epoch 84 | T: 17s | Train: 3.5075 (13.4%) | Phys: 0.00 | Val: 3.2104 (25.28%) | Best: 25.28%\n",
            "Epoch 85 | T: 17s | Train: 3.5010 (13.4%) | Phys: 0.00 | Val: 3.1600 (25.74%) | Best: 25.74%\n",
            "Epoch 86 | T: 17s | Train: 3.4895 (13.8%) | Phys: 0.00 | Val: 3.2188 (25.28%) | Best: 25.74%\n",
            "Epoch 87 | T: 17s | Train: 3.4865 (13.8%) | Phys: 0.00 | Val: 3.1630 (25.38%) | Best: 25.74%\n",
            "Epoch 88 | T: 17s | Train: 3.4656 (14.2%) | Phys: 0.00 | Val: 3.1827 (25.59%) | Best: 25.74%\n",
            "Epoch 89 | T: 17s | Train: 3.4802 (13.9%) | Phys: 0.00 | Val: 3.2243 (24.91%) | Best: 25.74%\n",
            "Epoch 90 | T: 17s | Train: 3.4905 (13.7%) | Phys: 0.00 | Val: 3.2448 (25.13%) | Best: 25.74%\n",
            "Epoch 91 | T: 17s | Train: 3.4957 (13.8%) | Phys: 0.00 | Val: 3.2020 (25.38%) | Best: 25.74%\n",
            "Epoch 92 | T: 17s | Train: 3.4713 (14.0%) | Phys: 0.00 | Val: 3.1648 (25.70%) | Best: 25.74%\n",
            "Epoch 93 | T: 17s | Train: 3.4505 (14.5%) | Phys: 0.00 | Val: 3.1878 (25.41%) | Best: 25.74%\n",
            "Epoch 94 | T: 17s | Train: 3.4641 (14.1%) | Phys: 0.00 | Val: 3.1598 (25.92%) | Best: 25.92%\n",
            "Epoch 95 | T: 17s | Train: 3.4762 (13.9%) | Phys: 0.00 | Val: 3.1947 (25.54%) | Best: 25.92%\n",
            "Epoch 96 | T: 17s | Train: 3.4357 (14.7%) | Phys: 0.00 | Val: 3.0950 (25.90%) | Best: 25.92%\n",
            "Epoch 97 | T: 17s | Train: 3.4781 (13.9%) | Phys: 0.00 | Val: 3.1591 (25.84%) | Best: 25.92%\n",
            "Epoch 98 | T: 17s | Train: 3.4606 (14.2%) | Phys: 0.00 | Val: 3.1292 (26.65%) | Best: 26.65%\n",
            "Epoch 99 | T: 17s | Train: 3.4189 (15.4%) | Phys: 0.00 | Val: 3.1225 (26.61%) | Best: 26.65%\n",
            "Epoch 100 | T: 17s | Train: 3.4527 (14.5%) | Phys: 0.00 | Val: 3.1327 (26.23%) | Best: 26.65%\n",
            "Epoch 101 | T: 17s | Train: 3.4430 (14.9%) | Phys: 0.00 | Val: 3.1499 (26.49%) | Best: 26.65%\n",
            "Epoch 102 | T: 17s | Train: 3.4561 (14.5%) | Phys: 0.00 | Val: 3.1189 (26.48%) | Best: 26.65%\n",
            "Epoch 103 | T: 17s | Train: 3.4588 (14.5%) | Phys: 0.00 | Val: 3.1553 (26.41%) | Best: 26.65%\n",
            "Epoch 104 | T: 17s | Train: 3.4354 (14.9%) | Phys: 0.00 | Val: 3.1695 (26.28%) | Best: 26.65%\n",
            "Epoch 105 | T: 17s | Train: 3.4594 (14.6%) | Phys: 0.00 | Val: 3.1460 (27.27%) | Best: 27.27%\n",
            "Epoch 106 | T: 17s | Train: 3.4585 (14.4%) | Phys: 0.00 | Val: 3.1155 (27.36%) | Best: 27.36%\n",
            "Epoch 107 | T: 17s | Train: 3.4513 (14.8%) | Phys: 0.00 | Val: 3.1195 (26.64%) | Best: 27.36%\n",
            "Epoch 108 | T: 17s | Train: 3.4322 (15.0%) | Phys: 0.00 | Val: 3.1483 (26.20%) | Best: 27.36%\n",
            "Epoch 109 | T: 17s | Train: 3.4405 (14.7%) | Phys: 0.00 | Val: 3.0970 (26.88%) | Best: 27.36%\n",
            "Epoch 110 | T: 17s | Train: 3.4417 (14.9%) | Phys: 0.00 | Val: 3.1401 (26.98%) | Best: 27.36%\n",
            "Epoch 111 | T: 17s | Train: 3.4212 (15.3%) | Phys: 0.00 | Val: 3.1016 (27.42%) | Best: 27.42%\n",
            "Epoch 112 | T: 17s | Train: 3.4254 (15.2%) | Phys: 0.00 | Val: 3.0989 (26.79%) | Best: 27.42%\n",
            "Epoch 113 | T: 17s | Train: 3.4107 (15.3%) | Phys: 0.00 | Val: 3.1131 (27.13%) | Best: 27.42%\n",
            "Epoch 114 | T: 17s | Train: 3.4391 (14.9%) | Phys: 0.00 | Val: 3.1047 (27.85%) | Best: 27.85%\n",
            "Epoch 115 | T: 17s | Train: 3.4194 (15.4%) | Phys: 0.00 | Val: 3.1310 (27.69%) | Best: 27.85%\n",
            "Epoch 116 | T: 17s | Train: 3.4348 (14.9%) | Phys: 0.00 | Val: 3.1033 (27.80%) | Best: 27.85%\n",
            "Epoch 117 | T: 17s | Train: 3.4124 (15.5%) | Phys: 0.00 | Val: 3.1570 (26.90%) | Best: 27.85%\n",
            "Epoch 118 | T: 17s | Train: 3.4100 (15.4%) | Phys: 0.00 | Val: 3.0589 (27.47%) | Best: 27.85%\n",
            "Epoch 119 | T: 17s | Train: 3.4150 (15.5%) | Phys: 0.00 | Val: 3.1036 (27.34%) | Best: 27.85%\n",
            "Epoch 120 | T: 17s | Train: 3.4179 (15.4%) | Phys: 0.00 | Val: 3.0766 (28.07%) | Best: 28.07%\n",
            "Epoch 121 | T: 17s | Train: 3.4141 (15.5%) | Phys: 0.00 | Val: 3.0874 (27.63%) | Best: 28.07%\n",
            "Epoch 122 | T: 17s | Train: 3.4340 (15.1%) | Phys: 0.00 | Val: 3.1029 (27.55%) | Best: 28.07%\n",
            "Epoch 123 | T: 17s | Train: 3.3795 (16.1%) | Phys: 0.00 | Val: 3.0536 (27.68%) | Best: 28.07%\n",
            "Epoch 124 | T: 17s | Train: 3.4202 (15.4%) | Phys: 0.00 | Val: 3.0630 (28.48%) | Best: 28.48%\n",
            "Epoch 125 | T: 17s | Train: 3.3849 (15.9%) | Phys: 0.00 | Val: 3.0430 (28.33%) | Best: 28.48%\n",
            "Epoch 126 | T: 17s | Train: 3.3679 (16.2%) | Phys: 0.00 | Val: 3.0183 (28.95%) | Best: 28.95%\n",
            "Epoch 127 | T: 17s | Train: 3.3785 (16.1%) | Phys: 0.00 | Val: 3.0363 (28.25%) | Best: 28.95%\n",
            "Epoch 128 | T: 17s | Train: 3.4101 (15.6%) | Phys: 0.00 | Val: 3.0740 (27.80%) | Best: 28.95%\n",
            "Epoch 129 | T: 17s | Train: 3.4242 (15.4%) | Phys: 0.00 | Val: 3.0813 (27.87%) | Best: 28.95%\n",
            "Epoch 130 | T: 17s | Train: 3.3578 (16.6%) | Phys: 0.00 | Val: 3.0161 (28.33%) | Best: 28.95%\n",
            "Epoch 131 | T: 17s | Train: 3.4064 (15.7%) | Phys: 0.00 | Val: 3.0269 (28.69%) | Best: 28.95%\n",
            "Epoch 132 | T: 17s | Train: 3.3913 (15.9%) | Phys: 0.00 | Val: 3.0583 (27.92%) | Best: 28.95%\n",
            "Epoch 133 | T: 17s | Train: 3.3900 (16.0%) | Phys: 0.00 | Val: 3.0095 (29.18%) | Best: 29.18%\n",
            "Epoch 134 | T: 17s | Train: 3.4241 (15.3%) | Phys: 0.00 | Val: 3.0397 (28.65%) | Best: 29.18%\n",
            "Epoch 135 | T: 17s | Train: 3.3577 (16.6%) | Phys: 0.00 | Val: 3.0435 (28.50%) | Best: 29.18%\n",
            "Epoch 136 | T: 17s | Train: 3.3911 (15.8%) | Phys: 0.00 | Val: 3.0228 (28.80%) | Best: 29.18%\n",
            "Epoch 137 | T: 17s | Train: 3.3681 (16.4%) | Phys: 0.00 | Val: 3.0290 (28.77%) | Best: 29.18%\n",
            "Epoch 138 | T: 17s | Train: 3.4236 (15.2%) | Phys: 0.00 | Val: 3.0470 (28.84%) | Best: 29.18%\n",
            "Epoch 139 | T: 17s | Train: 3.3794 (16.1%) | Phys: 0.00 | Val: 2.9791 (29.39%) | Best: 29.39%\n",
            "Epoch 140 | T: 17s | Train: 3.4056 (15.7%) | Phys: 0.00 | Val: 3.0281 (29.41%) | Best: 29.41%\n",
            "Epoch 141 | T: 17s | Train: 3.3441 (16.9%) | Phys: 0.00 | Val: 3.0077 (28.61%) | Best: 29.41%\n",
            "Epoch 142 | T: 17s | Train: 3.3767 (16.3%) | Phys: 0.00 | Val: 2.9928 (29.62%) | Best: 29.62%\n",
            "Epoch 143 | T: 17s | Train: 3.3612 (16.6%) | Phys: 0.00 | Val: 3.0331 (28.91%) | Best: 29.62%\n",
            "Epoch 144 | T: 17s | Train: 3.3720 (16.3%) | Phys: 0.00 | Val: 3.0127 (29.38%) | Best: 29.62%\n",
            "Epoch 145 | T: 17s | Train: 3.3306 (17.3%) | Phys: 0.00 | Val: 3.0030 (29.25%) | Best: 29.62%\n",
            "Epoch 146 | T: 17s | Train: 3.3545 (16.8%) | Phys: 0.00 | Val: 2.9713 (29.95%) | Best: 29.95%\n",
            "Epoch 147 | T: 17s | Train: 3.3475 (17.0%) | Phys: 0.00 | Val: 3.0251 (29.52%) | Best: 29.95%\n",
            "Epoch 148 | T: 17s | Train: 3.3524 (16.6%) | Phys: 0.00 | Val: 3.0115 (29.19%) | Best: 29.95%\n",
            "Epoch 149 | T: 17s | Train: 3.3490 (17.0%) | Phys: 0.00 | Val: 3.0008 (29.38%) | Best: 29.95%\n",
            "Epoch 150 | T: 17s | Train: 3.3737 (16.3%) | Phys: 0.00 | Val: 3.0042 (29.10%) | Best: 29.95%\n",
            "Epoch 151 | T: 17s | Train: 3.3419 (17.1%) | Phys: 0.00 | Val: 2.9987 (29.87%) | Best: 29.95%\n",
            "Epoch 152 | T: 17s | Train: 3.3811 (16.2%) | Phys: 0.00 | Val: 2.9892 (29.45%) | Best: 29.95%\n",
            "Epoch 153 | T: 17s | Train: 3.3592 (16.7%) | Phys: 0.00 | Val: 3.0342 (29.07%) | Best: 29.95%\n",
            "Epoch 154 | T: 17s | Train: 3.3577 (16.7%) | Phys: 0.00 | Val: 2.9918 (29.54%) | Best: 29.95%\n",
            "Epoch 155 | T: 17s | Train: 3.3574 (16.6%) | Phys: 0.00 | Val: 3.0209 (29.29%) | Best: 29.95%\n",
            "Epoch 156 | T: 17s | Train: 3.3606 (16.9%) | Phys: 0.00 | Val: 3.0035 (29.33%) | Best: 29.95%\n",
            "Epoch 157 | T: 17s | Train: 3.4034 (15.8%) | Phys: 0.00 | Val: 3.0218 (29.93%) | Best: 29.95%\n",
            "Epoch 158 | T: 17s | Train: 3.3299 (17.4%) | Phys: 0.00 | Val: 2.9110 (30.39%) | Best: 30.39%\n",
            "Epoch 159 | T: 17s | Train: 3.3370 (17.2%) | Phys: 0.00 | Val: 2.9953 (29.90%) | Best: 30.39%\n",
            "Epoch 160 | T: 17s | Train: 3.3021 (18.1%) | Phys: 0.00 | Val: 2.9802 (29.94%) | Best: 30.39%\n",
            "Epoch 161 | T: 17s | Train: 3.3114 (17.9%) | Phys: 0.00 | Val: 2.9621 (30.74%) | Best: 30.74%\n",
            "Epoch 162 | T: 17s | Train: 3.3791 (16.5%) | Phys: 0.00 | Val: 2.9970 (29.95%) | Best: 30.74%\n",
            "Epoch 163 | T: 17s | Train: 3.3242 (17.4%) | Phys: 0.00 | Val: 2.9331 (30.67%) | Best: 30.74%\n",
            "Epoch 164 | T: 17s | Train: 3.3677 (16.8%) | Phys: 0.00 | Val: 2.9843 (29.31%) | Best: 30.74%\n",
            "Epoch 165 | T: 17s | Train: 3.3163 (17.7%) | Phys: 0.00 | Val: 2.9326 (31.54%) | Best: 31.54%\n",
            "Epoch 166 | T: 17s | Train: 3.3127 (17.7%) | Phys: 0.00 | Val: 2.9419 (30.92%) | Best: 31.54%\n",
            "Epoch 167 | T: 17s | Train: 3.3729 (16.4%) | Phys: 0.00 | Val: 2.9145 (31.50%) | Best: 31.54%\n",
            "Epoch 168 | T: 17s | Train: 3.3316 (17.5%) | Phys: 0.00 | Val: 2.9153 (31.50%) | Best: 31.54%\n",
            "Epoch 169 | T: 17s | Train: 3.3194 (17.8%) | Phys: 0.00 | Val: 2.9674 (30.20%) | Best: 31.54%\n",
            "Epoch 170 | T: 17s | Train: 3.3116 (17.7%) | Phys: 0.00 | Val: 2.9245 (30.62%) | Best: 31.54%\n",
            "Epoch 171 | T: 17s | Train: 3.3251 (17.5%) | Phys: 0.00 | Val: 2.9340 (31.12%) | Best: 31.54%\n",
            "Epoch 172 | T: 17s | Train: 3.3329 (17.5%) | Phys: 0.00 | Val: 2.9177 (30.81%) | Best: 31.54%\n",
            "Epoch 173 | T: 17s | Train: 3.3355 (17.2%) | Phys: 0.00 | Val: 2.9286 (31.30%) | Best: 31.54%\n",
            "Epoch 174 | T: 17s | Train: 3.3258 (17.4%) | Phys: 0.00 | Val: 2.9333 (31.41%) | Best: 31.54%\n",
            "Epoch 175 | T: 17s | Train: 3.3147 (17.8%) | Phys: 0.00 | Val: 2.9049 (31.69%) | Best: 31.69%\n",
            "Epoch 176 | T: 17s | Train: 3.3385 (17.2%) | Phys: 0.00 | Val: 2.9565 (30.98%) | Best: 31.69%\n",
            "Epoch 177 | T: 17s | Train: 3.3190 (17.5%) | Phys: 0.00 | Val: 2.9264 (31.06%) | Best: 31.69%\n",
            "Epoch 178 | T: 17s | Train: 3.3363 (17.2%) | Phys: 0.00 | Val: 2.9027 (31.68%) | Best: 31.69%\n",
            "Epoch 179 | T: 17s | Train: 3.2934 (18.2%) | Phys: 0.00 | Val: 2.8814 (31.75%) | Best: 31.75%\n",
            "Epoch 180 | T: 17s | Train: 3.2884 (18.2%) | Phys: 0.00 | Val: 2.8823 (31.35%) | Best: 31.75%\n",
            "Epoch 181 | T: 17s | Train: 3.3090 (18.1%) | Phys: 0.00 | Val: 2.8917 (31.29%) | Best: 31.75%\n",
            "Epoch 182 | T: 17s | Train: 3.3255 (17.4%) | Phys: 0.00 | Val: 2.9706 (30.72%) | Best: 31.75%\n",
            "Epoch 183 | T: 17s | Train: 3.3121 (17.8%) | Phys: 0.00 | Val: 2.9192 (31.29%) | Best: 31.75%\n",
            "Epoch 184 | T: 17s | Train: 3.3103 (17.9%) | Phys: 0.00 | Val: 2.9090 (31.47%) | Best: 31.75%\n",
            "Epoch 185 | T: 17s | Train: 3.3189 (17.8%) | Phys: 0.00 | Val: 2.9032 (31.58%) | Best: 31.75%\n",
            "Epoch 186 | T: 17s | Train: 3.3349 (17.4%) | Phys: 0.00 | Val: 2.9515 (30.51%) | Best: 31.75%\n",
            "Epoch 187 | T: 17s | Train: 3.3192 (17.9%) | Phys: 0.00 | Val: 2.9209 (31.74%) | Best: 31.75%\n",
            "Epoch 188 | T: 17s | Train: 3.3023 (18.2%) | Phys: 0.00 | Val: 2.9372 (31.07%) | Best: 31.75%\n",
            "Epoch 189 | T: 17s | Train: 3.3172 (17.9%) | Phys: 0.00 | Val: 2.9506 (30.84%) | Best: 31.75%\n",
            "Epoch 190 | T: 17s | Train: 3.2784 (18.7%) | Phys: 0.00 | Val: 2.8629 (32.18%) | Best: 32.18%\n",
            "Epoch 191 | T: 17s | Train: 3.2957 (18.4%) | Phys: 0.00 | Val: 2.9357 (31.00%) | Best: 32.18%\n",
            "Epoch 192 | T: 17s | Train: 3.2610 (18.8%) | Phys: 0.00 | Val: 2.8945 (31.42%) | Best: 32.18%\n",
            "Epoch 193 | T: 17s | Train: 3.2700 (18.8%) | Phys: 0.00 | Val: 2.9216 (31.26%) | Best: 32.18%\n",
            "Epoch 194 | T: 17s | Train: 3.2560 (19.4%) | Phys: 0.00 | Val: 2.9093 (31.46%) | Best: 32.18%\n",
            "Epoch 195 | T: 17s | Train: 3.3049 (18.0%) | Phys: 0.00 | Val: 2.9103 (30.81%) | Best: 32.18%\n",
            "Epoch 196 | T: 17s | Train: 3.2733 (18.8%) | Phys: 0.00 | Val: 2.8436 (32.65%) | Best: 32.65%\n",
            "Epoch 197 | T: 17s | Train: 3.2770 (18.6%) | Phys: 0.00 | Val: 2.8540 (32.77%) | Best: 32.77%\n",
            "Epoch 198 | T: 17s | Train: 3.3481 (17.1%) | Phys: 0.00 | Val: 2.8812 (32.73%) | Best: 32.77%\n",
            "Epoch 199 | T: 17s | Train: 3.2463 (19.5%) | Phys: 0.00 | Val: 2.8513 (32.60%) | Best: 32.77%\n",
            "Epoch 200 | T: 17s | Train: 3.2618 (19.0%) | Phys: 0.00 | Val: 2.8796 (31.83%) | Best: 32.77%\n",
            "Epoch 201 | T: 17s | Train: 3.2896 (18.6%) | Phys: 0.00 | Val: 2.9184 (31.60%) | Best: 32.77%\n",
            "Epoch 202 | T: 17s | Train: 3.2602 (19.3%) | Phys: 0.00 | Val: 2.9416 (31.17%) | Best: 32.77%\n",
            "Epoch 203 | T: 17s | Train: 3.2526 (19.3%) | Phys: 0.00 | Val: 2.8333 (32.78%) | Best: 32.78%\n",
            "Epoch 204 | T: 17s | Train: 3.2760 (18.6%) | Phys: 0.00 | Val: 2.9241 (31.65%) | Best: 32.78%\n",
            "Epoch 205 | T: 17s | Train: 3.2886 (18.5%) | Phys: 0.00 | Val: 2.8429 (32.35%) | Best: 32.78%\n",
            "Epoch 206 | T: 17s | Train: 3.3137 (17.9%) | Phys: 0.00 | Val: 2.8765 (32.20%) | Best: 32.78%\n",
            "Epoch 207 | T: 17s | Train: 3.3096 (18.1%) | Phys: 0.00 | Val: 2.8937 (32.39%) | Best: 32.78%\n",
            "Epoch 208 | T: 17s | Train: 3.2742 (18.9%) | Phys: 0.00 | Val: 2.8280 (32.90%) | Best: 32.90%\n",
            "Epoch 209 | T: 17s | Train: 3.2476 (19.5%) | Phys: 0.00 | Val: 2.8182 (32.82%) | Best: 32.90%\n",
            "Epoch 210 | T: 17s | Train: 3.2545 (19.1%) | Phys: 0.00 | Val: 2.8405 (33.17%) | Best: 33.17%\n",
            "Epoch 211 | T: 17s | Train: 3.2868 (18.6%) | Phys: 0.00 | Val: 2.8704 (32.55%) | Best: 33.17%\n",
            "Epoch 212 | T: 17s | Train: 3.2759 (19.0%) | Phys: 0.00 | Val: 2.8252 (33.00%) | Best: 33.17%\n",
            "Epoch 213 | T: 17s | Train: 3.2755 (18.8%) | Phys: 0.00 | Val: 2.8389 (32.31%) | Best: 33.17%\n",
            "Epoch 214 | T: 17s | Train: 3.2670 (19.1%) | Phys: 0.00 | Val: 2.8389 (33.02%) | Best: 33.17%\n",
            "Epoch 215 | T: 17s | Train: 3.2089 (20.3%) | Phys: 0.00 | Val: 2.7797 (32.98%) | Best: 33.17%\n",
            "Epoch 216 | T: 17s | Train: 3.2871 (18.7%) | Phys: 0.00 | Val: 2.8009 (33.62%) | Best: 33.62%\n",
            "Epoch 217 | T: 17s | Train: 3.1984 (20.5%) | Phys: 0.00 | Val: 2.7954 (33.35%) | Best: 33.62%\n",
            "Epoch 218 | T: 17s | Train: 3.2526 (19.2%) | Phys: 0.00 | Val: 2.8304 (33.45%) | Best: 33.62%\n",
            "Epoch 219 | T: 17s | Train: 3.2310 (19.7%) | Phys: 0.00 | Val: 2.8037 (33.23%) | Best: 33.62%\n",
            "Epoch 220 | T: 17s | Train: 3.2340 (19.8%) | Phys: 0.00 | Val: 2.8162 (33.20%) | Best: 33.62%\n",
            "Epoch 221 | T: 17s | Train: 3.2441 (19.5%) | Phys: 0.00 | Val: 2.8049 (33.41%) | Best: 33.62%\n",
            "Epoch 222 | T: 17s | Train: 3.2517 (19.5%) | Phys: 0.00 | Val: 2.8847 (32.78%) | Best: 33.62%\n",
            "Epoch 223 | T: 17s | Train: 3.2161 (20.1%) | Phys: 0.00 | Val: 2.8286 (33.33%) | Best: 33.62%\n",
            "Epoch 224 | T: 17s | Train: 3.2208 (20.3%) | Phys: 0.00 | Val: 2.8011 (33.68%) | Best: 33.68%\n",
            "Epoch 225 | T: 17s | Train: 3.2596 (19.3%) | Phys: 0.00 | Val: 2.8377 (33.10%) | Best: 33.68%\n",
            "Epoch 226 | T: 17s | Train: 3.2375 (19.7%) | Phys: 0.00 | Val: 2.9070 (32.24%) | Best: 33.68%\n",
            "Epoch 227 | T: 17s | Train: 3.2384 (19.7%) | Phys: 0.00 | Val: 2.8591 (32.99%) | Best: 33.68%\n",
            "Epoch 228 | T: 17s | Train: 3.2360 (19.9%) | Phys: 0.00 | Val: 2.8165 (33.42%) | Best: 33.68%\n",
            "Epoch 229 | T: 17s | Train: 3.2418 (19.6%) | Phys: 0.00 | Val: 2.8468 (32.96%) | Best: 33.68%\n",
            "Epoch 230 | T: 17s | Train: 3.2458 (19.5%) | Phys: 0.00 | Val: 2.8293 (32.94%) | Best: 33.68%\n",
            "Epoch 231 | T: 17s | Train: 3.2327 (19.8%) | Phys: 0.00 | Val: 2.7947 (33.33%) | Best: 33.68%\n",
            "Epoch 232 | T: 17s | Train: 3.2494 (19.4%) | Phys: 0.00 | Val: 2.8212 (33.10%) | Best: 33.68%\n",
            "Epoch 233 | T: 17s | Train: 3.2480 (19.4%) | Phys: 0.00 | Val: 2.8337 (33.44%) | Best: 33.68%\n",
            "Epoch 234 | T: 17s | Train: 3.2325 (20.1%) | Phys: 0.00 | Val: 2.8078 (33.26%) | Best: 33.68%\n",
            "Epoch 235 | T: 17s | Train: 3.2494 (19.4%) | Phys: 0.00 | Val: 2.7915 (33.64%) | Best: 33.68%\n",
            "Epoch 236 | T: 17s | Train: 3.2121 (20.2%) | Phys: 0.00 | Val: 2.8159 (33.90%) | Best: 33.90%\n",
            "Epoch 237 | T: 17s | Train: 3.2143 (20.2%) | Phys: 0.00 | Val: 2.8176 (33.39%) | Best: 33.90%\n",
            "Epoch 238 | T: 17s | Train: 3.1692 (21.4%) | Phys: 0.00 | Val: 2.8045 (33.50%) | Best: 33.90%\n",
            "Epoch 239 | T: 17s | Train: 3.2534 (19.6%) | Phys: 0.00 | Val: 2.8056 (33.61%) | Best: 33.90%\n",
            "Epoch 240 | T: 17s | Train: 3.2046 (20.6%) | Phys: 0.00 | Val: 2.7397 (34.41%) | Best: 34.41%\n",
            "Epoch 241 | T: 17s | Train: 3.2232 (20.1%) | Phys: 0.00 | Val: 2.8256 (33.93%) | Best: 34.41%\n",
            "Epoch 242 | T: 17s | Train: 3.2059 (20.4%) | Phys: 0.00 | Val: 2.8162 (33.79%) | Best: 34.41%\n",
            "Epoch 243 | T: 17s | Train: 3.1879 (20.8%) | Phys: 0.00 | Val: 2.7754 (34.34%) | Best: 34.41%\n",
            "Epoch 244 | T: 17s | Train: 3.2405 (19.7%) | Phys: 0.00 | Val: 2.8365 (33.40%) | Best: 34.41%\n",
            "Epoch 245 | T: 17s | Train: 3.2296 (20.2%) | Phys: 0.00 | Val: 2.8020 (34.02%) | Best: 34.41%\n",
            "Epoch 246 | T: 17s | Train: 3.1900 (20.9%) | Phys: 0.00 | Val: 2.7519 (33.93%) | Best: 34.41%\n",
            "Epoch 247 | T: 17s | Train: 3.2125 (20.5%) | Phys: 0.00 | Val: 2.8172 (34.24%) | Best: 34.41%\n",
            "Epoch 248 | T: 17s | Train: 3.1989 (20.6%) | Phys: 0.00 | Val: 2.7905 (34.18%) | Best: 34.41%\n",
            "Epoch 249 | T: 17s | Train: 3.1844 (20.9%) | Phys: 0.00 | Val: 2.8337 (34.22%) | Best: 34.41%\n",
            "Epoch 250 | T: 17s | Train: 3.2288 (20.2%) | Phys: 0.00 | Val: 2.8475 (33.45%) | Best: 34.41%\n",
            "Epoch 251 | T: 17s | Train: 3.2115 (20.4%) | Phys: 0.00 | Val: 2.8523 (33.88%) | Best: 34.41%\n",
            "Epoch 252 | T: 17s | Train: 3.2139 (20.5%) | Phys: 0.00 | Val: 2.7641 (33.82%) | Best: 34.41%\n",
            "Epoch 253 | T: 17s | Train: 3.1930 (20.8%) | Phys: 0.00 | Val: 2.8046 (34.29%) | Best: 34.41%\n",
            "Epoch 254 | T: 17s | Train: 3.1781 (21.1%) | Phys: 0.00 | Val: 2.7734 (34.06%) | Best: 34.41%\n",
            "Epoch 255 | T: 17s | Train: 3.1851 (20.9%) | Phys: 0.00 | Val: 2.7905 (34.60%) | Best: 34.60%\n",
            "Epoch 256 | T: 17s | Train: 3.2018 (20.8%) | Phys: 0.00 | Val: 2.8167 (33.77%) | Best: 34.60%\n",
            "Epoch 257 | T: 17s | Train: 3.1956 (20.8%) | Phys: 0.00 | Val: 2.7892 (34.04%) | Best: 34.60%\n",
            "Epoch 258 | T: 17s | Train: 3.1692 (21.5%) | Phys: 0.00 | Val: 2.7498 (34.88%) | Best: 34.88%\n",
            "Epoch 259 | T: 17s | Train: 3.1524 (21.9%) | Phys: 0.00 | Val: 2.7631 (34.45%) | Best: 34.88%\n",
            "Epoch 260 | T: 17s | Train: 3.1534 (21.7%) | Phys: 0.00 | Val: 2.7545 (34.74%) | Best: 34.88%\n",
            "Epoch 261 | T: 17s | Train: 3.2094 (20.5%) | Phys: 0.00 | Val: 2.7662 (34.19%) | Best: 34.88%\n",
            "Epoch 262 | T: 17s | Train: 3.1782 (21.3%) | Phys: 0.00 | Val: 2.7505 (34.55%) | Best: 34.88%\n",
            "Epoch 263 | T: 17s | Train: 3.1773 (21.3%) | Phys: 0.00 | Val: 2.7464 (35.24%) | Best: 35.24%\n",
            "Epoch 264 | T: 17s | Train: 3.1658 (21.5%) | Phys: 0.00 | Val: 2.7745 (34.15%) | Best: 35.24%\n",
            "Epoch 265 | T: 17s | Train: 3.2031 (20.5%) | Phys: 0.00 | Val: 2.8082 (34.59%) | Best: 35.24%\n",
            "Epoch 266 | T: 17s | Train: 3.1784 (21.1%) | Phys: 0.00 | Val: 2.7200 (35.22%) | Best: 35.24%\n",
            "Epoch 267 | T: 17s | Train: 3.1557 (21.5%) | Phys: 0.00 | Val: 2.7230 (34.84%) | Best: 35.24%\n",
            "Epoch 268 | T: 17s | Train: 3.2209 (20.4%) | Phys: 0.00 | Val: 2.7506 (34.68%) | Best: 35.24%\n",
            "Epoch 269 | T: 17s | Train: 3.1734 (21.2%) | Phys: 0.00 | Val: 2.7778 (34.43%) | Best: 35.24%\n",
            "Epoch 270 | T: 17s | Train: 3.2085 (20.8%) | Phys: 0.00 | Val: 2.7682 (34.73%) | Best: 35.24%\n",
            "Epoch 271 | T: 17s | Train: 3.2018 (20.9%) | Phys: 0.00 | Val: 2.7488 (35.31%) | Best: 35.31%\n",
            "Epoch 272 | T: 17s | Train: 3.1658 (21.6%) | Phys: 0.00 | Val: 2.7511 (34.82%) | Best: 35.31%\n",
            "Epoch 273 | T: 17s | Train: 3.1899 (20.9%) | Phys: 0.00 | Val: 2.8139 (34.22%) | Best: 35.31%\n",
            "Epoch 274 | T: 17s | Train: 3.1892 (21.1%) | Phys: 0.00 | Val: 2.7324 (34.63%) | Best: 35.31%\n",
            "Epoch 275 | T: 17s | Train: 3.2378 (20.3%) | Phys: 0.00 | Val: 2.7303 (35.37%) | Best: 35.37%\n",
            "Epoch 276 | T: 17s | Train: 3.1893 (21.1%) | Phys: 0.00 | Val: 2.7661 (34.87%) | Best: 35.37%\n",
            "Epoch 277 | T: 17s | Train: 3.1927 (21.2%) | Phys: 0.00 | Val: 2.7694 (34.44%) | Best: 35.37%\n",
            "Epoch 278 | T: 17s | Train: 3.1821 (21.6%) | Phys: 0.00 | Val: 2.7661 (34.52%) | Best: 35.37%\n",
            "Epoch 279 | T: 17s | Train: 3.1830 (21.4%) | Phys: 0.00 | Val: 2.7532 (35.15%) | Best: 35.37%\n",
            "Epoch 280 | T: 17s | Train: 3.1566 (21.8%) | Phys: 0.00 | Val: 2.7409 (35.25%) | Best: 35.37%\n",
            "Epoch 281 | T: 17s | Train: 3.1789 (21.4%) | Phys: 0.00 | Val: 2.7631 (34.81%) | Best: 35.37%\n",
            "Epoch 282 | T: 17s | Train: 3.1948 (21.1%) | Phys: 0.00 | Val: 2.8324 (34.35%) | Best: 35.37%\n",
            "Epoch 283 | T: 17s | Train: 3.1642 (21.7%) | Phys: 0.00 | Val: 2.7095 (35.19%) | Best: 35.37%\n",
            "Epoch 284 | T: 17s | Train: 3.1939 (21.1%) | Phys: 0.00 | Val: 2.7417 (35.23%) | Best: 35.37%\n",
            "Epoch 285 | T: 17s | Train: 3.1742 (21.5%) | Phys: 0.00 | Val: 2.7206 (35.21%) | Best: 35.37%\n",
            "Epoch 286 | T: 17s | Train: 3.1686 (21.4%) | Phys: 0.00 | Val: 2.7294 (35.55%) | Best: 35.55%\n",
            "Epoch 287 | T: 17s | Train: 3.1758 (21.4%) | Phys: 0.00 | Val: 2.7154 (35.84%) | Best: 35.84%\n",
            "Epoch 288 | T: 17s | Train: 3.1590 (21.9%) | Phys: 0.00 | Val: 2.7322 (35.67%) | Best: 35.84%\n",
            "Epoch 289 | T: 17s | Train: 3.1679 (21.7%) | Phys: 0.00 | Val: 2.7203 (36.03%) | Best: 36.03%\n",
            "Epoch 290 | T: 17s | Train: 3.2094 (20.7%) | Phys: 0.00 | Val: 2.7173 (35.81%) | Best: 36.03%\n",
            "Epoch 291 | T: 17s | Train: 3.1430 (22.1%) | Phys: 0.00 | Val: 2.6934 (35.77%) | Best: 36.03%\n",
            "Epoch 292 | T: 17s | Train: 3.1876 (21.1%) | Phys: 0.00 | Val: 2.7738 (35.21%) | Best: 36.03%\n",
            "Epoch 293 | T: 17s | Train: 3.1658 (21.8%) | Phys: 0.00 | Val: 2.7319 (35.33%) | Best: 36.03%\n",
            "Epoch 294 | T: 17s | Train: 3.1414 (22.4%) | Phys: 0.00 | Val: 2.7450 (35.16%) | Best: 36.03%\n",
            "Epoch 295 | T: 17s | Train: 3.1517 (21.9%) | Phys: 0.00 | Val: 2.6934 (36.07%) | Best: 36.07%\n",
            "Epoch 296 | T: 17s | Train: 3.1759 (21.4%) | Phys: 0.00 | Val: 2.7126 (36.04%) | Best: 36.07%\n",
            "Epoch 297 | T: 17s | Train: 3.1310 (22.2%) | Phys: 0.00 | Val: 2.7161 (36.19%) | Best: 36.19%\n",
            "Epoch 298 | T: 17s | Train: 3.1564 (22.1%) | Phys: 0.00 | Val: 2.6987 (36.18%) | Best: 36.19%\n",
            "Epoch 299 | T: 17s | Train: 3.1654 (21.7%) | Phys: 0.00 | Val: 2.6895 (35.58%) | Best: 36.19%\n",
            "Epoch 300 | T: 17s | Train: 3.1419 (22.3%) | Phys: 0.00 | Val: 2.7018 (36.51%) | Best: 36.51%\n",
            "Epoch 301 | T: 17s | Train: 3.1387 (22.2%) | Phys: 0.00 | Val: 2.7135 (36.55%) | Best: 36.55%\n",
            "Epoch 302 | T: 17s | Train: 3.1207 (22.8%) | Phys: 0.00 | Val: 2.7013 (36.37%) | Best: 36.55%\n",
            "Epoch 303 | T: 17s | Train: 3.1791 (21.6%) | Phys: 0.00 | Val: 2.7196 (36.19%) | Best: 36.55%\n",
            "Epoch 304 | T: 17s | Train: 3.1365 (22.3%) | Phys: 0.00 | Val: 2.7450 (35.47%) | Best: 36.55%\n",
            "Epoch 305 | T: 17s | Train: 3.1346 (22.5%) | Phys: 0.00 | Val: 2.7409 (35.63%) | Best: 36.55%\n",
            "Epoch 306 | T: 17s | Train: 3.1742 (21.4%) | Phys: 0.00 | Val: 2.7555 (35.23%) | Best: 36.55%\n",
            "Epoch 307 | T: 17s | Train: 3.1773 (21.6%) | Phys: 0.00 | Val: 2.7417 (35.30%) | Best: 36.55%\n",
            "Epoch 308 | T: 17s | Train: 3.1306 (22.6%) | Phys: 0.00 | Val: 2.7545 (35.44%) | Best: 36.55%\n",
            "Epoch 309 | T: 17s | Train: 3.1545 (22.1%) | Phys: 0.00 | Val: 2.7014 (36.20%) | Best: 36.55%\n",
            "Epoch 310 | T: 17s | Train: 3.1866 (21.5%) | Phys: 0.00 | Val: 2.7779 (35.08%) | Best: 36.55%\n",
            "Epoch 311 | T: 17s | Train: 3.1847 (21.5%) | Phys: 0.00 | Val: 2.7508 (35.29%) | Best: 36.55%\n",
            "Epoch 312 | T: 17s | Train: 3.1380 (22.5%) | Phys: 0.00 | Val: 2.6942 (36.47%) | Best: 36.55%\n",
            "Epoch 313 | T: 17s | Train: 3.1387 (22.7%) | Phys: 0.00 | Val: 2.7114 (35.86%) | Best: 36.55%\n",
            "Epoch 314 | T: 17s | Train: 3.1489 (22.4%) | Phys: 0.00 | Val: 2.7497 (35.19%) | Best: 36.55%\n",
            "Epoch 315 | T: 17s | Train: 3.1274 (22.8%) | Phys: 0.00 | Val: 2.6529 (36.55%) | Best: 36.55%\n",
            "Epoch 316 | T: 17s | Train: 3.1411 (22.5%) | Phys: 0.00 | Val: 2.7359 (35.79%) | Best: 36.55%\n",
            "Epoch 317 | T: 17s | Train: 3.1656 (21.9%) | Phys: 0.00 | Val: 2.7347 (35.93%) | Best: 36.55%\n",
            "Epoch 318 | T: 17s | Train: 3.1584 (21.9%) | Phys: 0.00 | Val: 2.6561 (36.59%) | Best: 36.59%\n",
            "Epoch 319 | T: 17s | Train: 3.1745 (21.6%) | Phys: 0.00 | Val: 2.7228 (35.83%) | Best: 36.59%\n",
            "Epoch 320 | T: 17s | Train: 3.1101 (23.1%) | Phys: 0.00 | Val: 2.7183 (35.56%) | Best: 36.59%\n",
            "Epoch 321 | T: 17s | Train: 3.1382 (22.5%) | Phys: 0.00 | Val: 2.7512 (35.83%) | Best: 36.59%\n",
            "Epoch 322 | T: 17s | Train: 3.1209 (22.9%) | Phys: 0.00 | Val: 2.6733 (36.77%) | Best: 36.77%\n",
            "Epoch 323 | T: 17s | Train: 3.1509 (22.6%) | Phys: 0.00 | Val: 2.6181 (36.88%) | Best: 36.88%\n",
            "Epoch 324 | T: 17s | Train: 3.1195 (22.8%) | Phys: 0.00 | Val: 2.7038 (36.08%) | Best: 36.88%\n",
            "Epoch 325 | T: 17s | Train: 3.1227 (22.8%) | Phys: 0.00 | Val: 2.6997 (36.07%) | Best: 36.88%\n",
            "Epoch 326 | T: 17s | Train: 3.1148 (23.1%) | Phys: 0.00 | Val: 2.7031 (36.42%) | Best: 36.88%\n",
            "Epoch 327 | T: 17s | Train: 3.1251 (22.9%) | Phys: 0.00 | Val: 2.6567 (36.61%) | Best: 36.88%\n",
            "Epoch 328 | T: 17s | Train: 3.1288 (22.8%) | Phys: 0.00 | Val: 2.6723 (36.88%) | Best: 36.88%\n",
            "Epoch 329 | T: 17s | Train: 3.1062 (23.3%) | Phys: 0.00 | Val: 2.7148 (36.45%) | Best: 36.88%\n",
            "Epoch 330 | T: 17s | Train: 3.1380 (22.8%) | Phys: 0.00 | Val: 2.6833 (36.70%) | Best: 36.88%\n",
            "Epoch 331 | T: 17s | Train: 3.0708 (23.9%) | Phys: 0.00 | Val: 2.6593 (36.84%) | Best: 36.88%\n",
            "Epoch 332 | T: 17s | Train: 3.1207 (23.0%) | Phys: 0.00 | Val: 2.7598 (35.29%) | Best: 36.88%\n",
            "Epoch 333 | T: 17s | Train: 3.0870 (23.8%) | Phys: 0.00 | Val: 2.7040 (35.80%) | Best: 36.88%\n",
            "Epoch 334 | T: 17s | Train: 3.1237 (22.8%) | Phys: 0.00 | Val: 2.6497 (36.73%) | Best: 36.88%\n",
            "Epoch 335 | T: 17s | Train: 3.1209 (23.1%) | Phys: 0.00 | Val: 2.6498 (37.01%) | Best: 37.01%\n",
            "Epoch 336 | T: 17s | Train: 3.1173 (23.1%) | Phys: 0.00 | Val: 2.6552 (36.55%) | Best: 37.01%\n",
            "Epoch 337 | T: 17s | Train: 3.1050 (23.1%) | Phys: 0.00 | Val: 2.7232 (35.35%) | Best: 37.01%\n",
            "Epoch 338 | T: 17s | Train: 3.1323 (22.6%) | Phys: 0.00 | Val: 2.6975 (35.37%) | Best: 37.01%\n",
            "Epoch 339 | T: 17s | Train: 3.1207 (23.1%) | Phys: 0.00 | Val: 2.6507 (36.36%) | Best: 37.01%\n",
            "Epoch 340 | T: 17s | Train: 3.1074 (23.4%) | Phys: 0.00 | Val: 2.7312 (35.65%) | Best: 37.01%\n",
            "Epoch 341 | T: 17s | Train: 3.0814 (23.8%) | Phys: 0.00 | Val: 2.6304 (36.82%) | Best: 37.01%\n",
            "Epoch 342 | T: 17s | Train: 3.1096 (23.2%) | Phys: 0.00 | Val: 2.7328 (36.13%) | Best: 37.01%\n",
            "Epoch 343 | T: 17s | Train: 3.0410 (24.7%) | Phys: 0.00 | Val: 2.6904 (36.27%) | Best: 37.01%\n",
            "Epoch 344 | T: 17s | Train: 3.0806 (24.0%) | Phys: 0.00 | Val: 2.7319 (35.94%) | Best: 37.01%\n",
            "Epoch 345 | T: 17s | Train: 3.1040 (23.5%) | Phys: 0.00 | Val: 2.6931 (36.55%) | Best: 37.01%\n",
            "Epoch 346 | T: 17s | Train: 3.0044 (25.5%) | Phys: 0.00 | Val: 2.6389 (36.93%) | Best: 37.01%\n",
            "Epoch 347 | T: 17s | Train: 3.0613 (24.5%) | Phys: 0.00 | Val: 2.6749 (36.70%) | Best: 37.01%\n",
            "Epoch 348 | T: 17s | Train: 3.0910 (23.9%) | Phys: 0.00 | Val: 2.6845 (36.60%) | Best: 37.01%\n",
            "Epoch 349 | T: 17s | Train: 3.0631 (24.2%) | Phys: 0.00 | Val: 2.7262 (35.83%) | Best: 37.01%\n",
            "Epoch 350 | T: 17s | Train: 3.0903 (23.9%) | Phys: 0.00 | Val: 2.6867 (36.04%) | Best: 37.01%\n",
            "Epoch 351 | T: 17s | Train: 3.1171 (23.1%) | Phys: 0.00 | Val: 2.6938 (36.41%) | Best: 37.01%\n",
            "Epoch 352 | T: 17s | Train: 3.1278 (23.1%) | Phys: 0.00 | Val: 2.6883 (36.01%) | Best: 37.01%\n",
            "Epoch 353 | T: 17s | Train: 3.1033 (23.6%) | Phys: 0.00 | Val: 2.6598 (36.48%) | Best: 37.01%\n",
            "Epoch 354 | T: 17s | Train: 3.1099 (23.4%) | Phys: 0.00 | Val: 2.6389 (37.12%) | Best: 37.12%\n",
            "Epoch 355 | T: 17s | Train: 3.0674 (24.6%) | Phys: 0.00 | Val: 2.6547 (36.40%) | Best: 37.12%\n",
            "Epoch 356 | T: 17s | Train: 3.0987 (23.8%) | Phys: 0.00 | Val: 2.6672 (36.89%) | Best: 37.12%\n",
            "Epoch 357 | T: 17s | Train: 3.0495 (24.6%) | Phys: 0.00 | Val: 2.6976 (36.17%) | Best: 37.12%\n",
            "Epoch 358 | T: 17s | Train: 3.0307 (25.3%) | Phys: 0.00 | Val: 2.6474 (37.25%) | Best: 37.25%\n",
            "Epoch 359 | T: 17s | Train: 3.0609 (24.4%) | Phys: 0.00 | Val: 2.6789 (36.37%) | Best: 37.25%\n",
            "Epoch 360 | T: 17s | Train: 3.0558 (24.9%) | Phys: 0.00 | Val: 2.6449 (36.99%) | Best: 37.25%\n",
            "Epoch 361 | T: 17s | Train: 3.1003 (24.0%) | Phys: 0.00 | Val: 2.6797 (36.45%) | Best: 37.25%\n",
            "Epoch 362 | T: 17s | Train: 3.0855 (24.2%) | Phys: 0.00 | Val: 2.6667 (36.53%) | Best: 37.25%\n",
            "Epoch 363 | T: 17s | Train: 3.0982 (23.6%) | Phys: 0.00 | Val: 2.6868 (36.52%) | Best: 37.25%\n",
            "Epoch 364 | T: 17s | Train: 3.0477 (24.8%) | Phys: 0.00 | Val: 2.6669 (36.14%) | Best: 37.25%\n",
            "Epoch 365 | T: 17s | Train: 3.0718 (24.4%) | Phys: 0.00 | Val: 2.6842 (36.53%) | Best: 37.25%\n",
            "Epoch 366 | T: 17s | Train: 3.1099 (23.5%) | Phys: 0.00 | Val: 2.6858 (36.63%) | Best: 37.25%\n",
            "Epoch 367 | T: 17s | Train: 3.0973 (23.8%) | Phys: 0.00 | Val: 2.6701 (36.93%) | Best: 37.25%\n",
            "Epoch 368 | T: 17s | Train: 3.0846 (24.1%) | Phys: 0.00 | Val: 2.6561 (36.63%) | Best: 37.25%\n",
            "Epoch 369 | T: 17s | Train: 3.0779 (24.2%) | Phys: 0.00 | Val: 2.6214 (36.83%) | Best: 37.25%\n",
            "Epoch 370 | T: 17s | Train: 3.0438 (25.1%) | Phys: 0.00 | Val: 2.6437 (37.07%) | Best: 37.25%\n",
            "Epoch 371 | T: 17s | Train: 3.0874 (24.1%) | Phys: 0.00 | Val: 2.7184 (36.55%) | Best: 37.25%\n",
            "Epoch 372 | T: 17s | Train: 3.0697 (24.6%) | Phys: 0.00 | Val: 2.6719 (37.14%) | Best: 37.25%\n",
            "Epoch 373 | T: 17s | Train: 3.0669 (24.4%) | Phys: 0.00 | Val: 2.6322 (37.45%) | Best: 37.45%\n",
            "Epoch 374 | T: 17s | Train: 3.0732 (24.4%) | Phys: 0.00 | Val: 2.6709 (37.39%) | Best: 37.45%\n",
            "Epoch 375 | T: 17s | Train: 3.0468 (24.8%) | Phys: 0.00 | Val: 2.6592 (37.25%) | Best: 37.45%\n",
            "Epoch 376 | T: 17s | Train: 3.1293 (23.3%) | Phys: 0.00 | Val: 2.6711 (36.68%) | Best: 37.45%\n",
            "Epoch 377 | T: 17s | Train: 3.0671 (24.7%) | Phys: 0.00 | Val: 2.6689 (37.06%) | Best: 37.45%\n",
            "Epoch 378 | T: 17s | Train: 3.0906 (24.1%) | Phys: 0.00 | Val: 2.6749 (36.44%) | Best: 37.45%\n",
            "Epoch 379 | T: 17s | Train: 3.0608 (24.7%) | Phys: 0.00 | Val: 2.6715 (36.99%) | Best: 37.45%\n",
            "Epoch 380 | T: 17s | Train: 3.0651 (24.7%) | Phys: 0.00 | Val: 2.6778 (37.18%) | Best: 37.45%\n",
            "Epoch 381 | T: 17s | Train: 3.0468 (24.9%) | Phys: 0.00 | Val: 2.6597 (37.35%) | Best: 37.45%\n",
            "Epoch 382 | T: 17s | Train: 3.0825 (24.1%) | Phys: 0.00 | Val: 2.6281 (37.60%) | Best: 37.60%\n",
            "Epoch 383 | T: 17s | Train: 3.0524 (25.2%) | Phys: 0.00 | Val: 2.6194 (37.33%) | Best: 37.60%\n",
            "Epoch 384 | T: 17s | Train: 3.0512 (25.1%) | Phys: 0.00 | Val: 2.6484 (37.28%) | Best: 37.60%\n",
            "Epoch 385 | T: 17s | Train: 3.0417 (25.1%) | Phys: 0.00 | Val: 2.6179 (37.83%) | Best: 37.83%\n",
            "Epoch 386 | T: 17s | Train: 3.0578 (24.7%) | Phys: 0.00 | Val: 2.6551 (37.60%) | Best: 37.83%\n",
            "Epoch 387 | T: 17s | Train: 3.0484 (25.1%) | Phys: 0.00 | Val: 2.6687 (37.19%) | Best: 37.83%\n",
            "Epoch 388 | T: 17s | Train: 3.0829 (24.4%) | Phys: 0.00 | Val: 2.6608 (37.33%) | Best: 37.83%\n",
            "Epoch 389 | T: 17s | Train: 3.0592 (24.6%) | Phys: 0.00 | Val: 2.6444 (37.24%) | Best: 37.83%\n",
            "Epoch 390 | T: 17s | Train: 3.0744 (24.5%) | Phys: 0.00 | Val: 2.6175 (37.94%) | Best: 37.94%\n",
            "Epoch 391 | T: 17s | Train: 3.0653 (24.9%) | Phys: 0.00 | Val: 2.6469 (37.14%) | Best: 37.94%\n",
            "Epoch 392 | T: 17s | Train: 3.0956 (23.9%) | Phys: 0.00 | Val: 2.6905 (36.48%) | Best: 37.94%\n",
            "Epoch 393 | T: 17s | Train: 3.0823 (24.2%) | Phys: 0.00 | Val: 2.6196 (37.37%) | Best: 37.94%\n",
            "Epoch 394 | T: 17s | Train: 3.0399 (25.4%) | Phys: 0.00 | Val: 2.6216 (37.51%) | Best: 37.94%\n",
            "Epoch 395 | T: 17s | Train: 3.0396 (25.3%) | Phys: 0.00 | Val: 2.5889 (38.08%) | Best: 38.08%\n",
            "Epoch 396 | T: 17s | Train: 3.0398 (25.4%) | Phys: 0.00 | Val: 2.6383 (37.54%) | Best: 38.08%\n",
            "Epoch 397 | T: 17s | Train: 3.0565 (25.3%) | Phys: 0.00 | Val: 2.6556 (37.26%) | Best: 38.08%\n",
            "Epoch 398 | T: 17s | Train: 3.0449 (25.4%) | Phys: 0.00 | Val: 2.6405 (37.54%) | Best: 38.08%\n",
            "Epoch 399 | T: 17s | Train: 3.0573 (25.0%) | Phys: 0.00 | Val: 2.6478 (37.47%) | Best: 38.08%\n",
            "Epoch 400 | T: 17s | Train: 3.0247 (25.8%) | Phys: 0.00 | Val: 2.6434 (37.43%) | Best: 38.08%\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "import math\n",
        "import time\n",
        "\n",
        "REG_MODE = 'baseline'\n",
        "SIGR_ALPHA = 0.1   # Strength of the physics constraint\n",
        "SKETCH_DIM = 64    # Dimension of the random observer\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "LEARNING_RATE = 0.1\n",
        "EPOCHS = 400\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "if torch.backends.mps.is_available(): DEVICE = 'mps'\n",
        "\n",
        "# Regularization Config\n",
        "MIXUP_ALPHA = 0.8\n",
        "CUTMIX_ALPHA = 1.0\n",
        "\n",
        "print(f\"Training on device: {DEVICE}\")\n",
        "\n",
        "def get_data_loaders():\n",
        "    print('==> Preparing data with Strong Augmentation...')\n",
        "\n",
        "    mean = (0.5071, 0.4867, 0.4408)\n",
        "    std = (0.2675, 0.2565, 0.2761)\n",
        "\n",
        "    # FIX 1: Add RandAugment\n",
        "    transform_train = transforms.Compose([\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandAugment(num_ops=2, magnitude=9),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean, std),\n",
        "    ])\n",
        "\n",
        "    transform_test = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean, std),\n",
        "    ])\n",
        "\n",
        "    trainset = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=transform_train)\n",
        "    # Increase workers to handle augmentation load\n",
        "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
        "\n",
        "    testset = torchvision.datasets.CIFAR100(root='./data', train=False, download=True, transform=transform_test)\n",
        "    testloader = torch.utils.data.DataLoader(testset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
        "\n",
        "    return trainloader, testloader\n",
        "\n",
        "\n",
        "# ------------------------------------------\n",
        "# Physics Engine: The Regularizers\n",
        "# ------------------------------------------\n",
        "\n",
        "def sigreg_weak_loss(x, sketch_dim=64):\n",
        "    \"\"\"\n",
        "    Forces Covariance(x) ~ Identity.\n",
        "    Matches the 2nd Moment (Spherical Cloud).\n",
        "    \"\"\"\n",
        "    N, C = x.size()\n",
        "    # 1. Sketching (Optional for C=512, but good for consistency)\n",
        "    if C > sketch_dim:\n",
        "        S = torch.randn(sketch_dim, C, device=x.device) / (C ** 0.5)\n",
        "        x = x @ S.T  # [N, sketch_dim]\n",
        "    else:\n",
        "        sketch_dim = C\n",
        "\n",
        "    # 2. Centering & Covariance\n",
        "    x = x - x.mean(dim=0, keepdim=True)\n",
        "    cov = (x.T @ x) / (N - 1 + 1e-6)\n",
        "\n",
        "    # 3. Target Identity\n",
        "    target = torch.eye(sketch_dim, device=x.device)\n",
        "\n",
        "    # 4. Off-diagonal suppression + Diagonal maintenance\n",
        "    return torch.norm(cov - target, p='fro')\n",
        "\n",
        "def sigreg_strong_loss(x, sketch_dim=64):\n",
        "    \"\"\"\n",
        "    Forces ECF(x) ~ ECF(Gaussian).\n",
        "    Matches ALL Moments (Maximum Entropy Cloud).\n",
        "    Exact implementation of LeJEPA Algorithm 1.\n",
        "    \"\"\"\n",
        "    N, C = x.size()\n",
        "\n",
        "    # 1. Projection (The Observer)\n",
        "    # Project channels down to sketch_dim\n",
        "    A = torch.randn(C, sketch_dim, device=x.device)\n",
        "    A = A / (A.norm(p=2, dim=0, keepdim=True) + 1e-6)\n",
        "\n",
        "    # 2. Integration Points\n",
        "    t = torch.linspace(-5, 5, 17, device=x.device)\n",
        "\n",
        "    # 3. Theoretical Gaussian CF\n",
        "    exp_f = torch.exp(-0.5 * t**2)\n",
        "\n",
        "    # 4. Empirical CF\n",
        "    # proj: [N, sketch_dim]\n",
        "    proj = x @ A\n",
        "\n",
        "    # args: [N, sketch_dim, T]\n",
        "    args = proj.unsqueeze(2) * t.view(1, 1, -1)\n",
        "\n",
        "    # ecf: [sketch_dim, T] (Mean over batch)\n",
        "    ecf = torch.exp(1j * args).mean(dim=0)\n",
        "\n",
        "    # 5. Weighted L2 Distance\n",
        "    # |ecf - gauss|^2 * gauss_weight\n",
        "    diff_sq = (ecf - exp_f.unsqueeze(0)).abs().square()\n",
        "    err = diff_sq * exp_f.unsqueeze(0)\n",
        "\n",
        "    # 6. Integrate\n",
        "    loss = torch.trapz(err, t, dim=1) * N\n",
        "\n",
        "    return loss.mean()\n",
        "\n",
        "# ==========================================\n",
        "# 3. Mixup / CutMix Utilities\n",
        "# ==========================================\n",
        "def rand_bbox(size, lam):\n",
        "    W = size[2]\n",
        "    H = size[3]\n",
        "    cut_rat = np.sqrt(1. - lam)\n",
        "    cut_w = int(W * cut_rat)\n",
        "    cut_h = int(H * cut_rat)\n",
        "\n",
        "    cx = np.random.randint(W)\n",
        "    cy = np.random.randint(H)\n",
        "\n",
        "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
        "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
        "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
        "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
        "\n",
        "    return bbx1, bby1, bbx2, bby2\n",
        "\n",
        "class LinearBlock(nn.Module):\n",
        "    def __init__(self, dim, hidden_dim, reg_mode='baseline', sketch_dim=64):\n",
        "        super().__init__()\n",
        "        self.fc = nn.Linear(dim, hidden_dim)\n",
        "        # Note: NO BATCH NORM. We rely purely on SIGReg.\n",
        "        self.reg_mode = reg_mode\n",
        "        self.sketch_dim = sketch_dim\n",
        "\n",
        "    def forward(self, x):\n",
        "        pre_act = self.fc(x)\n",
        "\n",
        "        reg_loss = torch.tensor(0.0, device=x.device)\n",
        "        if self.reg_mode != 'baseline':\n",
        "            if self.reg_mode == 'weak':\n",
        "                reg_loss = sigreg_weak_loss(pre_act, self.sketch_dim)\n",
        "            elif self.reg_mode == 'strong':\n",
        "                reg_loss = sigreg_strong_loss(pre_act, self.sketch_dim)\n",
        "\n",
        "        out = F.relu(pre_act)\n",
        "\n",
        "        return out, reg_loss\n",
        "\n",
        "class ThermoMLP(nn.Module):\n",
        "    def __init__(self, input_dim=3072, hidden_dim=1024, num_classes=100, depth=6, reg_mode='weak', sketch_dim=64):\n",
        "        super().__init__()\n",
        "\n",
        "        layers = []\n",
        "        # Input Layer\n",
        "        layers.append(LinearBlock(input_dim, hidden_dim, reg_mode))\n",
        "\n",
        "        # Deep Layers (No Residuals!)\n",
        "        for _ in range(depth - 2):\n",
        "            layers.append(LinearBlock(hidden_dim, hidden_dim, reg_mode, sketch_dim))\n",
        "\n",
        "        self.layers = nn.ModuleList(layers)\n",
        "        self.classifier = nn.Linear(hidden_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Flatten: [B, C, H, W] -> [B, 3072]\n",
        "        x = x.flatten(1)\n",
        "\n",
        "        total_phys_loss = 0.0\n",
        "\n",
        "        for layer in self.layers:\n",
        "            x, l_loss = layer(x)\n",
        "            total_phys_loss += l_loss\n",
        "\n",
        "        out = self.classifier(x)\n",
        "\n",
        "        # Normalize loss scale\n",
        "        return out, (total_phys_loss / len(self.layers))\n",
        "\n",
        "# ==========================================\n",
        "# 5. Training Engine (Updated for Mixup/CutMix)\n",
        "# ==========================================\n",
        "def train(epoch, net, trainloader, optimizer, criterion):\n",
        "    net.train()\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    phys_loss_meter = 0\n",
        "\n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
        "\n",
        "        # Apply Mixup/CutMix\n",
        "        r = np.random.rand(1)\n",
        "        if r < 0.5: # Mixup\n",
        "            lam = np.random.beta(MIXUP_ALPHA, MIXUP_ALPHA)\n",
        "            index = torch.randperm(inputs.size(0)).to(DEVICE)\n",
        "            inputs = lam * inputs + (1 - lam) * inputs[index, :]\n",
        "            targets_a, targets_b = targets, targets[index]\n",
        "        else: # CutMix\n",
        "            lam = np.random.beta(CUTMIX_ALPHA, CUTMIX_ALPHA)\n",
        "            rand_index = torch.randperm(inputs.size(0)).to(DEVICE)\n",
        "            target_a = targets\n",
        "            target_b = targets[rand_index]\n",
        "            bbx1, bby1, bbx2, bby2 = rand_bbox(inputs.size(), lam)\n",
        "            inputs[:, :, bbx1:bbx2, bby1:bby2] = inputs[rand_index, :, bbx1:bbx2, bby1:bby2]\n",
        "            lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (inputs.size()[-1] * inputs.size()[-2]))\n",
        "            targets_a, targets_b = target_a, target_b\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward\n",
        "        outputs, p_loss = net(inputs)\n",
        "\n",
        "        # Task Loss\n",
        "        c_loss = criterion(outputs, targets_a) * lam + criterion(outputs, targets_b) * (1. - lam)\n",
        "\n",
        "        # Total Loss\n",
        "        loss = (1 - SIGR_ALPHA) * c_loss + (SIGR_ALPHA * p_loss)\n",
        "\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(net.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += ((1 - SIGR_ALPHA) * c_loss).item() # Log only task loss for comparison\n",
        "        phys_loss_meter += (SIGR_ALPHA * p_loss).item()\n",
        "\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += (lam * predicted.eq(targets_a).float() + (1 - lam) * predicted.eq(targets_b).float()).sum().item()\n",
        "\n",
        "    acc = 100. * correct / total\n",
        "    return train_loss / (batch_idx + 1), acc, phys_loss_meter / (batch_idx + 1)\n",
        "\n",
        "def test(epoch, net, testloader, criterion):\n",
        "    net.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "            inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
        "            outputs, _ = net(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "    acc = 100. * correct / total\n",
        "    return test_loss / (batch_idx + 1), acc\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    trainloader, testloader = get_data_loaders()\n",
        "\n",
        "    net = ThermoMLP(reg_mode=REG_MODE, sketch_dim=SKETCH_DIM).to(DEVICE)\n",
        "    net = net.to(DEVICE)\n",
        "\n",
        "    # Standard CrossEntropy for final eval, SoftLabel for training is handled by Mixup logic\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    optimizer = optim.SGD(net.parameters(), lr=LEARNING_RATE, momentum=0.0, weight_decay=0.0)\n",
        "\n",
        "    print(f\"Starting training for {EPOCHS} epochs with RandAugment + Mixup/CutMix...\")\n",
        "\n",
        "    best_acc = 0\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        start_time = time.time()\n",
        "\n",
        "        train_loss, train_acc, physics_loss = train(epoch, net, trainloader, optimizer, criterion)\n",
        "        test_loss, test_acc = test(epoch, net, testloader, criterion)\n",
        "\n",
        "        if test_acc > best_acc:\n",
        "            best_acc = test_acc\n",
        "            # torch.save(net.state_dict(), f'thermo_resnet_{REG_MODE}.pth')\n",
        "\n",
        "        epoch_time = time.time() - start_time\n",
        "\n",
        "        print(f\"Epoch {epoch+1} | T: {epoch_time:.0f}s | \"\n",
        "              f\"Train: {train_loss:.4f} ({train_acc:.1f}%) | \"\n",
        "              f\"Phys: {physics_loss:.2f} | \"\n",
        "              f\"Val: {test_loss:.4f} ({test_acc:.2f}%) | \"\n",
        "              f\"Best: {best_acc:.2f}%\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "name": "CIFAR-100 MLP Baseline",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
