{
  "cells": [
    {
      "cell_type": "code",
      "id": "3amcbKc5hLopVnnXSD9Wt1yi",
      "metadata": {
        "tags": [],
        "id": "3amcbKc5hLopVnnXSD9Wt1yi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c6a2a9c-e41c-4af9-f54e-98c67b661cb9"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "\n",
        "# ==========================================\n",
        "# 1. Configuration & Physics\n",
        "# ==========================================\n",
        "# OPTIONS: 'baseline', 'weak' (Covariance), 'strong' (LeJEPA/Epps-Pulley)\n",
        "REG_MODE = 'baseline'\n",
        "SIGR_ALPHA = 0.01   # Strength of the physics constraint\n",
        "SKETCH_DIM = 64    # Dimension of the random observer\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "LEARNING_RATE = 0.1\n",
        "EPOCHS = 400\n",
        "WEIGHT_DECAY = 5e-4\n",
        "MOMENTUM = 0.9\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "MIXUP_ALPHA = 0.8\n",
        "CUTMIX_ALPHA = 1.0\n",
        "\n",
        "# Check for Apple Silicon (MPS)\n",
        "if torch.backends.mps.is_available():\n",
        "    DEVICE = 'mps'\n",
        "\n",
        "print(f\"Training on: {DEVICE} | Mode: {REG_MODE} | Alpha: {SIGR_ALPHA}\")\n",
        "\n",
        "def rand_bbox(size, lam):\n",
        "    W = size[2]\n",
        "    H = size[3]\n",
        "    cut_rat = np.sqrt(1. - lam)\n",
        "    cut_w = int(W * cut_rat)\n",
        "    cut_h = int(H * cut_rat)\n",
        "\n",
        "    cx = np.random.randint(W)\n",
        "    cy = np.random.randint(H)\n",
        "\n",
        "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
        "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
        "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
        "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
        "\n",
        "    return bbx1, bby1, bbx2, bby2\n",
        "\n",
        "# ------------------------------------------\n",
        "# Physics Engine: The Regularizers\n",
        "# ------------------------------------------\n",
        "\n",
        "def sigreg_weak_loss(x, sketch_dim=64):\n",
        "    \"\"\"\n",
        "    Forces Covariance(x) ~ Identity.\n",
        "    Matches the 2nd Moment (Spherical Cloud).\n",
        "    \"\"\"\n",
        "    N, C = x.size()\n",
        "    # 1. Sketching (Optional for C=512, but good for consistency)\n",
        "    if C > sketch_dim:\n",
        "        S = torch.randn(sketch_dim, C, device=x.device) / (C ** 0.5)\n",
        "        x = x @ S.T  # [N, sketch_dim]\n",
        "    else:\n",
        "        sketch_dim = C\n",
        "\n",
        "    # 2. Centering & Covariance\n",
        "    x = x - x.mean(dim=0, keepdim=True)\n",
        "    cov = (x.T @ x) / (N - 1 + 1e-6)\n",
        "\n",
        "    # 3. Target Identity\n",
        "    target = torch.eye(sketch_dim, device=x.device)\n",
        "\n",
        "    # 4. Off-diagonal suppression + Diagonal maintenance\n",
        "    return torch.norm(cov - target, p='fro')\n",
        "\n",
        "def sigreg_strong_loss(x, sketch_dim=64):\n",
        "    \"\"\"\n",
        "    Forces ECF(x) ~ ECF(Gaussian).\n",
        "    Matches ALL Moments (Maximum Entropy Cloud).\n",
        "    Exact implementation of LeJEPA Algorithm 1.\n",
        "    \"\"\"\n",
        "    N, C = x.size()\n",
        "\n",
        "    # 1. Projection (The Observer)\n",
        "    # Project channels down to sketch_dim\n",
        "    A = torch.randn(C, sketch_dim, device=x.device)\n",
        "    A = A / (A.norm(p=2, dim=0, keepdim=True) + 1e-6)\n",
        "\n",
        "    # 2. Integration Points\n",
        "    t = torch.linspace(-5, 5, 17, device=x.device)\n",
        "\n",
        "    # 3. Theoretical Gaussian CF\n",
        "    exp_f = torch.exp(-0.5 * t**2)\n",
        "\n",
        "    # 4. Empirical CF\n",
        "    # proj: [N, sketch_dim]\n",
        "    proj = x @ A\n",
        "\n",
        "    # args: [N, sketch_dim, T]\n",
        "    args = proj.unsqueeze(2) * t.view(1, 1, -1)\n",
        "\n",
        "    # ecf: [sketch_dim, T] (Mean over batch)\n",
        "    ecf = torch.exp(1j * args).mean(dim=0)\n",
        "\n",
        "    # 5. Weighted L2 Distance\n",
        "    # |ecf - gauss|^2 * gauss_weight\n",
        "    diff_sq = (ecf - exp_f.unsqueeze(0)).abs().square()\n",
        "    err = diff_sq * exp_f.unsqueeze(0)\n",
        "\n",
        "    # 6. Integrate\n",
        "    loss = torch.trapz(err, t, dim=1) * N\n",
        "\n",
        "    return loss.mean()\n",
        "\n",
        "# ==========================================\n",
        "# 2. Thermodynamic ResNet-18\n",
        "# ==========================================\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "\n",
        "        # --- PHYSICS INJECTION ---\n",
        "        # We calculate loss here, but we need to return it to the main model.\n",
        "        # We use Global Average Pooling to get a (N, C) vector for regularization\n",
        "        # This forces the semantic features to be isotropic.\n",
        "        reg_loss = torch.tensor(0.0, device=x.device)\n",
        "\n",
        "        if REG_MODE != 'baseline':\n",
        "            # Pool spatial dims: [N, C, H, W] -> [N, C]\n",
        "            flat = F.adaptive_avg_pool2d(out, (1, 1)).view(out.size(0), -1)\n",
        "\n",
        "            if REG_MODE == 'weak':\n",
        "                reg_loss = sigreg_weak_loss(flat, SKETCH_DIM)\n",
        "            elif REG_MODE == 'strong':\n",
        "                reg_loss = sigreg_strong_loss(flat, SKETCH_DIM)\n",
        "\n",
        "        return out, reg_loss\n",
        "\n",
        "class ThermoResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=100):\n",
        "        super(ThermoResNet, self).__init__()\n",
        "        self.in_planes = 64\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.ModuleList(layers) # Changed to ModuleList to iterate manually\n",
        "\n",
        "    def forward(self, x):\n",
        "        total_phys_loss = 0.0\n",
        "\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "\n",
        "        # Manually iterate through layers to collect physics losses\n",
        "        for layer_group in [self.layer1, self.layer2, self.layer3, self.layer4]:\n",
        "            for block in layer_group:\n",
        "                out, l_loss = block(out)\n",
        "                total_phys_loss += l_loss\n",
        "\n",
        "        out = F.avg_pool2d(out, 4)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "\n",
        "        # Normalize p_loss by number of layers to keep scale consistent\n",
        "        return out, (total_phys_loss / 8.0)\n",
        "\n",
        "def ResNet18():\n",
        "    return ThermoResNet(BasicBlock, [2, 2, 2, 2])\n",
        "\n",
        "# ==========================================\n",
        "# 3. Data Preparation\n",
        "# ==========================================\n",
        "def get_data_loaders():\n",
        "    print('==> Preparing data...')\n",
        "    mean = (0.5070751592371323, 0.48654887331495095, 0.4409178433670343)\n",
        "    std = (0.2673342858792401, 0.2564384629170883, 0.27615047132568404)\n",
        "\n",
        "    transform_train = transforms.Compose([\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean, std),\n",
        "    ])\n",
        "\n",
        "    transform_test = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean, std),\n",
        "    ])\n",
        "\n",
        "    trainset = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=transform_train)\n",
        "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
        "\n",
        "    testset = torchvision.datasets.CIFAR100(root='./data', train=False, download=True, transform=transform_test)\n",
        "    testloader = torch.utils.data.DataLoader(testset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
        "\n",
        "    return trainloader, testloader\n",
        "\n",
        "# ==========================================\n",
        "# 4. Training Engine\n",
        "# ==========================================\n",
        "def train(epoch, net, trainloader, optimizer, criterion):\n",
        "    net.train()\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    phys_loss_meter = 0\n",
        "\n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
        "\n",
        "        # Apply Mixup/CutMix\n",
        "        r = np.random.rand(1)\n",
        "        if r < 0.5: # Mixup\n",
        "            lam = np.random.beta(MIXUP_ALPHA, MIXUP_ALPHA)\n",
        "            index = torch.randperm(inputs.size(0)).to(DEVICE)\n",
        "            inputs = lam * inputs + (1 - lam) * inputs[index, :]\n",
        "            targets_a, targets_b = targets, targets[index]\n",
        "        else: # CutMix\n",
        "            lam = np.random.beta(CUTMIX_ALPHA, CUTMIX_ALPHA)\n",
        "            rand_index = torch.randperm(inputs.size(0)).to(DEVICE)\n",
        "            target_a = targets\n",
        "            target_b = targets[rand_index]\n",
        "            bbx1, bby1, bbx2, bby2 = rand_bbox(inputs.size(), lam)\n",
        "            inputs[:, :, bbx1:bbx2, bby1:bby2] = inputs[rand_index, :, bbx1:bbx2, bby1:bby2]\n",
        "            lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (inputs.size()[-1] * inputs.size()[-2]))\n",
        "            targets_a, targets_b = target_a, target_b\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward\n",
        "        outputs, p_loss = net(inputs)\n",
        "\n",
        "        # Task Loss\n",
        "        c_loss = criterion(outputs, targets_a) * lam + criterion(outputs, targets_b) * (1. - lam)\n",
        "\n",
        "        # Total Loss\n",
        "        loss = (1 - SIGR_ALPHA) * c_loss + (SIGR_ALPHA * p_loss)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += ((1 - SIGR_ALPHA) * c_loss).item() # Log only task loss for comparison\n",
        "        phys_loss_meter += (SIGR_ALPHA * p_loss).item()\n",
        "\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += (lam * predicted.eq(targets_a).float() + (1 - lam) * predicted.eq(targets_b).float()).sum().item()\n",
        "\n",
        "    acc = 100. * correct / total\n",
        "    return train_loss / (batch_idx + 1), acc, phys_loss_meter / (batch_idx + 1)\n",
        "\n",
        "def test(epoch, net, testloader, criterion):\n",
        "    net.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "            inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
        "            outputs, _ = net(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "    acc = 100. * correct / total\n",
        "    return test_loss / (batch_idx + 1), acc\n",
        "\n",
        "# ==========================================\n",
        "# 5. Main Execution\n",
        "# ==========================================\n",
        "if __name__ == '__main__':\n",
        "    trainloader, testloader = get_data_loaders()\n",
        "\n",
        "    print(f'==> Building model (Mode: {REG_MODE})...')\n",
        "    net = ResNet18()\n",
        "    net = net.to(DEVICE)\n",
        "\n",
        "    if DEVICE == 'cuda':\n",
        "        net = torch.nn.DataParallel(net)\n",
        "        cudnn.benchmark = True\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(net.parameters(), lr=LEARNING_RATE, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
        "\n",
        "    print(f\"Starting training for {EPOCHS} epochs...\")\n",
        "    best_acc = 0\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        start_time = time.time()\n",
        "\n",
        "        tr_loss, tr_acc, phys_loss = train(epoch, net, trainloader, optimizer, criterion)\n",
        "        te_loss, te_acc = test(epoch, net, testloader, criterion)\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        if te_acc > best_acc:\n",
        "            best_acc = te_acc\n",
        "            # torch.save(net.state_dict(), f'thermo_resnet_{REG_MODE}.pth')\n",
        "\n",
        "        epoch_time = time.time() - start_time\n",
        "\n",
        "        print(f\"Epoch {epoch+1:03d} | T: {epoch_time:.0f}s | \"\n",
        "              f\"Train: {tr_loss:.4f} ({tr_acc:.1f}%) | \"\n",
        "              f\"Phys: {phys_loss:.2f} | \"\n",
        "              f\"Val: {te_loss:.4f} ({te_acc:.2f}%) | \"\n",
        "              f\"Best: {best_acc:.2f}%\")\n",
        "\n",
        "    print(f\"Final Best: {best_acc:.2f}%\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on: cuda | Mode: baseline | Alpha: 0.01\n",
            "==> Preparing data...\n",
            "==> Building model (Mode: baseline)...\n",
            "Starting training for 400 epochs...\n",
            "Epoch 001 | T: 22s | Train: 4.2674 (5.3%) | Phys: 0.00 | Val: 3.8380 (9.31%) | Best: 9.31%\n",
            "Epoch 002 | T: 21s | Train: 4.0147 (8.7%) | Phys: 0.00 | Val: 3.5303 (16.33%) | Best: 16.33%\n",
            "Epoch 003 | T: 21s | Train: 3.8255 (12.7%) | Phys: 0.00 | Val: 3.1762 (22.42%) | Best: 22.42%\n",
            "Epoch 004 | T: 21s | Train: 3.6367 (16.3%) | Phys: 0.00 | Val: 2.9784 (25.67%) | Best: 25.67%\n",
            "Epoch 005 | T: 21s | Train: 3.4603 (20.0%) | Phys: 0.00 | Val: 3.0577 (25.31%) | Best: 25.67%\n",
            "Epoch 006 | T: 21s | Train: 3.3640 (22.1%) | Phys: 0.00 | Val: 2.4932 (36.10%) | Best: 36.10%\n",
            "Epoch 007 | T: 21s | Train: 3.1980 (25.8%) | Phys: 0.00 | Val: 2.2807 (40.03%) | Best: 40.03%\n",
            "Epoch 008 | T: 21s | Train: 3.1281 (27.5%) | Phys: 0.00 | Val: 2.4255 (37.24%) | Best: 40.03%\n",
            "Epoch 009 | T: 21s | Train: 3.0527 (29.3%) | Phys: 0.00 | Val: 2.1696 (43.45%) | Best: 43.45%\n",
            "Epoch 010 | T: 21s | Train: 3.0051 (30.4%) | Phys: 0.00 | Val: 2.1497 (44.18%) | Best: 44.18%\n",
            "Epoch 011 | T: 21s | Train: 2.9709 (31.9%) | Phys: 0.00 | Val: 2.0067 (47.21%) | Best: 47.21%\n",
            "Epoch 012 | T: 21s | Train: 2.9077 (33.2%) | Phys: 0.00 | Val: 1.9790 (47.44%) | Best: 47.44%\n",
            "Epoch 013 | T: 21s | Train: 2.8269 (34.9%) | Phys: 0.00 | Val: 2.0510 (46.17%) | Best: 47.44%\n",
            "Epoch 014 | T: 21s | Train: 2.8174 (35.2%) | Phys: 0.00 | Val: 1.9600 (49.23%) | Best: 49.23%\n",
            "Epoch 015 | T: 21s | Train: 2.7645 (36.7%) | Phys: 0.00 | Val: 2.0519 (46.33%) | Best: 49.23%\n",
            "Epoch 016 | T: 21s | Train: 2.8040 (36.1%) | Phys: 0.00 | Val: 1.8453 (52.22%) | Best: 52.22%\n",
            "Epoch 017 | T: 21s | Train: 2.7586 (36.8%) | Phys: 0.00 | Val: 1.7228 (53.56%) | Best: 53.56%\n",
            "Epoch 018 | T: 21s | Train: 2.7088 (38.4%) | Phys: 0.00 | Val: 1.7926 (52.24%) | Best: 53.56%\n",
            "Epoch 019 | T: 21s | Train: 2.6890 (38.7%) | Phys: 0.00 | Val: 1.8181 (52.24%) | Best: 53.56%\n",
            "Epoch 020 | T: 21s | Train: 2.6982 (38.8%) | Phys: 0.00 | Val: 1.8376 (52.35%) | Best: 53.56%\n",
            "Epoch 021 | T: 21s | Train: 2.7303 (38.0%) | Phys: 0.00 | Val: 1.8771 (52.13%) | Best: 53.56%\n",
            "Epoch 022 | T: 21s | Train: 2.7323 (38.1%) | Phys: 0.00 | Val: 1.7258 (54.74%) | Best: 54.74%\n",
            "Epoch 023 | T: 21s | Train: 2.7163 (38.3%) | Phys: 0.00 | Val: 1.9301 (51.48%) | Best: 54.74%\n",
            "Epoch 024 | T: 21s | Train: 2.6656 (39.3%) | Phys: 0.00 | Val: 1.7342 (55.46%) | Best: 55.46%\n",
            "Epoch 025 | T: 21s | Train: 2.6225 (40.5%) | Phys: 0.00 | Val: 1.7600 (53.46%) | Best: 55.46%\n",
            "Epoch 026 | T: 21s | Train: 2.6535 (39.8%) | Phys: 0.00 | Val: 1.8945 (50.90%) | Best: 55.46%\n",
            "Epoch 027 | T: 21s | Train: 2.6590 (39.7%) | Phys: 0.00 | Val: 1.7138 (55.24%) | Best: 55.46%\n",
            "Epoch 028 | T: 21s | Train: 2.6498 (40.0%) | Phys: 0.00 | Val: 1.7896 (54.19%) | Best: 55.46%\n",
            "Epoch 029 | T: 21s | Train: 2.6416 (40.1%) | Phys: 0.00 | Val: 1.9136 (51.61%) | Best: 55.46%\n",
            "Epoch 030 | T: 21s | Train: 2.6907 (39.2%) | Phys: 0.00 | Val: 1.6992 (57.24%) | Best: 57.24%\n",
            "Epoch 031 | T: 20s | Train: 2.6235 (40.7%) | Phys: 0.00 | Val: 1.6031 (57.78%) | Best: 57.78%\n",
            "Epoch 032 | T: 21s | Train: 2.5992 (41.0%) | Phys: 0.00 | Val: 1.5890 (57.64%) | Best: 57.78%\n",
            "Epoch 033 | T: 21s | Train: 2.6256 (40.6%) | Phys: 0.00 | Val: 1.7231 (53.95%) | Best: 57.78%\n",
            "Epoch 034 | T: 21s | Train: 2.6200 (40.8%) | Phys: 0.00 | Val: 1.6919 (57.19%) | Best: 57.78%\n",
            "Epoch 035 | T: 21s | Train: 2.6549 (40.0%) | Phys: 0.00 | Val: 1.7122 (55.20%) | Best: 57.78%\n",
            "Epoch 036 | T: 20s | Train: 2.5751 (41.5%) | Phys: 0.00 | Val: 1.7269 (54.64%) | Best: 57.78%\n",
            "Epoch 037 | T: 20s | Train: 2.5912 (41.5%) | Phys: 0.00 | Val: 1.7786 (54.74%) | Best: 57.78%\n",
            "Epoch 038 | T: 20s | Train: 2.6396 (40.4%) | Phys: 0.00 | Val: 1.7039 (54.98%) | Best: 57.78%\n",
            "Epoch 039 | T: 21s | Train: 2.5757 (41.5%) | Phys: 0.00 | Val: 1.5433 (58.70%) | Best: 58.70%\n",
            "Epoch 040 | T: 21s | Train: 2.5763 (41.8%) | Phys: 0.00 | Val: 1.5847 (58.33%) | Best: 58.70%\n",
            "Epoch 041 | T: 21s | Train: 2.6194 (40.8%) | Phys: 0.00 | Val: 1.6724 (56.05%) | Best: 58.70%\n",
            "Epoch 042 | T: 21s | Train: 2.5513 (42.4%) | Phys: 0.00 | Val: 1.7278 (55.85%) | Best: 58.70%\n",
            "Epoch 043 | T: 21s | Train: 2.5803 (41.8%) | Phys: 0.00 | Val: 1.5662 (58.41%) | Best: 58.70%\n",
            "Epoch 044 | T: 21s | Train: 2.5287 (42.8%) | Phys: 0.00 | Val: 1.8598 (52.41%) | Best: 58.70%\n",
            "Epoch 045 | T: 21s | Train: 2.5779 (41.5%) | Phys: 0.00 | Val: 1.8369 (52.68%) | Best: 58.70%\n",
            "Epoch 046 | T: 21s | Train: 2.5698 (42.0%) | Phys: 0.00 | Val: 1.5340 (59.36%) | Best: 59.36%\n",
            "Epoch 047 | T: 21s | Train: 2.6025 (41.3%) | Phys: 0.00 | Val: 1.5593 (59.02%) | Best: 59.36%\n",
            "Epoch 048 | T: 20s | Train: 2.5371 (42.9%) | Phys: 0.00 | Val: 1.6936 (56.13%) | Best: 59.36%\n",
            "Epoch 049 | T: 20s | Train: 2.5819 (41.7%) | Phys: 0.00 | Val: 1.6987 (57.07%) | Best: 59.36%\n",
            "Epoch 050 | T: 21s | Train: 2.5642 (42.0%) | Phys: 0.00 | Val: 1.6960 (55.56%) | Best: 59.36%\n",
            "Epoch 051 | T: 21s | Train: 2.5280 (42.9%) | Phys: 0.00 | Val: 1.5671 (58.65%) | Best: 59.36%\n",
            "Epoch 052 | T: 21s | Train: 2.4979 (43.6%) | Phys: 0.00 | Val: 1.4991 (59.20%) | Best: 59.36%\n",
            "Epoch 053 | T: 21s | Train: 2.5126 (43.1%) | Phys: 0.00 | Val: 1.7157 (55.11%) | Best: 59.36%\n",
            "Epoch 054 | T: 21s | Train: 2.5639 (42.2%) | Phys: 0.00 | Val: 1.6456 (57.66%) | Best: 59.36%\n",
            "Epoch 055 | T: 21s | Train: 2.5769 (41.9%) | Phys: 0.00 | Val: 1.6999 (55.11%) | Best: 59.36%\n",
            "Epoch 056 | T: 21s | Train: 2.5719 (41.8%) | Phys: 0.00 | Val: 1.7580 (55.01%) | Best: 59.36%\n",
            "Epoch 057 | T: 21s | Train: 2.5125 (43.1%) | Phys: 0.00 | Val: 1.6147 (57.94%) | Best: 59.36%\n",
            "Epoch 058 | T: 21s | Train: 2.5141 (42.9%) | Phys: 0.00 | Val: 1.5450 (58.92%) | Best: 59.36%\n",
            "Epoch 059 | T: 21s | Train: 2.5735 (42.1%) | Phys: 0.00 | Val: 1.5640 (57.94%) | Best: 59.36%\n",
            "Epoch 060 | T: 20s | Train: 2.5085 (43.2%) | Phys: 0.00 | Val: 1.7110 (55.12%) | Best: 59.36%\n",
            "Epoch 061 | T: 21s | Train: 2.4794 (43.9%) | Phys: 0.00 | Val: 1.5374 (59.83%) | Best: 59.83%\n",
            "Epoch 062 | T: 21s | Train: 2.5158 (43.1%) | Phys: 0.00 | Val: 1.6512 (57.11%) | Best: 59.83%\n",
            "Epoch 063 | T: 21s | Train: 2.5259 (43.5%) | Phys: 0.00 | Val: 1.6204 (58.62%) | Best: 59.83%\n",
            "Epoch 064 | T: 21s | Train: 2.5233 (43.1%) | Phys: 0.00 | Val: 1.7038 (57.41%) | Best: 59.83%\n",
            "Epoch 065 | T: 21s | Train: 2.4877 (43.8%) | Phys: 0.00 | Val: 1.6053 (58.50%) | Best: 59.83%\n",
            "Epoch 066 | T: 21s | Train: 2.4960 (43.8%) | Phys: 0.00 | Val: 1.7266 (54.83%) | Best: 59.83%\n",
            "Epoch 067 | T: 21s | Train: 2.5472 (42.5%) | Phys: 0.00 | Val: 1.6401 (56.68%) | Best: 59.83%\n",
            "Epoch 068 | T: 21s | Train: 2.4737 (44.5%) | Phys: 0.00 | Val: 1.5415 (59.34%) | Best: 59.83%\n",
            "Epoch 069 | T: 21s | Train: 2.4815 (44.0%) | Phys: 0.00 | Val: 1.5994 (58.42%) | Best: 59.83%\n",
            "Epoch 070 | T: 21s | Train: 2.5062 (43.7%) | Phys: 0.00 | Val: 1.6058 (57.90%) | Best: 59.83%\n",
            "Epoch 071 | T: 21s | Train: 2.5469 (42.5%) | Phys: 0.00 | Val: 1.5374 (59.81%) | Best: 59.83%\n",
            "Epoch 072 | T: 20s | Train: 2.5071 (43.7%) | Phys: 0.00 | Val: 1.5887 (58.83%) | Best: 59.83%\n",
            "Epoch 073 | T: 20s | Train: 2.4764 (44.3%) | Phys: 0.00 | Val: 1.7162 (55.38%) | Best: 59.83%\n",
            "Epoch 074 | T: 21s | Train: 2.5099 (43.7%) | Phys: 0.00 | Val: 1.5513 (59.31%) | Best: 59.83%\n",
            "Epoch 075 | T: 21s | Train: 2.5896 (41.5%) | Phys: 0.00 | Val: 1.7381 (54.99%) | Best: 59.83%\n",
            "Epoch 076 | T: 21s | Train: 2.4475 (44.6%) | Phys: 0.00 | Val: 1.6228 (57.89%) | Best: 59.83%\n",
            "Epoch 077 | T: 21s | Train: 2.4890 (43.8%) | Phys: 0.00 | Val: 1.6996 (56.67%) | Best: 59.83%\n",
            "Epoch 078 | T: 21s | Train: 2.4740 (44.1%) | Phys: 0.00 | Val: 1.5065 (60.67%) | Best: 60.67%\n",
            "Epoch 079 | T: 21s | Train: 2.5323 (42.5%) | Phys: 0.00 | Val: 1.5889 (59.29%) | Best: 60.67%\n",
            "Epoch 080 | T: 21s | Train: 2.5270 (43.2%) | Phys: 0.00 | Val: 1.8700 (52.83%) | Best: 60.67%\n",
            "Epoch 081 | T: 21s | Train: 2.4620 (44.1%) | Phys: 0.00 | Val: 1.5510 (58.99%) | Best: 60.67%\n",
            "Epoch 082 | T: 21s | Train: 2.4781 (43.9%) | Phys: 0.00 | Val: 1.6599 (56.36%) | Best: 60.67%\n",
            "Epoch 083 | T: 21s | Train: 2.5189 (43.1%) | Phys: 0.00 | Val: 1.6681 (57.71%) | Best: 60.67%\n",
            "Epoch 084 | T: 20s | Train: 2.4301 (44.9%) | Phys: 0.00 | Val: 1.5897 (59.41%) | Best: 60.67%\n",
            "Epoch 085 | T: 21s | Train: 2.5097 (43.6%) | Phys: 0.00 | Val: 1.6205 (57.94%) | Best: 60.67%\n",
            "Epoch 086 | T: 21s | Train: 2.4727 (44.4%) | Phys: 0.00 | Val: 1.5413 (60.52%) | Best: 60.67%\n",
            "Epoch 087 | T: 21s | Train: 2.5036 (43.4%) | Phys: 0.00 | Val: 1.5441 (59.70%) | Best: 60.67%\n",
            "Epoch 088 | T: 21s | Train: 2.4523 (44.6%) | Phys: 0.00 | Val: 1.5861 (58.30%) | Best: 60.67%\n",
            "Epoch 089 | T: 21s | Train: 2.5276 (42.9%) | Phys: 0.00 | Val: 1.7321 (55.27%) | Best: 60.67%\n",
            "Epoch 090 | T: 21s | Train: 2.4504 (44.6%) | Phys: 0.00 | Val: 1.5681 (59.24%) | Best: 60.67%\n",
            "Epoch 091 | T: 21s | Train: 2.4622 (44.4%) | Phys: 0.00 | Val: 1.5779 (58.04%) | Best: 60.67%\n",
            "Epoch 092 | T: 21s | Train: 2.4923 (43.9%) | Phys: 0.00 | Val: 1.6415 (57.58%) | Best: 60.67%\n",
            "Epoch 093 | T: 21s | Train: 2.4595 (44.5%) | Phys: 0.00 | Val: 1.5429 (60.91%) | Best: 60.91%\n",
            "Epoch 094 | T: 21s | Train: 2.4489 (45.2%) | Phys: 0.00 | Val: 1.5397 (60.70%) | Best: 60.91%\n",
            "Epoch 095 | T: 21s | Train: 2.4917 (43.6%) | Phys: 0.00 | Val: 1.5188 (60.18%) | Best: 60.91%\n",
            "Epoch 096 | T: 20s | Train: 2.4609 (44.5%) | Phys: 0.00 | Val: 1.6838 (56.11%) | Best: 60.91%\n",
            "Epoch 097 | T: 20s | Train: 2.4680 (44.7%) | Phys: 0.00 | Val: 1.6600 (57.23%) | Best: 60.91%\n",
            "Epoch 098 | T: 21s | Train: 2.4363 (45.3%) | Phys: 0.00 | Val: 1.4909 (60.73%) | Best: 60.91%\n",
            "Epoch 099 | T: 21s | Train: 2.4618 (44.4%) | Phys: 0.00 | Val: 1.6016 (59.15%) | Best: 60.91%\n",
            "Epoch 100 | T: 21s | Train: 2.4568 (44.7%) | Phys: 0.00 | Val: 1.4813 (61.28%) | Best: 61.28%\n",
            "Epoch 101 | T: 21s | Train: 2.4685 (44.0%) | Phys: 0.00 | Val: 1.5980 (57.52%) | Best: 61.28%\n",
            "Epoch 102 | T: 21s | Train: 2.4796 (44.0%) | Phys: 0.00 | Val: 1.7539 (55.43%) | Best: 61.28%\n",
            "Epoch 103 | T: 21s | Train: 2.5072 (43.4%) | Phys: 0.00 | Val: 1.6779 (57.39%) | Best: 61.28%\n",
            "Epoch 104 | T: 21s | Train: 2.4106 (45.5%) | Phys: 0.00 | Val: 1.5125 (61.32%) | Best: 61.32%\n",
            "Epoch 105 | T: 21s | Train: 2.4727 (44.7%) | Phys: 0.00 | Val: 1.6023 (59.78%) | Best: 61.32%\n",
            "Epoch 106 | T: 21s | Train: 2.4345 (45.1%) | Phys: 0.00 | Val: 1.6506 (56.71%) | Best: 61.32%\n",
            "Epoch 107 | T: 20s | Train: 2.4409 (45.3%) | Phys: 0.00 | Val: 1.5372 (60.60%) | Best: 61.32%\n",
            "Epoch 108 | T: 20s | Train: 2.4756 (44.2%) | Phys: 0.00 | Val: 1.4826 (61.33%) | Best: 61.33%\n",
            "Epoch 109 | T: 21s | Train: 2.4478 (45.4%) | Phys: 0.00 | Val: 1.5523 (60.05%) | Best: 61.33%\n",
            "Epoch 110 | T: 21s | Train: 2.4893 (44.0%) | Phys: 0.00 | Val: 1.4780 (61.96%) | Best: 61.96%\n",
            "Epoch 111 | T: 21s | Train: 2.4768 (44.4%) | Phys: 0.00 | Val: 1.4660 (61.58%) | Best: 61.96%\n",
            "Epoch 112 | T: 21s | Train: 2.4199 (45.6%) | Phys: 0.00 | Val: 1.5800 (60.01%) | Best: 61.96%\n",
            "Epoch 113 | T: 21s | Train: 2.4300 (45.4%) | Phys: 0.00 | Val: 1.5641 (59.05%) | Best: 61.96%\n",
            "Epoch 114 | T: 21s | Train: 2.4622 (44.8%) | Phys: 0.00 | Val: 1.6680 (58.30%) | Best: 61.96%\n",
            "Epoch 115 | T: 20s | Train: 2.4390 (45.0%) | Phys: 0.00 | Val: 1.5375 (60.36%) | Best: 61.96%\n",
            "Epoch 116 | T: 21s | Train: 2.4335 (45.1%) | Phys: 0.00 | Val: 1.6170 (58.83%) | Best: 61.96%\n",
            "Epoch 117 | T: 21s | Train: 2.3875 (46.2%) | Phys: 0.00 | Val: 1.5557 (59.69%) | Best: 61.96%\n",
            "Epoch 118 | T: 21s | Train: 2.3925 (46.4%) | Phys: 0.00 | Val: 1.5611 (58.91%) | Best: 61.96%\n",
            "Epoch 119 | T: 21s | Train: 2.4508 (45.2%) | Phys: 0.00 | Val: 1.4396 (61.83%) | Best: 61.96%\n",
            "Epoch 120 | T: 20s | Train: 2.4054 (45.8%) | Phys: 0.00 | Val: 1.4836 (60.85%) | Best: 61.96%\n",
            "Epoch 121 | T: 21s | Train: 2.3799 (46.3%) | Phys: 0.00 | Val: 1.6237 (58.41%) | Best: 61.96%\n",
            "Epoch 122 | T: 21s | Train: 2.3809 (46.7%) | Phys: 0.00 | Val: 1.7913 (55.77%) | Best: 61.96%\n",
            "Epoch 123 | T: 21s | Train: 2.4296 (45.8%) | Phys: 0.00 | Val: 1.4877 (61.49%) | Best: 61.96%\n",
            "Epoch 124 | T: 21s | Train: 2.4112 (45.8%) | Phys: 0.00 | Val: 1.5899 (59.32%) | Best: 61.96%\n",
            "Epoch 125 | T: 21s | Train: 2.4401 (45.0%) | Phys: 0.00 | Val: 1.4276 (62.77%) | Best: 62.77%\n",
            "Epoch 126 | T: 21s | Train: 2.3893 (45.9%) | Phys: 0.00 | Val: 1.4675 (61.24%) | Best: 62.77%\n",
            "Epoch 127 | T: 21s | Train: 2.3628 (47.0%) | Phys: 0.00 | Val: 1.3535 (63.64%) | Best: 63.64%\n",
            "Epoch 128 | T: 21s | Train: 2.4329 (45.2%) | Phys: 0.00 | Val: 1.6324 (58.71%) | Best: 63.64%\n",
            "Epoch 129 | T: 21s | Train: 2.4635 (44.4%) | Phys: 0.00 | Val: 1.4490 (63.52%) | Best: 63.64%\n",
            "Epoch 130 | T: 21s | Train: 2.4001 (45.8%) | Phys: 0.00 | Val: 1.4190 (62.32%) | Best: 63.64%\n",
            "Epoch 131 | T: 20s | Train: 2.4088 (45.8%) | Phys: 0.00 | Val: 1.5681 (60.50%) | Best: 63.64%\n",
            "Epoch 132 | T: 21s | Train: 2.3966 (46.1%) | Phys: 0.00 | Val: 1.5502 (60.65%) | Best: 63.64%\n",
            "Epoch 133 | T: 21s | Train: 2.4300 (45.2%) | Phys: 0.00 | Val: 1.4498 (63.20%) | Best: 63.64%\n",
            "Epoch 134 | T: 21s | Train: 2.4012 (45.5%) | Phys: 0.00 | Val: 1.5535 (60.40%) | Best: 63.64%\n",
            "Epoch 135 | T: 21s | Train: 2.4232 (45.3%) | Phys: 0.00 | Val: 1.4164 (62.34%) | Best: 63.64%\n",
            "Epoch 136 | T: 21s | Train: 2.3604 (47.1%) | Phys: 0.00 | Val: 1.5731 (60.44%) | Best: 63.64%\n",
            "Epoch 137 | T: 21s | Train: 2.3850 (46.5%) | Phys: 0.00 | Val: 1.5737 (59.38%) | Best: 63.64%\n",
            "Epoch 138 | T: 21s | Train: 2.3973 (46.7%) | Phys: 0.00 | Val: 1.4811 (60.96%) | Best: 63.64%\n",
            "Epoch 139 | T: 21s | Train: 2.3805 (46.6%) | Phys: 0.00 | Val: 1.3827 (63.14%) | Best: 63.64%\n",
            "Epoch 140 | T: 21s | Train: 2.3902 (46.2%) | Phys: 0.00 | Val: 1.4810 (61.15%) | Best: 63.64%\n",
            "Epoch 141 | T: 21s | Train: 2.3723 (46.3%) | Phys: 0.00 | Val: 1.6345 (58.78%) | Best: 63.64%\n",
            "Epoch 142 | T: 21s | Train: 2.3262 (48.0%) | Phys: 0.00 | Val: 1.4013 (62.90%) | Best: 63.64%\n",
            "Epoch 143 | T: 20s | Train: 2.4007 (46.2%) | Phys: 0.00 | Val: 1.3174 (65.16%) | Best: 65.16%\n",
            "Epoch 144 | T: 20s | Train: 2.3477 (47.3%) | Phys: 0.00 | Val: 1.4536 (63.03%) | Best: 65.16%\n",
            "Epoch 145 | T: 20s | Train: 2.3285 (48.0%) | Phys: 0.00 | Val: 1.4054 (63.58%) | Best: 65.16%\n",
            "Epoch 146 | T: 21s | Train: 2.3952 (46.2%) | Phys: 0.00 | Val: 1.5163 (61.05%) | Best: 65.16%\n",
            "Epoch 147 | T: 21s | Train: 2.3566 (47.0%) | Phys: 0.00 | Val: 1.3875 (63.83%) | Best: 65.16%\n",
            "Epoch 148 | T: 21s | Train: 2.3117 (48.1%) | Phys: 0.00 | Val: 1.4097 (63.82%) | Best: 65.16%\n",
            "Epoch 149 | T: 21s | Train: 2.3791 (46.6%) | Phys: 0.00 | Val: 1.4994 (61.78%) | Best: 65.16%\n",
            "Epoch 150 | T: 21s | Train: 2.3494 (47.1%) | Phys: 0.00 | Val: 1.4143 (63.94%) | Best: 65.16%\n",
            "Epoch 151 | T: 21s | Train: 2.3587 (47.2%) | Phys: 0.00 | Val: 1.4487 (62.06%) | Best: 65.16%\n",
            "Epoch 152 | T: 21s | Train: 2.3532 (47.1%) | Phys: 0.00 | Val: 1.4150 (62.14%) | Best: 65.16%\n",
            "Epoch 153 | T: 21s | Train: 2.3155 (48.1%) | Phys: 0.00 | Val: 1.5992 (59.81%) | Best: 65.16%\n",
            "Epoch 154 | T: 21s | Train: 2.3357 (47.9%) | Phys: 0.00 | Val: 1.5653 (60.94%) | Best: 65.16%\n",
            "Epoch 155 | T: 21s | Train: 2.2791 (49.1%) | Phys: 0.00 | Val: 1.4976 (61.96%) | Best: 65.16%\n",
            "Epoch 156 | T: 21s | Train: 2.3066 (48.3%) | Phys: 0.00 | Val: 1.3944 (63.52%) | Best: 65.16%\n",
            "Epoch 157 | T: 20s | Train: 2.2562 (49.3%) | Phys: 0.00 | Val: 1.4022 (63.79%) | Best: 65.16%\n",
            "Epoch 158 | T: 21s | Train: 2.3673 (47.0%) | Phys: 0.00 | Val: 1.3586 (64.70%) | Best: 65.16%\n",
            "Epoch 159 | T: 21s | Train: 2.3207 (48.0%) | Phys: 0.00 | Val: 1.3788 (63.27%) | Best: 65.16%\n",
            "Epoch 160 | T: 21s | Train: 2.3572 (47.0%) | Phys: 0.00 | Val: 1.4928 (61.23%) | Best: 65.16%\n",
            "Epoch 161 | T: 21s | Train: 2.3622 (46.8%) | Phys: 0.00 | Val: 1.4789 (60.72%) | Best: 65.16%\n",
            "Epoch 162 | T: 21s | Train: 2.3608 (47.0%) | Phys: 0.00 | Val: 1.3369 (64.87%) | Best: 65.16%\n",
            "Epoch 163 | T: 21s | Train: 2.3259 (47.8%) | Phys: 0.00 | Val: 1.4851 (62.18%) | Best: 65.16%\n",
            "Epoch 164 | T: 21s | Train: 2.2909 (48.9%) | Phys: 0.00 | Val: 1.4880 (61.67%) | Best: 65.16%\n",
            "Epoch 165 | T: 21s | Train: 2.2297 (50.5%) | Phys: 0.00 | Val: 1.3019 (65.64%) | Best: 65.64%\n",
            "Epoch 166 | T: 20s | Train: 2.2509 (49.2%) | Phys: 0.00 | Val: 1.3970 (63.99%) | Best: 65.64%\n",
            "Epoch 167 | T: 20s | Train: 2.3255 (48.2%) | Phys: 0.00 | Val: 1.5145 (62.05%) | Best: 65.64%\n",
            "Epoch 168 | T: 21s | Train: 2.3073 (48.4%) | Phys: 0.00 | Val: 1.4904 (61.58%) | Best: 65.64%\n",
            "Epoch 169 | T: 21s | Train: 2.2807 (49.0%) | Phys: 0.00 | Val: 1.3866 (64.26%) | Best: 65.64%\n",
            "Epoch 170 | T: 21s | Train: 2.3036 (48.1%) | Phys: 0.00 | Val: 1.4887 (62.04%) | Best: 65.64%\n",
            "Epoch 171 | T: 21s | Train: 2.3293 (47.8%) | Phys: 0.00 | Val: 1.3958 (63.67%) | Best: 65.64%\n",
            "Epoch 172 | T: 21s | Train: 2.3345 (47.7%) | Phys: 0.00 | Val: 1.3928 (64.41%) | Best: 65.64%\n",
            "Epoch 173 | T: 21s | Train: 2.2767 (48.7%) | Phys: 0.00 | Val: 1.3811 (63.83%) | Best: 65.64%\n",
            "Epoch 174 | T: 21s | Train: 2.3346 (47.4%) | Phys: 0.00 | Val: 1.2771 (66.13%) | Best: 66.13%\n",
            "Epoch 175 | T: 20s | Train: 2.2909 (48.5%) | Phys: 0.00 | Val: 1.2840 (66.20%) | Best: 66.20%\n",
            "Epoch 176 | T: 21s | Train: 2.2833 (49.1%) | Phys: 0.00 | Val: 1.3205 (65.12%) | Best: 66.20%\n",
            "Epoch 177 | T: 21s | Train: 2.2694 (49.2%) | Phys: 0.00 | Val: 1.3582 (63.85%) | Best: 66.20%\n",
            "Epoch 178 | T: 20s | Train: 2.2782 (49.2%) | Phys: 0.00 | Val: 1.3379 (65.00%) | Best: 66.20%\n",
            "Epoch 179 | T: 21s | Train: 2.2662 (49.2%) | Phys: 0.00 | Val: 1.3788 (65.09%) | Best: 66.20%\n",
            "Epoch 180 | T: 21s | Train: 2.2221 (50.1%) | Phys: 0.00 | Val: 1.4486 (62.44%) | Best: 66.20%\n",
            "Epoch 181 | T: 21s | Train: 2.3169 (48.1%) | Phys: 0.00 | Val: 1.3190 (65.61%) | Best: 66.20%\n",
            "Epoch 182 | T: 21s | Train: 2.2793 (49.2%) | Phys: 0.00 | Val: 1.3648 (64.92%) | Best: 66.20%\n",
            "Epoch 183 | T: 21s | Train: 2.2387 (49.9%) | Phys: 0.00 | Val: 1.2898 (66.55%) | Best: 66.55%\n",
            "Epoch 184 | T: 21s | Train: 2.2327 (49.8%) | Phys: 0.00 | Val: 1.2709 (66.45%) | Best: 66.55%\n",
            "Epoch 185 | T: 21s | Train: 2.1979 (51.3%) | Phys: 0.00 | Val: 1.3625 (65.38%) | Best: 66.55%\n",
            "Epoch 186 | T: 21s | Train: 2.2185 (50.7%) | Phys: 0.00 | Val: 1.2669 (67.67%) | Best: 67.67%\n",
            "Epoch 187 | T: 21s | Train: 2.2233 (50.1%) | Phys: 0.00 | Val: 1.2576 (66.60%) | Best: 67.67%\n",
            "Epoch 188 | T: 21s | Train: 2.1924 (51.3%) | Phys: 0.00 | Val: 1.3813 (64.51%) | Best: 67.67%\n",
            "Epoch 189 | T: 21s | Train: 2.2395 (49.8%) | Phys: 0.00 | Val: 1.3877 (65.65%) | Best: 67.67%\n",
            "Epoch 190 | T: 21s | Train: 2.2298 (50.1%) | Phys: 0.00 | Val: 1.2876 (66.62%) | Best: 67.67%\n",
            "Epoch 191 | T: 20s | Train: 2.2707 (49.0%) | Phys: 0.00 | Val: 1.2657 (67.06%) | Best: 67.67%\n",
            "Epoch 192 | T: 21s | Train: 2.2435 (50.1%) | Phys: 0.00 | Val: 1.3880 (65.06%) | Best: 67.67%\n",
            "Epoch 193 | T: 21s | Train: 2.2751 (49.3%) | Phys: 0.00 | Val: 1.3095 (66.79%) | Best: 67.67%\n",
            "Epoch 194 | T: 21s | Train: 2.2371 (49.9%) | Phys: 0.00 | Val: 1.4081 (64.19%) | Best: 67.67%\n",
            "Epoch 195 | T: 21s | Train: 2.2262 (50.2%) | Phys: 0.00 | Val: 1.3626 (64.47%) | Best: 67.67%\n",
            "Epoch 196 | T: 21s | Train: 2.2400 (49.9%) | Phys: 0.00 | Val: 1.3194 (65.18%) | Best: 67.67%\n",
            "Epoch 197 | T: 21s | Train: 2.2334 (50.1%) | Phys: 0.00 | Val: 1.2824 (67.44%) | Best: 67.67%\n",
            "Epoch 198 | T: 21s | Train: 2.2097 (50.4%) | Phys: 0.00 | Val: 1.2367 (68.36%) | Best: 68.36%\n",
            "Epoch 199 | T: 21s | Train: 2.1949 (51.1%) | Phys: 0.00 | Val: 1.2752 (67.22%) | Best: 68.36%\n",
            "Epoch 200 | T: 21s | Train: 2.2018 (50.9%) | Phys: 0.00 | Val: 1.2133 (69.00%) | Best: 69.00%\n",
            "Epoch 201 | T: 21s | Train: 2.2324 (50.4%) | Phys: 0.00 | Val: 1.3606 (65.70%) | Best: 69.00%\n",
            "Epoch 202 | T: 20s | Train: 2.2094 (50.8%) | Phys: 0.00 | Val: 1.2241 (68.44%) | Best: 69.00%\n",
            "Epoch 203 | T: 20s | Train: 2.1270 (52.8%) | Phys: 0.00 | Val: 1.4915 (62.74%) | Best: 69.00%\n",
            "Epoch 204 | T: 21s | Train: 2.1933 (51.1%) | Phys: 0.00 | Val: 1.2968 (65.97%) | Best: 69.00%\n",
            "Epoch 205 | T: 21s | Train: 2.1773 (51.5%) | Phys: 0.00 | Val: 1.3041 (66.31%) | Best: 69.00%\n",
            "Epoch 206 | T: 21s | Train: 2.1873 (51.1%) | Phys: 0.00 | Val: 1.1552 (69.30%) | Best: 69.30%\n",
            "Epoch 207 | T: 21s | Train: 2.1549 (51.9%) | Phys: 0.00 | Val: 1.2627 (66.66%) | Best: 69.30%\n",
            "Epoch 208 | T: 21s | Train: 2.1794 (51.2%) | Phys: 0.00 | Val: 1.2731 (67.91%) | Best: 69.30%\n",
            "Epoch 209 | T: 21s | Train: 2.1116 (53.0%) | Phys: 0.00 | Val: 1.2681 (67.99%) | Best: 69.30%\n",
            "Epoch 210 | T: 21s | Train: 2.1491 (52.0%) | Phys: 0.00 | Val: 1.1615 (68.82%) | Best: 69.30%\n",
            "Epoch 211 | T: 21s | Train: 2.1329 (52.3%) | Phys: 0.00 | Val: 1.3792 (64.41%) | Best: 69.30%\n",
            "Epoch 212 | T: 21s | Train: 2.1627 (51.6%) | Phys: 0.00 | Val: 1.3208 (66.72%) | Best: 69.30%\n",
            "Epoch 213 | T: 21s | Train: 2.1426 (52.4%) | Phys: 0.00 | Val: 1.2639 (68.09%) | Best: 69.30%\n",
            "Epoch 214 | T: 21s | Train: 2.1530 (52.1%) | Phys: 0.00 | Val: 1.2414 (67.73%) | Best: 69.30%\n",
            "Epoch 215 | T: 21s | Train: 2.1238 (52.4%) | Phys: 0.00 | Val: 1.1803 (69.05%) | Best: 69.30%\n",
            "Epoch 216 | T: 21s | Train: 2.1564 (52.0%) | Phys: 0.00 | Val: 1.1852 (69.27%) | Best: 69.30%\n",
            "Epoch 217 | T: 20s | Train: 2.1589 (52.1%) | Phys: 0.00 | Val: 1.2049 (68.81%) | Best: 69.30%\n",
            "Epoch 218 | T: 21s | Train: 2.1402 (52.4%) | Phys: 0.00 | Val: 1.2999 (66.07%) | Best: 69.30%\n",
            "Epoch 219 | T: 21s | Train: 2.1526 (51.9%) | Phys: 0.00 | Val: 1.3137 (66.20%) | Best: 69.30%\n",
            "Epoch 220 | T: 21s | Train: 2.1181 (52.7%) | Phys: 0.00 | Val: 1.1919 (69.38%) | Best: 69.38%\n",
            "Epoch 221 | T: 21s | Train: 2.1483 (51.9%) | Phys: 0.00 | Val: 1.2384 (68.42%) | Best: 69.38%\n",
            "Epoch 222 | T: 20s | Train: 2.0881 (53.5%) | Phys: 0.00 | Val: 1.1416 (70.16%) | Best: 70.16%\n",
            "Epoch 223 | T: 21s | Train: 2.1144 (53.1%) | Phys: 0.00 | Val: 1.2109 (68.90%) | Best: 70.16%\n",
            "Epoch 224 | T: 21s | Train: 2.1108 (52.9%) | Phys: 0.00 | Val: 1.1987 (69.94%) | Best: 70.16%\n",
            "Epoch 225 | T: 21s | Train: 2.1562 (52.1%) | Phys: 0.00 | Val: 1.2672 (67.94%) | Best: 70.16%\n",
            "Epoch 226 | T: 20s | Train: 2.0987 (53.5%) | Phys: 0.00 | Val: 1.1524 (70.13%) | Best: 70.16%\n",
            "Epoch 227 | T: 21s | Train: 2.0390 (54.6%) | Phys: 0.00 | Val: 1.2091 (68.75%) | Best: 70.16%\n",
            "Epoch 228 | T: 21s | Train: 2.0891 (53.4%) | Phys: 0.00 | Val: 1.1031 (70.46%) | Best: 70.46%\n",
            "Epoch 229 | T: 21s | Train: 2.1161 (52.6%) | Phys: 0.00 | Val: 1.2173 (69.51%) | Best: 70.46%\n",
            "Epoch 230 | T: 21s | Train: 2.1095 (53.0%) | Phys: 0.00 | Val: 1.2448 (68.42%) | Best: 70.46%\n",
            "Epoch 231 | T: 21s | Train: 2.0723 (53.9%) | Phys: 0.00 | Val: 1.1836 (69.59%) | Best: 70.46%\n",
            "Epoch 232 | T: 21s | Train: 2.1509 (51.8%) | Phys: 0.00 | Val: 1.1805 (69.83%) | Best: 70.46%\n",
            "Epoch 233 | T: 21s | Train: 2.0657 (53.9%) | Phys: 0.00 | Val: 1.1262 (70.64%) | Best: 70.64%\n",
            "Epoch 234 | T: 21s | Train: 2.1508 (52.1%) | Phys: 0.00 | Val: 1.2466 (69.44%) | Best: 70.64%\n",
            "Epoch 235 | T: 21s | Train: 2.0164 (55.4%) | Phys: 0.00 | Val: 1.2438 (68.85%) | Best: 70.64%\n",
            "Epoch 236 | T: 21s | Train: 2.0606 (54.2%) | Phys: 0.00 | Val: 1.2108 (69.02%) | Best: 70.64%\n",
            "Epoch 237 | T: 21s | Train: 2.1023 (52.9%) | Phys: 0.00 | Val: 1.1181 (69.83%) | Best: 70.64%\n",
            "Epoch 238 | T: 21s | Train: 2.1038 (53.2%) | Phys: 0.00 | Val: 1.1976 (70.10%) | Best: 70.64%\n",
            "Epoch 239 | T: 21s | Train: 2.0849 (53.8%) | Phys: 0.00 | Val: 1.2219 (68.69%) | Best: 70.64%\n",
            "Epoch 240 | T: 21s | Train: 2.0154 (55.1%) | Phys: 0.00 | Val: 1.1824 (69.89%) | Best: 70.64%\n",
            "Epoch 241 | T: 21s | Train: 2.0598 (54.1%) | Phys: 0.00 | Val: 1.0909 (71.11%) | Best: 71.11%\n",
            "Epoch 242 | T: 21s | Train: 2.0357 (54.9%) | Phys: 0.00 | Val: 1.1650 (69.98%) | Best: 71.11%\n",
            "Epoch 243 | T: 20s | Train: 1.9692 (56.5%) | Phys: 0.00 | Val: 1.1959 (70.09%) | Best: 71.11%\n",
            "Epoch 244 | T: 21s | Train: 2.0112 (55.0%) | Phys: 0.00 | Val: 1.1391 (70.68%) | Best: 71.11%\n",
            "Epoch 245 | T: 21s | Train: 2.0402 (54.6%) | Phys: 0.00 | Val: 1.2382 (68.91%) | Best: 71.11%\n",
            "Epoch 246 | T: 21s | Train: 1.9564 (57.1%) | Phys: 0.00 | Val: 1.2086 (70.54%) | Best: 71.11%\n",
            "Epoch 247 | T: 21s | Train: 2.0564 (54.5%) | Phys: 0.00 | Val: 1.1201 (71.50%) | Best: 71.50%\n",
            "Epoch 248 | T: 21s | Train: 2.0313 (54.7%) | Phys: 0.00 | Val: 1.2276 (68.37%) | Best: 71.50%\n",
            "Epoch 249 | T: 21s | Train: 1.9866 (56.4%) | Phys: 0.00 | Val: 1.1173 (71.67%) | Best: 71.67%\n",
            "Epoch 250 | T: 20s | Train: 1.9845 (56.4%) | Phys: 0.00 | Val: 1.0804 (71.90%) | Best: 71.90%\n",
            "Epoch 251 | T: 21s | Train: 2.0070 (55.4%) | Phys: 0.00 | Val: 1.0790 (72.15%) | Best: 72.15%\n",
            "Epoch 252 | T: 21s | Train: 1.9637 (56.7%) | Phys: 0.00 | Val: 1.0568 (71.20%) | Best: 72.15%\n",
            "Epoch 253 | T: 21s | Train: 1.9656 (56.4%) | Phys: 0.00 | Val: 1.1777 (69.94%) | Best: 72.15%\n",
            "Epoch 254 | T: 21s | Train: 1.9692 (56.7%) | Phys: 0.00 | Val: 1.1617 (70.62%) | Best: 72.15%\n",
            "Epoch 255 | T: 21s | Train: 1.9461 (57.2%) | Phys: 0.00 | Val: 1.0613 (71.85%) | Best: 72.15%\n",
            "Epoch 256 | T: 21s | Train: 1.9226 (57.1%) | Phys: 0.00 | Val: 1.1016 (71.45%) | Best: 72.15%\n",
            "Epoch 257 | T: 21s | Train: 1.9909 (56.2%) | Phys: 0.00 | Val: 1.1512 (70.97%) | Best: 72.15%\n",
            "Epoch 258 | T: 21s | Train: 1.9787 (56.7%) | Phys: 0.00 | Val: 1.0733 (72.41%) | Best: 72.41%\n",
            "Epoch 259 | T: 21s | Train: 1.9784 (56.2%) | Phys: 0.00 | Val: 1.1214 (71.31%) | Best: 72.41%\n",
            "Epoch 260 | T: 21s | Train: 1.9864 (55.8%) | Phys: 0.00 | Val: 1.0830 (72.10%) | Best: 72.41%\n",
            "Epoch 261 | T: 20s | Train: 2.0063 (55.6%) | Phys: 0.00 | Val: 1.1331 (71.67%) | Best: 72.41%\n",
            "Epoch 262 | T: 20s | Train: 1.9361 (56.9%) | Phys: 0.00 | Val: 1.1001 (71.09%) | Best: 72.41%\n",
            "Epoch 263 | T: 21s | Train: 1.9628 (56.1%) | Phys: 0.00 | Val: 1.0884 (71.92%) | Best: 72.41%\n",
            "Epoch 264 | T: 21s | Train: 1.9324 (57.3%) | Phys: 0.00 | Val: 1.1360 (71.81%) | Best: 72.41%\n",
            "Epoch 265 | T: 21s | Train: 1.9766 (56.1%) | Phys: 0.00 | Val: 1.1985 (70.41%) | Best: 72.41%\n",
            "Epoch 266 | T: 21s | Train: 1.9490 (56.7%) | Phys: 0.00 | Val: 1.0554 (72.65%) | Best: 72.65%\n",
            "Epoch 267 | T: 21s | Train: 1.8969 (57.9%) | Phys: 0.00 | Val: 1.1183 (70.71%) | Best: 72.65%\n",
            "Epoch 268 | T: 21s | Train: 1.9614 (56.3%) | Phys: 0.00 | Val: 1.1794 (70.12%) | Best: 72.65%\n",
            "Epoch 269 | T: 21s | Train: 1.9418 (57.0%) | Phys: 0.00 | Val: 1.1037 (72.14%) | Best: 72.65%\n",
            "Epoch 270 | T: 21s | Train: 1.8968 (57.9%) | Phys: 0.00 | Val: 1.1428 (70.64%) | Best: 72.65%\n",
            "Epoch 271 | T: 21s | Train: 1.8937 (58.2%) | Phys: 0.00 | Val: 1.0189 (73.05%) | Best: 73.05%\n",
            "Epoch 272 | T: 21s | Train: 1.9025 (58.1%) | Phys: 0.00 | Val: 1.0575 (72.73%) | Best: 73.05%\n",
            "Epoch 273 | T: 20s | Train: 1.9692 (56.5%) | Phys: 0.00 | Val: 1.1703 (70.98%) | Best: 73.05%\n",
            "Epoch 274 | T: 20s | Train: 1.9058 (57.8%) | Phys: 0.00 | Val: 1.1240 (71.51%) | Best: 73.05%\n",
            "Epoch 275 | T: 21s | Train: 1.9482 (56.5%) | Phys: 0.00 | Val: 1.0468 (73.75%) | Best: 73.75%\n",
            "Epoch 276 | T: 20s | Train: 1.8667 (58.9%) | Phys: 0.00 | Val: 1.0624 (72.94%) | Best: 73.75%\n",
            "Epoch 277 | T: 21s | Train: 1.9055 (57.7%) | Phys: 0.00 | Val: 1.0553 (72.50%) | Best: 73.75%\n",
            "Epoch 278 | T: 21s | Train: 1.8514 (58.6%) | Phys: 0.00 | Val: 1.0327 (72.75%) | Best: 73.75%\n",
            "Epoch 279 | T: 21s | Train: 1.8376 (59.6%) | Phys: 0.00 | Val: 1.0400 (73.62%) | Best: 73.75%\n",
            "Epoch 280 | T: 21s | Train: 1.8833 (58.3%) | Phys: 0.00 | Val: 1.0633 (73.01%) | Best: 73.75%\n",
            "Epoch 281 | T: 21s | Train: 1.8844 (58.7%) | Phys: 0.00 | Val: 1.0065 (74.33%) | Best: 74.33%\n",
            "Epoch 282 | T: 21s | Train: 1.8333 (59.4%) | Phys: 0.00 | Val: 0.9984 (74.49%) | Best: 74.49%\n",
            "Epoch 283 | T: 21s | Train: 1.8096 (59.9%) | Phys: 0.00 | Val: 1.1003 (72.46%) | Best: 74.49%\n",
            "Epoch 284 | T: 21s | Train: 1.8542 (59.2%) | Phys: 0.00 | Val: 1.0358 (74.05%) | Best: 74.49%\n",
            "Epoch 285 | T: 20s | Train: 1.8132 (60.2%) | Phys: 0.00 | Val: 0.9980 (74.01%) | Best: 74.49%\n",
            "Epoch 286 | T: 21s | Train: 1.8355 (59.0%) | Phys: 0.00 | Val: 1.0530 (73.02%) | Best: 74.49%\n",
            "Epoch 287 | T: 21s | Train: 1.7924 (60.8%) | Phys: 0.00 | Val: 1.0815 (71.72%) | Best: 74.49%\n",
            "Epoch 288 | T: 21s | Train: 1.7849 (60.8%) | Phys: 0.00 | Val: 1.0153 (74.11%) | Best: 74.49%\n",
            "Epoch 289 | T: 21s | Train: 1.8161 (59.7%) | Phys: 0.00 | Val: 0.9969 (74.53%) | Best: 74.53%\n",
            "Epoch 290 | T: 21s | Train: 1.8175 (59.7%) | Phys: 0.00 | Val: 1.0129 (74.15%) | Best: 74.53%\n",
            "Epoch 291 | T: 21s | Train: 1.7803 (60.6%) | Phys: 0.00 | Val: 1.0018 (73.95%) | Best: 74.53%\n",
            "Epoch 292 | T: 21s | Train: 1.7129 (62.7%) | Phys: 0.00 | Val: 0.9967 (74.18%) | Best: 74.53%\n",
            "Epoch 293 | T: 21s | Train: 1.7723 (61.2%) | Phys: 0.00 | Val: 0.9927 (74.55%) | Best: 74.55%\n",
            "Epoch 294 | T: 21s | Train: 1.7570 (61.6%) | Phys: 0.00 | Val: 0.9765 (75.41%) | Best: 75.41%\n",
            "Epoch 295 | T: 21s | Train: 1.7897 (60.4%) | Phys: 0.00 | Val: 0.9893 (74.58%) | Best: 75.41%\n",
            "Epoch 296 | T: 20s | Train: 1.7369 (62.1%) | Phys: 0.00 | Val: 1.0073 (74.21%) | Best: 75.41%\n",
            "Epoch 297 | T: 20s | Train: 1.8065 (60.3%) | Phys: 0.00 | Val: 1.0667 (74.17%) | Best: 75.41%\n",
            "Epoch 298 | T: 20s | Train: 1.7635 (61.4%) | Phys: 0.00 | Val: 0.9348 (75.60%) | Best: 75.60%\n",
            "Epoch 299 | T: 21s | Train: 1.7273 (62.2%) | Phys: 0.00 | Val: 1.0601 (73.90%) | Best: 75.60%\n",
            "Epoch 300 | T: 21s | Train: 1.7415 (61.7%) | Phys: 0.00 | Val: 0.9537 (75.42%) | Best: 75.60%\n",
            "Epoch 301 | T: 21s | Train: 1.7104 (62.2%) | Phys: 0.00 | Val: 0.9571 (75.62%) | Best: 75.62%\n",
            "Epoch 302 | T: 21s | Train: 1.7448 (61.5%) | Phys: 0.00 | Val: 0.9041 (75.94%) | Best: 75.94%\n",
            "Epoch 303 | T: 21s | Train: 1.7503 (61.3%) | Phys: 0.00 | Val: 1.0203 (74.14%) | Best: 75.94%\n",
            "Epoch 304 | T: 21s | Train: 1.6333 (64.4%) | Phys: 0.00 | Val: 0.9608 (75.17%) | Best: 75.94%\n",
            "Epoch 305 | T: 20s | Train: 1.7339 (61.8%) | Phys: 0.00 | Val: 1.0187 (75.11%) | Best: 75.94%\n",
            "Epoch 306 | T: 21s | Train: 1.7280 (61.5%) | Phys: 0.00 | Val: 0.9092 (76.43%) | Best: 76.43%\n",
            "Epoch 307 | T: 21s | Train: 1.7421 (61.5%) | Phys: 0.00 | Val: 0.9426 (75.88%) | Best: 76.43%\n",
            "Epoch 308 | T: 20s | Train: 1.7115 (62.5%) | Phys: 0.00 | Val: 0.9288 (76.71%) | Best: 76.71%\n",
            "Epoch 309 | T: 20s | Train: 1.7354 (61.4%) | Phys: 0.00 | Val: 0.9123 (76.79%) | Best: 76.79%\n",
            "Epoch 310 | T: 21s | Train: 1.6299 (63.8%) | Phys: 0.00 | Val: 0.9307 (76.50%) | Best: 76.79%\n",
            "Epoch 311 | T: 21s | Train: 1.6061 (64.6%) | Phys: 0.00 | Val: 0.9423 (76.55%) | Best: 76.79%\n",
            "Epoch 312 | T: 21s | Train: 1.6832 (63.5%) | Phys: 0.00 | Val: 0.9590 (75.88%) | Best: 76.79%\n",
            "Epoch 313 | T: 21s | Train: 1.6606 (63.4%) | Phys: 0.00 | Val: 0.9504 (76.16%) | Best: 76.79%\n",
            "Epoch 314 | T: 21s | Train: 1.6981 (62.8%) | Phys: 0.00 | Val: 0.9338 (76.52%) | Best: 76.79%\n",
            "Epoch 315 | T: 21s | Train: 1.6802 (62.6%) | Phys: 0.00 | Val: 0.9093 (77.09%) | Best: 77.09%\n",
            "Epoch 316 | T: 21s | Train: 1.6860 (62.7%) | Phys: 0.00 | Val: 0.9195 (77.08%) | Best: 77.09%\n",
            "Epoch 317 | T: 21s | Train: 1.6460 (63.6%) | Phys: 0.00 | Val: 0.9227 (75.98%) | Best: 77.09%\n",
            "Epoch 318 | T: 21s | Train: 1.6482 (63.6%) | Phys: 0.00 | Val: 0.9066 (76.76%) | Best: 77.09%\n",
            "Epoch 319 | T: 21s | Train: 1.6145 (64.4%) | Phys: 0.00 | Val: 0.9277 (76.50%) | Best: 77.09%\n",
            "Epoch 320 | T: 20s | Train: 1.5881 (65.8%) | Phys: 0.00 | Val: 0.9621 (76.29%) | Best: 77.09%\n",
            "Epoch 321 | T: 21s | Train: 1.6180 (64.5%) | Phys: 0.00 | Val: 0.9498 (76.86%) | Best: 77.09%\n",
            "Epoch 322 | T: 21s | Train: 1.5964 (65.4%) | Phys: 0.00 | Val: 0.9388 (77.14%) | Best: 77.14%\n",
            "Epoch 323 | T: 21s | Train: 1.6209 (64.0%) | Phys: 0.00 | Val: 0.9112 (77.47%) | Best: 77.47%\n",
            "Epoch 324 | T: 21s | Train: 1.5593 (66.1%) | Phys: 0.00 | Val: 0.9182 (76.82%) | Best: 77.47%\n",
            "Epoch 325 | T: 21s | Train: 1.6475 (63.2%) | Phys: 0.00 | Val: 0.8862 (77.81%) | Best: 77.81%\n",
            "Epoch 326 | T: 21s | Train: 1.5971 (64.9%) | Phys: 0.00 | Val: 0.8925 (77.75%) | Best: 77.81%\n",
            "Epoch 327 | T: 21s | Train: 1.6463 (63.3%) | Phys: 0.00 | Val: 0.8678 (78.26%) | Best: 78.26%\n",
            "Epoch 328 | T: 21s | Train: 1.6102 (64.6%) | Phys: 0.00 | Val: 0.9072 (77.19%) | Best: 78.26%\n",
            "Epoch 329 | T: 20s | Train: 1.6024 (64.4%) | Phys: 0.00 | Val: 0.8420 (78.69%) | Best: 78.69%\n",
            "Epoch 330 | T: 21s | Train: 1.5607 (66.0%) | Phys: 0.00 | Val: 0.9101 (77.65%) | Best: 78.69%\n",
            "Epoch 331 | T: 21s | Train: 1.4906 (68.0%) | Phys: 0.00 | Val: 0.9074 (77.86%) | Best: 78.69%\n",
            "Epoch 332 | T: 20s | Train: 1.6431 (63.0%) | Phys: 0.00 | Val: 0.9098 (77.80%) | Best: 78.69%\n",
            "Epoch 333 | T: 20s | Train: 1.5462 (65.8%) | Phys: 0.00 | Val: 0.8900 (78.45%) | Best: 78.69%\n",
            "Epoch 334 | T: 21s | Train: 1.5243 (66.4%) | Phys: 0.00 | Val: 0.8304 (79.02%) | Best: 79.02%\n",
            "Epoch 335 | T: 21s | Train: 1.5748 (64.9%) | Phys: 0.00 | Val: 0.8623 (78.40%) | Best: 79.02%\n",
            "Epoch 336 | T: 20s | Train: 1.5289 (66.2%) | Phys: 0.00 | Val: 0.8332 (78.68%) | Best: 79.02%\n",
            "Epoch 337 | T: 20s | Train: 1.5485 (66.2%) | Phys: 0.00 | Val: 0.9155 (77.70%) | Best: 79.02%\n",
            "Epoch 338 | T: 21s | Train: 1.5991 (63.8%) | Phys: 0.00 | Val: 0.8835 (78.74%) | Best: 79.02%\n",
            "Epoch 339 | T: 21s | Train: 1.4878 (67.2%) | Phys: 0.00 | Val: 0.8524 (78.78%) | Best: 79.02%\n",
            "Epoch 340 | T: 20s | Train: 1.4645 (67.8%) | Phys: 0.00 | Val: 0.9068 (78.36%) | Best: 79.02%\n",
            "Epoch 341 | T: 21s | Train: 1.4913 (67.1%) | Phys: 0.00 | Val: 0.8680 (78.64%) | Best: 79.02%\n",
            "Epoch 342 | T: 21s | Train: 1.4902 (67.0%) | Phys: 0.00 | Val: 0.8800 (77.88%) | Best: 79.02%\n",
            "Epoch 343 | T: 21s | Train: 1.4364 (68.0%) | Phys: 0.00 | Val: 0.8955 (78.38%) | Best: 79.02%\n",
            "Epoch 344 | T: 20s | Train: 1.4655 (67.5%) | Phys: 0.00 | Val: 0.8505 (78.86%) | Best: 79.02%\n",
            "Epoch 345 | T: 20s | Train: 1.4392 (68.3%) | Phys: 0.00 | Val: 0.7869 (79.38%) | Best: 79.38%\n",
            "Epoch 346 | T: 21s | Train: 1.5076 (66.1%) | Phys: 0.00 | Val: 0.8260 (79.35%) | Best: 79.38%\n",
            "Epoch 347 | T: 21s | Train: 1.4099 (69.3%) | Phys: 0.00 | Val: 0.8323 (79.11%) | Best: 79.38%\n",
            "Epoch 348 | T: 21s | Train: 1.4234 (68.8%) | Phys: 0.00 | Val: 0.8232 (79.24%) | Best: 79.38%\n",
            "Epoch 349 | T: 20s | Train: 1.4517 (67.5%) | Phys: 0.00 | Val: 0.8473 (79.25%) | Best: 79.38%\n",
            "Epoch 350 | T: 21s | Train: 1.4279 (68.6%) | Phys: 0.00 | Val: 0.8313 (78.93%) | Best: 79.38%\n",
            "Epoch 351 | T: 21s | Train: 1.4612 (67.7%) | Phys: 0.00 | Val: 0.8169 (79.76%) | Best: 79.76%\n",
            "Epoch 352 | T: 21s | Train: 1.4548 (67.3%) | Phys: 0.00 | Val: 0.8231 (79.28%) | Best: 79.76%\n",
            "Epoch 353 | T: 21s | Train: 1.4882 (65.9%) | Phys: 0.00 | Val: 0.8560 (79.53%) | Best: 79.76%\n",
            "Epoch 354 | T: 21s | Train: 1.4552 (67.3%) | Phys: 0.00 | Val: 0.8001 (79.91%) | Best: 79.91%\n",
            "Epoch 355 | T: 21s | Train: 1.4036 (68.7%) | Phys: 0.00 | Val: 0.7702 (80.30%) | Best: 80.30%\n",
            "Epoch 356 | T: 20s | Train: 1.3656 (70.3%) | Phys: 0.00 | Val: 0.8495 (80.12%) | Best: 80.30%\n",
            "Epoch 357 | T: 21s | Train: 1.4156 (68.1%) | Phys: 0.00 | Val: 0.8050 (80.05%) | Best: 80.30%\n",
            "Epoch 358 | T: 21s | Train: 1.3648 (69.7%) | Phys: 0.00 | Val: 0.8232 (80.07%) | Best: 80.30%\n",
            "Epoch 359 | T: 21s | Train: 1.4507 (67.1%) | Phys: 0.00 | Val: 0.8254 (80.05%) | Best: 80.30%\n",
            "Epoch 360 | T: 21s | Train: 1.3634 (69.2%) | Phys: 0.00 | Val: 0.7895 (80.41%) | Best: 80.41%\n",
            "Epoch 361 | T: 21s | Train: 1.4822 (65.8%) | Phys: 0.00 | Val: 0.8206 (80.18%) | Best: 80.41%\n",
            "Epoch 362 | T: 21s | Train: 1.3760 (68.8%) | Phys: 0.00 | Val: 0.7865 (80.64%) | Best: 80.64%\n",
            "Epoch 363 | T: 21s | Train: 1.4147 (67.5%) | Phys: 0.00 | Val: 0.8450 (80.08%) | Best: 80.64%\n",
            "Epoch 364 | T: 21s | Train: 1.3411 (69.8%) | Phys: 0.00 | Val: 0.7740 (80.97%) | Best: 80.97%\n",
            "Epoch 365 | T: 21s | Train: 1.3462 (69.9%) | Phys: 0.00 | Val: 0.7990 (80.54%) | Best: 80.97%\n",
            "Epoch 366 | T: 21s | Train: 1.3698 (68.7%) | Phys: 0.00 | Val: 0.7849 (80.90%) | Best: 80.97%\n",
            "Epoch 367 | T: 21s | Train: 1.4361 (66.3%) | Phys: 0.00 | Val: 0.8011 (80.57%) | Best: 80.97%\n",
            "Epoch 368 | T: 20s | Train: 1.4151 (66.7%) | Phys: 0.00 | Val: 0.8072 (80.70%) | Best: 80.97%\n",
            "Epoch 369 | T: 21s | Train: 1.3346 (68.9%) | Phys: 0.00 | Val: 0.7776 (81.24%) | Best: 81.24%\n",
            "Epoch 370 | T: 21s | Train: 1.3905 (68.3%) | Phys: 0.00 | Val: 0.7996 (80.65%) | Best: 81.24%\n",
            "Epoch 371 | T: 21s | Train: 1.3507 (68.7%) | Phys: 0.00 | Val: 0.7951 (81.01%) | Best: 81.24%\n",
            "Epoch 372 | T: 21s | Train: 1.3004 (70.5%) | Phys: 0.00 | Val: 0.7783 (81.15%) | Best: 81.24%\n",
            "Epoch 373 | T: 21s | Train: 1.3107 (70.6%) | Phys: 0.00 | Val: 0.7741 (81.67%) | Best: 81.67%\n",
            "Epoch 374 | T: 21s | Train: 1.3080 (69.8%) | Phys: 0.00 | Val: 0.7694 (81.13%) | Best: 81.67%\n",
            "Epoch 375 | T: 21s | Train: 1.3206 (69.5%) | Phys: 0.00 | Val: 0.7576 (81.64%) | Best: 81.67%\n",
            "Epoch 376 | T: 21s | Train: 1.3173 (69.6%) | Phys: 0.00 | Val: 0.7842 (81.30%) | Best: 81.67%\n",
            "Epoch 377 | T: 21s | Train: 1.3361 (68.7%) | Phys: 0.00 | Val: 0.8043 (81.04%) | Best: 81.67%\n",
            "Epoch 378 | T: 21s | Train: 1.3207 (69.9%) | Phys: 0.00 | Val: 0.7891 (81.30%) | Best: 81.67%\n",
            "Epoch 379 | T: 20s | Train: 1.3479 (69.0%) | Phys: 0.00 | Val: 0.7899 (81.55%) | Best: 81.67%\n",
            "Epoch 380 | T: 21s | Train: 1.2947 (70.5%) | Phys: 0.00 | Val: 0.7721 (81.45%) | Best: 81.67%\n",
            "Epoch 381 | T: 21s | Train: 1.3020 (70.0%) | Phys: 0.00 | Val: 0.7550 (81.68%) | Best: 81.68%\n",
            "Epoch 382 | T: 21s | Train: 1.2649 (71.2%) | Phys: 0.00 | Val: 0.7324 (81.70%) | Best: 81.70%\n",
            "Epoch 383 | T: 21s | Train: 1.2964 (69.9%) | Phys: 0.00 | Val: 0.7619 (81.84%) | Best: 81.84%\n",
            "Epoch 384 | T: 21s | Train: 1.2702 (70.6%) | Phys: 0.00 | Val: 0.7366 (81.93%) | Best: 81.93%\n",
            "Epoch 385 | T: 21s | Train: 1.2989 (69.4%) | Phys: 0.00 | Val: 0.7701 (81.51%) | Best: 81.93%\n",
            "Epoch 386 | T: 21s | Train: 1.2526 (71.3%) | Phys: 0.00 | Val: 0.7544 (81.75%) | Best: 81.93%\n",
            "Epoch 387 | T: 21s | Train: 1.2546 (71.0%) | Phys: 0.00 | Val: 0.7584 (81.81%) | Best: 81.93%\n",
            "Epoch 388 | T: 21s | Train: 1.3446 (68.1%) | Phys: 0.00 | Val: 0.7565 (81.95%) | Best: 81.95%\n",
            "Epoch 389 | T: 21s | Train: 1.2955 (70.0%) | Phys: 0.00 | Val: 0.7498 (81.89%) | Best: 81.95%\n",
            "Epoch 390 | T: 20s | Train: 1.2921 (70.2%) | Phys: 0.00 | Val: 0.7592 (81.70%) | Best: 81.95%\n",
            "Epoch 391 | T: 20s | Train: 1.2879 (70.0%) | Phys: 0.00 | Val: 0.7592 (81.63%) | Best: 81.95%\n",
            "Epoch 392 | T: 21s | Train: 1.3050 (69.6%) | Phys: 0.00 | Val: 0.7643 (81.63%) | Best: 81.95%\n",
            "Epoch 393 | T: 21s | Train: 1.2961 (69.9%) | Phys: 0.00 | Val: 0.7326 (81.86%) | Best: 81.95%\n",
            "Epoch 394 | T: 21s | Train: 1.2948 (70.0%) | Phys: 0.00 | Val: 0.7505 (81.86%) | Best: 81.95%\n",
            "Epoch 395 | T: 21s | Train: 1.2807 (70.3%) | Phys: 0.00 | Val: 0.7446 (81.87%) | Best: 81.95%\n",
            "Epoch 396 | T: 21s | Train: 1.2979 (69.8%) | Phys: 0.00 | Val: 0.7185 (82.13%) | Best: 82.13%\n",
            "Epoch 397 | T: 21s | Train: 1.2790 (70.6%) | Phys: 0.00 | Val: 0.8522 (80.92%) | Best: 82.13%\n",
            "Epoch 398 | T: 21s | Train: 1.2460 (71.7%) | Phys: 0.00 | Val: 0.7811 (81.89%) | Best: 82.13%\n",
            "Epoch 399 | T: 21s | Train: 1.2490 (71.2%) | Phys: 0.00 | Val: 0.7941 (81.72%) | Best: 82.13%\n",
            "Epoch 400 | T: 21s | Train: 1.2805 (70.8%) | Phys: 0.00 | Val: 0.8636 (80.54%) | Best: 82.13%\n",
            "Final Best: 82.13%\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": [],
      "name": "CIFAR-100 SIGReg baseline 1600 EP",
      "gpuType": "L4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}